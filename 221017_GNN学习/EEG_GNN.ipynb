{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82b39bc-94ac-4169-b2b7-8395f665dc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "def get_edge_index(self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    #用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "    chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],9:[5,3],10:[6,3],11:[6,0],12:[7,3],\n",
    "                    13:[8,3],14:[7,4],15:[6,4],16:[0,5],17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "                    25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "    maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "    for k, v in chan_to_1020.items():\n",
    "        maps[v[0]][v[1]] = k + 1\n",
    "    print(maps)\n",
    "    \n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if maps[i][j] == 0:\n",
    "                continue\n",
    "            # up\n",
    "            d = i - 1\n",
    "            while d >= 0:\n",
    "                if maps[d][j] != 0:\n",
    "                    edge_index[0]\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[d][j] - 1)\n",
    "                    weight.append(1 / math.sqrt((d - i)**2))\n",
    "                    break\n",
    "                d = d - 1\n",
    "            # down\n",
    "            d = i + 1\n",
    "            while d <= 8:\n",
    "                if maps[d][j] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[d][j] - 1)\n",
    "                    weight.append(1 / math.sqrt((d - i)**2))\n",
    "                    break\n",
    "                d = d + 1\n",
    "            # left\n",
    "            d = j - 1\n",
    "            while d >= 0:\n",
    "                if maps[i][d] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[i][d] - 1)\n",
    "                    weight.append(1 / math.sqrt((d - j)**2))\n",
    "                    break\n",
    "                d = d - 1\n",
    "\n",
    "            # right\n",
    "            d = j + 1\n",
    "            while d <= 8:\n",
    "                if maps[i][d] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[i][d] - 1)\n",
    "                    weight.append(1 / math.sqrt((d - j)**2))\n",
    "                    break\n",
    "                d = d + 1\n",
    "\n",
    "            # left-up\n",
    "            dx = i - 1\n",
    "            dy = j - 1\n",
    "            while dx >= 0 and dy >= 0:\n",
    "                if maps[dx][dy] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[dx][dy] - 1)\n",
    "                    weight.append(1 / math.sqrt((dx - i)**2 + (dy - j)**2))\n",
    "                    break\n",
    "                dx = dx - 1\n",
    "                dy = dy - 1\n",
    "            # right-up\n",
    "            dx = i - 1\n",
    "            dy = j + 1\n",
    "            while dx >= 0 and dy <= 8:\n",
    "                if maps[dx][dy] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[dx][dy] - 1)\n",
    "                    weight.append(1 / math.sqrt((dx - i)**2 + (dy - j)**2))\n",
    "                    break\n",
    "                dx = dx - 1\n",
    "                dy = dy + 1\n",
    "\n",
    "            # left-down\n",
    "            dx = i + 1\n",
    "            dy = j - 1\n",
    "            while dx <= 8 and dy >= 0:\n",
    "                if maps[dx][dy] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[dx][dy] - 1)\n",
    "                    weight.append(1 / math.sqrt((dx - i)**2 + (dy - j)**2))\n",
    "                    break\n",
    "                dx = dx + 1\n",
    "                dy = dy - 1\n",
    "\n",
    "            # right-down\n",
    "            dx = i + 1\n",
    "            dy = j + 1\n",
    "            while dx <= 8 and dy <= 8:\n",
    "                if maps[dx][dy] != 0:\n",
    "                    edge_index[0].append(maps[i][j] - 1)\n",
    "                    edge_index[1].append(maps[dx][dy] - 1)\n",
    "                    weight.append(1 / math.sqrt((dx - i)**2 + (dy - j)**2))\n",
    "                    break\n",
    "                dx = dx + 1\n",
    "                dy = dy + 1\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_split(train_ratio=0.7):\n",
    "    load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "    trials = np.load(load_dir + 'trials.npy')\n",
    "    bases = np.load(load_dir + 'bases.npy')\n",
    "    labels = np.load(load_dir + 'labels.npy')\n",
    "    # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "    # 去基线\n",
    "    for i, base in enumerate(bases):\n",
    "        trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "    # 离散化标签\n",
    "    labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "    # 复制标签以对齐样本\n",
    "    labels = np.repeat(labels, 60, axis = 0)\n",
    "    # print(labels.shape)\n",
    "    \n",
    "    shuffle_list = np.arange(trials.shape[0])\n",
    "    np.random.shuffle(shuffle_list)\n",
    "    trials = trials[shuffle_list]\n",
    "    labels = labels[shuffle_list]\n",
    "    \n",
    "    train_features, train_labels = trials[:int(trials.shape[0] * train_ratio)], labels[:int(trials.shape[0] * train_ratio)]\n",
    "    test_features, test_labels = trials[int(trials.shape[0] * train_ratio):], labels[int(trials.shape[0] * train_ratio):]\n",
    "    \n",
    "    save_dir = 'data/data_split/'\n",
    "    np.save(save_dir + 'train_features.npy', train_features)\n",
    "    np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "    np.save(save_dir + 'test_features.npy', test_features)\n",
    "    np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "data_split(train_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    self_loop_only = None\n",
    "    def __init__(self, root, is_train_data, self_loop_only):\n",
    "        self.is_train_data = is_train_data\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        self.self_loop_only = self_loop_only\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['tranin.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        edge_index = get_edge_index(self_loop_only=self.self_loop_only) # 仅有自环，无其他边\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor([labels[i]], dtype=torch.float)\n",
    "            data = Data(x = x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(embed_dim, 128) # SAGEConv\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GCNConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GCNConv(64, 16)\n",
    "        self.pool3 = TopKPooling(16, ratio=0.8)\n",
    "        # self.item_embedding = torch.nn.Embedding(num_embeddings=df.item_id.max() + 10, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(128, 256)\n",
    "        self.lin2 = torch.nn.Linear(256, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # x = self.item_embedding(x)\n",
    "        # x = x.squeeze(1) # n*128\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        # x, edeg_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch) # pool 之后得到 n*128个点\n",
    "        # x1 = gap(x, batch)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edeg_index, _, batch, _, _ = self.pool2(x, edeg_index, None, batch)\n",
    "        # x2 = gap(x, batch)\n",
    "        # x = F.relu(self.conv3(x, edge_index))\n",
    "        # x, edeg_index, _, batch, _, _ = self.pool3(x, edeg_index, None, batch)\n",
    "        # x3 = gap(x, batch)\n",
    "        # x = x1 + x2 + x3 # 获取不同尺度的全局特征\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.resize(batch_size, data.num_nodes // batch_size, data.num_node_features)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        # x = x.resize(batch_size, x.shape[0] * x.shape[1] // batch_size)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        \n",
    "        x = torch.sigmoid(self.lin3(x))\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  1  0 17  0  0  0]\n",
      " [ 0  0  0  2  0 18  0  0  0]\n",
      " [ 4  0  3  0 19  0 20  0 21]\n",
      " [ 0  5  0  6  0 23  0 22  0]\n",
      " [ 8  0  7  0 24  0 25  0 26]\n",
      " [ 0  9  0 10  0 28  0 27  0]\n",
      " [12  0  0 11 16  0 29  0 30]\n",
      " [ 0  0  0 13 15 31  0  0  0]\n",
      " [ 0  0  0 14  0 32  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  1  0 17  0  0  0]\n",
      " [ 0  0  0  2  0 18  0  0  0]\n",
      " [ 4  0  3  0 19  0 20  0 21]\n",
      " [ 0  5  0  6  0 23  0 22  0]\n",
      " [ 8  0  7  0 24  0 25  0 26]\n",
      " [ 0  9  0 10  0 28  0 27  0]\n",
      " [12  0  0 11 16  0 29  0 30]\n",
      " [ 0  0  0 13 15 31  0  0  0]\n",
      " [ 0  0  0 14  0 32  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "self_loop_only=True\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, self_loop_only=self_loop_only)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=32, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, self_loop_only=self_loop_only)\n",
    "testDataLoader = DataLoader(testData, batch_size=23040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "crit = torch.nn.MSELoss()\n",
    "\n",
    "def train(emo_dim, self_loop_only=True):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    acc_all = 0\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        loss_all += batch.num_graphs * loss.item()\n",
    "        opt.step()\n",
    "        num_eq = torch.eq(torch.where(output >=0.5,1, 0).reshape(-1), torch.tensor(batch.y[:, emo_dim], dtype=torch.long)).sum()\n",
    "        acc_all += num_eq\n",
    "        if batch_id % 400 == 0:\n",
    "            acc= num_eq / trainDataLoader.batch_size\n",
    "            print(f'batch_id={batch_id:4d}, loss={loss.item():.6f}, acc={acc:.4f}')\n",
    "    train_loss = loss_all / trainDataLoader.dataset.len()\n",
    "    train_acc = acc_all / trainDataLoader.dataset.len()\n",
    "    \n",
    "    # check测试集的性能\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model.eval()\n",
    "    for batch in testDataLoader: # 我们只有一个batch\n",
    "        output = model(batch)\n",
    "        val_loss = crit(output, batch.y[:, emo_dim])\n",
    "        num_eq = torch.eq(torch.where(output >=0.5,1, 0).reshape(-1), torch.tensor(batch.y[:, emo_dim], dtype=torch.long)).sum()\n",
    "        val_acc = num_eq / testDataLoader.dataset.len()\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:  0\n",
      "batch_id=   0, loss=0.240168, acc=0.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:549: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id= 400, loss=0.255837, acc=0.5000\n",
      "batch_id= 800, loss=0.241796, acc=0.6562\n",
      "batch_id=1200, loss=0.249394, acc=0.5312\n",
      "batch_id=1600, loss=0.248266, acc=0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([23040])) that is different to the input size (torch.Size([23040, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:  0, train_loss=0.247642, train_acc=0.5641, val_loss=0.246269, val_acc=0.5652\n",
      "->epoch:  1\n",
      "batch_id=   0, loss=0.240840, acc=0.6250\n",
      "batch_id= 400, loss=0.240462, acc=0.6250\n",
      "batch_id= 800, loss=0.252825, acc=0.5000\n",
      "batch_id=1200, loss=0.235027, acc=0.6562\n",
      "batch_id=1600, loss=0.251256, acc=0.5312\n",
      "->epoch:  1, train_loss=0.246368, train_acc=0.5657, val_loss=0.246051, val_acc=0.5652\n",
      "->epoch:  2\n",
      "batch_id=   0, loss=0.243565, acc=0.5938\n",
      "batch_id= 400, loss=0.249159, acc=0.5312\n",
      "batch_id= 800, loss=0.257464, acc=0.4688\n",
      "batch_id=1200, loss=0.246650, acc=0.5625\n",
      "batch_id=1600, loss=0.246406, acc=0.5625\n",
      "->epoch:  2, train_loss=0.246101, train_acc=0.5661, val_loss=0.246077, val_acc=0.5654\n",
      "->epoch:  3\n",
      "batch_id=   0, loss=0.246393, acc=0.5625\n",
      "batch_id= 400, loss=0.242355, acc=0.5938\n",
      "batch_id= 800, loss=0.254806, acc=0.5000\n",
      "batch_id=1200, loss=0.260550, acc=0.4375\n",
      "batch_id=1600, loss=0.258067, acc=0.4688\n",
      "->epoch:  3, train_loss=0.246037, train_acc=0.5659, val_loss=0.246070, val_acc=0.5654\n",
      "->epoch:  4\n",
      "batch_id=   0, loss=0.240359, acc=0.6250\n",
      "batch_id= 400, loss=0.242898, acc=0.5938\n",
      "batch_id= 800, loss=0.234599, acc=0.6562\n",
      "batch_id=1200, loss=0.237620, acc=0.6562\n",
      "batch_id=1600, loss=0.225660, acc=0.7188\n",
      "->epoch:  4, train_loss=0.245956, train_acc=0.5658, val_loss=0.245887, val_acc=0.5653\n",
      "->epoch:  5\n",
      "batch_id=   0, loss=0.231954, acc=0.6875\n",
      "batch_id= 400, loss=0.242747, acc=0.5938\n",
      "batch_id= 800, loss=0.258445, acc=0.4688\n",
      "batch_id=1200, loss=0.235156, acc=0.6562\n",
      "batch_id=1600, loss=0.253122, acc=0.5000\n",
      "->epoch:  5, train_loss=0.245918, train_acc=0.5658, val_loss=0.245858, val_acc=0.5653\n",
      "->epoch:  6\n",
      "batch_id=   0, loss=0.242261, acc=0.5938\n",
      "batch_id= 400, loss=0.242937, acc=0.5938\n",
      "batch_id= 800, loss=0.253032, acc=0.5000\n",
      "batch_id=1200, loss=0.249796, acc=0.5312\n",
      "batch_id=1600, loss=0.241641, acc=0.5938\n",
      "->epoch:  6, train_loss=0.245843, train_acc=0.5658, val_loss=0.245898, val_acc=0.5653\n",
      "->epoch:  7\n",
      "batch_id=   0, loss=0.253055, acc=0.5000\n",
      "batch_id= 400, loss=0.241992, acc=0.5938\n",
      "batch_id= 800, loss=0.264639, acc=0.4062\n",
      "batch_id=1200, loss=0.242234, acc=0.5938\n",
      "batch_id=1600, loss=0.242749, acc=0.5938\n",
      "->epoch:  7, train_loss=0.245836, train_acc=0.5658, val_loss=0.245800, val_acc=0.5653\n",
      "->epoch:  8\n",
      "batch_id=   0, loss=0.257523, acc=0.4688\n",
      "batch_id= 400, loss=0.238484, acc=0.6250\n",
      "batch_id= 800, loss=0.253900, acc=0.5000\n",
      "batch_id=1200, loss=0.242133, acc=0.5938\n",
      "batch_id=1600, loss=0.238279, acc=0.6250\n",
      "->epoch:  8, train_loss=0.245789, train_acc=0.5658, val_loss=0.246151, val_acc=0.5653\n",
      "->epoch:  9\n",
      "batch_id=   0, loss=0.236574, acc=0.6250\n",
      "batch_id= 400, loss=0.250125, acc=0.5312\n",
      "batch_id= 800, loss=0.246273, acc=0.5625\n",
      "batch_id=1200, loss=0.242386, acc=0.5938\n",
      "batch_id=1600, loss=0.249543, acc=0.5312\n",
      "->epoch:  9, train_loss=0.245808, train_acc=0.5658, val_loss=0.245784, val_acc=0.5653\n",
      "->epoch: 10\n",
      "batch_id=   0, loss=0.250246, acc=0.5312\n",
      "batch_id= 400, loss=0.230011, acc=0.6875\n",
      "batch_id= 800, loss=0.261858, acc=0.4375\n",
      "batch_id=1200, loss=0.246284, acc=0.5625\n",
      "batch_id=1600, loss=0.231801, acc=0.6875\n",
      "->epoch: 10, train_loss=0.245774, train_acc=0.5658, val_loss=0.245783, val_acc=0.5653\n",
      "->epoch: 11\n",
      "batch_id=   0, loss=0.242421, acc=0.5938\n",
      "batch_id= 400, loss=0.238408, acc=0.6250\n",
      "batch_id= 800, loss=0.237163, acc=0.6562\n",
      "batch_id=1200, loss=0.232258, acc=0.6562\n",
      "batch_id=1600, loss=0.238674, acc=0.6250\n",
      "->epoch: 11, train_loss=0.245735, train_acc=0.5658, val_loss=0.245773, val_acc=0.5653\n",
      "->epoch: 12\n",
      "batch_id=   0, loss=0.250363, acc=0.5312\n",
      "batch_id= 400, loss=0.257059, acc=0.4688\n",
      "batch_id= 800, loss=0.253755, acc=0.5000\n",
      "batch_id=1200, loss=0.239334, acc=0.6250\n",
      "batch_id=1600, loss=0.255326, acc=0.5000\n",
      "->epoch: 12, train_loss=0.245752, train_acc=0.5658, val_loss=0.245861, val_acc=0.5653\n",
      "->epoch: 13\n",
      "batch_id=   0, loss=0.251232, acc=0.5312\n",
      "batch_id= 400, loss=0.242455, acc=0.5938\n",
      "batch_id= 800, loss=0.253777, acc=0.5000\n",
      "batch_id=1200, loss=0.258240, acc=0.4688\n",
      "batch_id=1600, loss=0.230257, acc=0.6875\n",
      "->epoch: 13, train_loss=0.245750, train_acc=0.5658, val_loss=0.245786, val_acc=0.5653\n",
      "->epoch: 14\n",
      "batch_id=   0, loss=0.237419, acc=0.6250\n",
      "batch_id= 400, loss=0.246170, acc=0.5625\n",
      "batch_id= 800, loss=0.256850, acc=0.4688\n",
      "batch_id=1200, loss=0.234358, acc=0.6562\n",
      "batch_id=1600, loss=0.224572, acc=0.7188\n",
      "->epoch: 14, train_loss=0.245734, train_acc=0.5658, val_loss=0.245815, val_acc=0.5653\n",
      "->epoch: 15\n",
      "batch_id=   0, loss=0.246316, acc=0.5625\n",
      "batch_id= 400, loss=0.257701, acc=0.4688\n",
      "batch_id= 800, loss=0.246122, acc=0.5625\n",
      "batch_id=1200, loss=0.242109, acc=0.5938\n",
      "batch_id=1600, loss=0.238361, acc=0.6250\n",
      "->epoch: 15, train_loss=0.245744, train_acc=0.5658, val_loss=0.245765, val_acc=0.5653\n",
      "->epoch: 16\n",
      "batch_id=   0, loss=0.234550, acc=0.6562\n",
      "batch_id= 400, loss=0.250172, acc=0.5312\n",
      "batch_id= 800, loss=0.238242, acc=0.6250\n",
      "batch_id=1200, loss=0.242021, acc=0.5938\n",
      "batch_id=1600, loss=0.242468, acc=0.5938\n",
      "->epoch: 16, train_loss=0.245700, train_acc=0.5658, val_loss=0.245788, val_acc=0.5653\n",
      "->epoch: 17\n",
      "batch_id=   0, loss=0.249809, acc=0.5312\n",
      "batch_id= 400, loss=0.238613, acc=0.6250\n",
      "batch_id= 800, loss=0.258352, acc=0.4688\n",
      "batch_id=1200, loss=0.253908, acc=0.5000\n",
      "batch_id=1600, loss=0.231290, acc=0.6875\n",
      "->epoch: 17, train_loss=0.245730, train_acc=0.5658, val_loss=0.245758, val_acc=0.5653\n",
      "->epoch: 18\n",
      "batch_id=   0, loss=0.246123, acc=0.5625\n",
      "batch_id= 400, loss=0.230106, acc=0.6875\n",
      "batch_id= 800, loss=0.242472, acc=0.5938\n",
      "batch_id=1200, loss=0.261682, acc=0.4375\n",
      "batch_id=1600, loss=0.250333, acc=0.5312\n",
      "->epoch: 18, train_loss=0.245727, train_acc=0.5658, val_loss=0.245760, val_acc=0.5653\n",
      "->epoch: 19\n",
      "batch_id=   0, loss=0.254299, acc=0.5000\n",
      "batch_id= 400, loss=0.254555, acc=0.5000\n",
      "batch_id= 800, loss=0.237720, acc=0.6250\n",
      "batch_id=1200, loss=0.254495, acc=0.5000\n",
      "batch_id=1600, loss=0.253891, acc=0.5000\n",
      "->epoch: 19, train_loss=0.245711, train_acc=0.5658, val_loss=0.245762, val_acc=0.5653\n",
      "->epoch: 20\n",
      "batch_id=   0, loss=0.246116, acc=0.5625\n",
      "batch_id= 400, loss=0.249812, acc=0.5312\n",
      "batch_id= 800, loss=0.238263, acc=0.6250\n",
      "batch_id=1200, loss=0.270166, acc=0.3750\n",
      "batch_id=1600, loss=0.246121, acc=0.5625\n",
      "->epoch: 20, train_loss=0.245712, train_acc=0.5658, val_loss=0.245781, val_acc=0.5653\n",
      "->epoch: 21\n",
      "batch_id=   0, loss=0.233159, acc=0.6562\n",
      "batch_id= 400, loss=0.230341, acc=0.6875\n",
      "batch_id= 800, loss=0.234000, acc=0.6562\n",
      "batch_id=1200, loss=0.257357, acc=0.4688\n",
      "batch_id=1600, loss=0.246136, acc=0.5625\n",
      "->epoch: 21, train_loss=0.245706, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 22\n",
      "batch_id=   0, loss=0.258191, acc=0.4688\n",
      "batch_id= 400, loss=0.230046, acc=0.6875\n",
      "batch_id= 800, loss=0.226482, acc=0.7188\n",
      "batch_id=1200, loss=0.237970, acc=0.6250\n",
      "batch_id=1600, loss=0.262262, acc=0.4375\n",
      "->epoch: 22, train_loss=0.245704, train_acc=0.5658, val_loss=0.245753, val_acc=0.5653\n",
      "->epoch: 23\n",
      "batch_id=   0, loss=0.242253, acc=0.5938\n",
      "batch_id= 400, loss=0.226640, acc=0.7188\n",
      "batch_id= 800, loss=0.242197, acc=0.5938\n",
      "batch_id=1200, loss=0.250683, acc=0.5312\n",
      "batch_id=1600, loss=0.241955, acc=0.5938\n",
      "->epoch: 23, train_loss=0.245711, train_acc=0.5658, val_loss=0.245761, val_acc=0.5653\n",
      "->epoch: 24\n",
      "batch_id=   0, loss=0.238418, acc=0.6250\n",
      "batch_id= 400, loss=0.242051, acc=0.5938\n",
      "batch_id= 800, loss=0.246121, acc=0.5625\n",
      "batch_id=1200, loss=0.230468, acc=0.6875\n",
      "batch_id=1600, loss=0.233962, acc=0.6562\n",
      "->epoch: 24, train_loss=0.245702, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 25\n",
      "batch_id=   0, loss=0.229486, acc=0.6875\n",
      "batch_id= 400, loss=0.253985, acc=0.5000\n",
      "batch_id= 800, loss=0.237972, acc=0.6250\n",
      "batch_id=1200, loss=0.254029, acc=0.5000\n",
      "batch_id=1600, loss=0.250217, acc=0.5312\n",
      "->epoch: 25, train_loss=0.245707, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 26\n",
      "batch_id=   0, loss=0.246151, acc=0.5625\n",
      "batch_id= 400, loss=0.258382, acc=0.4688\n",
      "batch_id= 800, loss=0.237995, acc=0.6250\n",
      "batch_id=1200, loss=0.257778, acc=0.4688\n",
      "batch_id=1600, loss=0.242159, acc=0.5938\n",
      "->epoch: 26, train_loss=0.245705, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 27\n",
      "batch_id=   0, loss=0.261663, acc=0.4375\n",
      "batch_id= 400, loss=0.238208, acc=0.6250\n",
      "batch_id= 800, loss=0.242183, acc=0.5938\n",
      "batch_id=1200, loss=0.254686, acc=0.5000\n",
      "batch_id=1600, loss=0.253916, acc=0.5000\n",
      "->epoch: 27, train_loss=0.245703, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 28\n",
      "batch_id=   0, loss=0.242108, acc=0.5938\n",
      "batch_id= 400, loss=0.250054, acc=0.5312\n",
      "batch_id= 800, loss=0.250325, acc=0.5312\n",
      "batch_id=1200, loss=0.250207, acc=0.5312\n",
      "batch_id=1600, loss=0.254279, acc=0.5000\n",
      "->epoch: 28, train_loss=0.245691, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 29\n",
      "batch_id=   0, loss=0.233653, acc=0.6562\n",
      "batch_id= 400, loss=0.246115, acc=0.5625\n",
      "batch_id= 800, loss=0.250050, acc=0.5312\n",
      "batch_id=1200, loss=0.230063, acc=0.6875\n",
      "batch_id=1600, loss=0.250106, acc=0.5312\n",
      "->epoch: 29, train_loss=0.245688, train_acc=0.5658, val_loss=0.245759, val_acc=0.5653\n",
      "->epoch: 30\n",
      "batch_id=   0, loss=0.254743, acc=0.5000\n",
      "batch_id= 400, loss=0.242125, acc=0.5938\n",
      "batch_id= 800, loss=0.246118, acc=0.5625\n",
      "batch_id=1200, loss=0.241956, acc=0.5938\n",
      "batch_id=1600, loss=0.233844, acc=0.6562\n",
      "->epoch: 30, train_loss=0.245693, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 31\n",
      "batch_id=   0, loss=0.254592, acc=0.5000\n",
      "batch_id= 400, loss=0.237278, acc=0.6250\n",
      "batch_id= 800, loss=0.255200, acc=0.5000\n",
      "batch_id=1200, loss=0.257713, acc=0.4688\n",
      "batch_id=1600, loss=0.246114, acc=0.5625\n",
      "->epoch: 31, train_loss=0.245693, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 32\n",
      "batch_id=   0, loss=0.242115, acc=0.5938\n",
      "batch_id= 400, loss=0.233814, acc=0.6562\n",
      "batch_id= 800, loss=0.238158, acc=0.6250\n",
      "batch_id=1200, loss=0.242125, acc=0.5938\n",
      "batch_id=1600, loss=0.246138, acc=0.5625\n",
      "->epoch: 32, train_loss=0.245696, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 33\n",
      "batch_id=   0, loss=0.214392, acc=0.8125\n",
      "batch_id= 400, loss=0.242092, acc=0.5938\n",
      "batch_id= 800, loss=0.234411, acc=0.6562\n",
      "batch_id=1200, loss=0.234328, acc=0.6562\n",
      "batch_id=1600, loss=0.255011, acc=0.5000\n",
      "->epoch: 33, train_loss=0.245696, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 34\n",
      "batch_id=   0, loss=0.250174, acc=0.5312\n",
      "batch_id= 400, loss=0.246114, acc=0.5625\n",
      "batch_id= 800, loss=0.250106, acc=0.5312\n",
      "batch_id=1200, loss=0.225380, acc=0.7188\n",
      "batch_id=1600, loss=0.254036, acc=0.5000\n",
      "->epoch: 34, train_loss=0.245686, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 35\n",
      "batch_id=   0, loss=0.250180, acc=0.5312\n",
      "batch_id= 400, loss=0.237970, acc=0.6250\n",
      "batch_id= 800, loss=0.258813, acc=0.4688\n",
      "batch_id=1200, loss=0.274612, acc=0.3438\n",
      "batch_id=1600, loss=0.246118, acc=0.5625\n",
      "->epoch: 35, train_loss=0.245684, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 36\n",
      "batch_id=   0, loss=0.261705, acc=0.4375\n",
      "batch_id= 400, loss=0.234869, acc=0.6562\n",
      "batch_id= 800, loss=0.242192, acc=0.5938\n",
      "batch_id=1200, loss=0.233959, acc=0.6562\n",
      "batch_id=1600, loss=0.241771, acc=0.5938\n",
      "->epoch: 36, train_loss=0.245678, train_acc=0.5658, val_loss=0.245779, val_acc=0.5653\n",
      "->epoch: 37\n",
      "batch_id=   0, loss=0.246300, acc=0.5625\n",
      "batch_id= 400, loss=0.242077, acc=0.5938\n",
      "batch_id= 800, loss=0.250348, acc=0.5312\n",
      "batch_id=1200, loss=0.258523, acc=0.4688\n",
      "batch_id=1600, loss=0.262297, acc=0.4375\n",
      "->epoch: 37, train_loss=0.245699, train_acc=0.5658, val_loss=0.245748, val_acc=0.5653\n",
      "->epoch: 38\n",
      "batch_id=   0, loss=0.266020, acc=0.4062\n",
      "batch_id= 400, loss=0.261718, acc=0.4375\n",
      "batch_id= 800, loss=0.250073, acc=0.5312\n",
      "batch_id=1200, loss=0.238155, acc=0.6250\n",
      "batch_id=1600, loss=0.229157, acc=0.6875\n",
      "->epoch: 38, train_loss=0.245686, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 39\n",
      "batch_id=   0, loss=0.237907, acc=0.6250\n",
      "batch_id= 400, loss=0.229213, acc=0.6875\n",
      "batch_id= 800, loss=0.250018, acc=0.5312\n",
      "batch_id=1200, loss=0.246183, acc=0.5625\n",
      "batch_id=1600, loss=0.254068, acc=0.5000\n",
      "->epoch: 39, train_loss=0.245684, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 40\n",
      "batch_id=   0, loss=0.242122, acc=0.5938\n",
      "batch_id= 400, loss=0.254100, acc=0.5000\n",
      "batch_id= 800, loss=0.250338, acc=0.5312\n",
      "batch_id=1200, loss=0.258066, acc=0.4688\n",
      "batch_id=1600, loss=0.242228, acc=0.5938\n",
      "->epoch: 40, train_loss=0.245684, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 41\n",
      "batch_id=   0, loss=0.257923, acc=0.4688\n",
      "batch_id= 400, loss=0.233121, acc=0.6562\n",
      "batch_id= 800, loss=0.250078, acc=0.5312\n",
      "batch_id=1200, loss=0.250051, acc=0.5312\n",
      "batch_id=1600, loss=0.254033, acc=0.5000\n",
      "->epoch: 41, train_loss=0.245685, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 42\n",
      "batch_id=   0, loss=0.242002, acc=0.5938\n",
      "batch_id= 400, loss=0.250415, acc=0.5312\n",
      "batch_id= 800, loss=0.237464, acc=0.6250\n",
      "batch_id=1200, loss=0.229871, acc=0.6875\n",
      "batch_id=1600, loss=0.242140, acc=0.5938\n",
      "->epoch: 42, train_loss=0.245681, train_acc=0.5658, val_loss=0.245757, val_acc=0.5653\n",
      "->epoch: 43\n",
      "batch_id=   0, loss=0.249976, acc=0.5312\n",
      "batch_id= 400, loss=0.229914, acc=0.6875\n",
      "batch_id= 800, loss=0.234226, acc=0.6562\n",
      "batch_id=1200, loss=0.254437, acc=0.5000\n",
      "batch_id=1600, loss=0.238022, acc=0.6250\n",
      "->epoch: 43, train_loss=0.245683, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 44\n",
      "batch_id=   0, loss=0.222104, acc=0.7500\n",
      "batch_id= 400, loss=0.234552, acc=0.6562\n",
      "batch_id= 800, loss=0.246193, acc=0.5625\n",
      "batch_id=1200, loss=0.229552, acc=0.6875\n",
      "batch_id=1600, loss=0.246110, acc=0.5625\n",
      "->epoch: 44, train_loss=0.245676, train_acc=0.5658, val_loss=0.245757, val_acc=0.5653\n",
      "->epoch: 45\n",
      "batch_id=   0, loss=0.249955, acc=0.5312\n",
      "batch_id= 400, loss=0.262044, acc=0.4375\n",
      "batch_id= 800, loss=0.222833, acc=0.7500\n",
      "batch_id=1200, loss=0.238277, acc=0.6250\n",
      "batch_id=1600, loss=0.237864, acc=0.6250\n",
      "->epoch: 45, train_loss=0.245680, train_acc=0.5658, val_loss=0.245757, val_acc=0.5653\n",
      "->epoch: 46\n",
      "batch_id=   0, loss=0.250371, acc=0.5312\n",
      "batch_id= 400, loss=0.246123, acc=0.5625\n",
      "batch_id= 800, loss=0.238557, acc=0.6250\n",
      "batch_id=1200, loss=0.238050, acc=0.6250\n",
      "batch_id=1600, loss=0.253862, acc=0.5000\n",
      "->epoch: 46, train_loss=0.245682, train_acc=0.5658, val_loss=0.245752, val_acc=0.5653\n",
      "->epoch: 47\n",
      "batch_id=   0, loss=0.220918, acc=0.7500\n",
      "batch_id= 400, loss=0.250264, acc=0.5312\n",
      "batch_id= 800, loss=0.233963, acc=0.6562\n",
      "batch_id=1200, loss=0.250206, acc=0.5312\n",
      "batch_id=1600, loss=0.246105, acc=0.5625\n",
      "->epoch: 47, train_loss=0.245687, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 48\n",
      "batch_id=   0, loss=0.246116, acc=0.5625\n",
      "batch_id= 400, loss=0.250206, acc=0.5312\n",
      "batch_id= 800, loss=0.246110, acc=0.5625\n",
      "batch_id=1200, loss=0.238300, acc=0.6250\n",
      "batch_id=1600, loss=0.246121, acc=0.5625\n",
      "->epoch: 48, train_loss=0.245680, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 49\n",
      "batch_id=   0, loss=0.254429, acc=0.5000\n",
      "batch_id= 400, loss=0.262823, acc=0.4375\n",
      "batch_id= 800, loss=0.246129, acc=0.5625\n",
      "batch_id=1200, loss=0.231097, acc=0.6875\n",
      "batch_id=1600, loss=0.250604, acc=0.5312\n",
      "->epoch: 49, train_loss=0.245676, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n",
      "->epoch: 50\n",
      "batch_id=   0, loss=0.237795, acc=0.6250\n",
      "batch_id= 400, loss=0.237741, acc=0.6250\n",
      "batch_id= 800, loss=0.271278, acc=0.3750\n",
      "batch_id=1200, loss=0.246128, acc=0.5625\n",
      "batch_id=1600, loss=0.233954, acc=0.6562\n",
      "->epoch: 50, train_loss=0.245678, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 51\n",
      "batch_id=   0, loss=0.250062, acc=0.5312\n",
      "batch_id= 400, loss=0.253970, acc=0.5000\n",
      "batch_id= 800, loss=0.230088, acc=0.6875\n",
      "batch_id=1200, loss=0.262315, acc=0.4375\n",
      "batch_id=1600, loss=0.246122, acc=0.5625\n",
      "->epoch: 51, train_loss=0.245676, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 52\n",
      "batch_id=   0, loss=0.242141, acc=0.5938\n",
      "batch_id= 400, loss=0.237829, acc=0.6250\n",
      "batch_id= 800, loss=0.246127, acc=0.5625\n",
      "batch_id=1200, loss=0.242015, acc=0.5938\n",
      "batch_id=1600, loss=0.229806, acc=0.6875\n",
      "->epoch: 52, train_loss=0.245676, train_acc=0.5658, val_loss=0.245757, val_acc=0.5653\n",
      "->epoch: 53\n",
      "batch_id=   0, loss=0.246123, acc=0.5625\n",
      "batch_id= 400, loss=0.233359, acc=0.6562\n",
      "batch_id= 800, loss=0.238039, acc=0.6250\n",
      "batch_id=1200, loss=0.258623, acc=0.4688\n",
      "batch_id=1600, loss=0.246114, acc=0.5625\n",
      "->epoch: 53, train_loss=0.245681, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 54\n",
      "batch_id=   0, loss=0.226403, acc=0.7188\n",
      "batch_id= 400, loss=0.254331, acc=0.5000\n",
      "batch_id= 800, loss=0.246120, acc=0.5625\n",
      "batch_id=1200, loss=0.250101, acc=0.5312\n",
      "batch_id=1600, loss=0.237703, acc=0.6250\n",
      "->epoch: 54, train_loss=0.245677, train_acc=0.5658, val_loss=0.245749, val_acc=0.5653\n",
      "->epoch: 55\n",
      "batch_id=   0, loss=0.266101, acc=0.4062\n",
      "batch_id= 400, loss=0.250206, acc=0.5312\n",
      "batch_id= 800, loss=0.253992, acc=0.5000\n",
      "batch_id=1200, loss=0.225977, acc=0.7188\n",
      "batch_id=1600, loss=0.246134, acc=0.5625\n",
      "->epoch: 55, train_loss=0.245679, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 56\n",
      "batch_id=   0, loss=0.229177, acc=0.6875\n",
      "batch_id= 400, loss=0.250243, acc=0.5312\n",
      "batch_id= 800, loss=0.230206, acc=0.6875\n",
      "batch_id=1200, loss=0.246117, acc=0.5625\n",
      "batch_id=1600, loss=0.233342, acc=0.6562\n",
      "->epoch: 56, train_loss=0.245680, train_acc=0.5658, val_loss=0.245762, val_acc=0.5653\n",
      "->epoch: 57\n",
      "batch_id=   0, loss=0.241807, acc=0.5938\n",
      "batch_id= 400, loss=0.250202, acc=0.5312\n",
      "batch_id= 800, loss=0.274634, acc=0.3438\n",
      "batch_id=1200, loss=0.246118, acc=0.5625\n",
      "batch_id=1600, loss=0.274074, acc=0.3438\n",
      "->epoch: 57, train_loss=0.245680, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 58\n",
      "batch_id=   0, loss=0.258222, acc=0.4688\n",
      "batch_id= 400, loss=0.250028, acc=0.5312\n",
      "batch_id= 800, loss=0.228706, acc=0.6875\n",
      "batch_id=1200, loss=0.246124, acc=0.5625\n",
      "batch_id=1600, loss=0.250143, acc=0.5312\n",
      "->epoch: 58, train_loss=0.245667, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n",
      "->epoch: 59\n",
      "batch_id=   0, loss=0.250056, acc=0.5312\n",
      "batch_id= 400, loss=0.226494, acc=0.7188\n",
      "batch_id= 800, loss=0.246113, acc=0.5625\n",
      "batch_id=1200, loss=0.234064, acc=0.6562\n",
      "batch_id=1600, loss=0.250219, acc=0.5312\n",
      "->epoch: 59, train_loss=0.245665, train_acc=0.5658, val_loss=0.245753, val_acc=0.5653\n",
      "->epoch: 60\n",
      "batch_id=   0, loss=0.220938, acc=0.7500\n",
      "batch_id= 400, loss=0.242077, acc=0.5938\n",
      "batch_id= 800, loss=0.246111, acc=0.5625\n",
      "batch_id=1200, loss=0.242062, acc=0.5938\n",
      "batch_id=1600, loss=0.221011, acc=0.7500\n",
      "->epoch: 60, train_loss=0.245673, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 61\n",
      "batch_id=   0, loss=0.246130, acc=0.5625\n",
      "batch_id= 400, loss=0.221642, acc=0.7500\n",
      "batch_id= 800, loss=0.246152, acc=0.5625\n",
      "batch_id=1200, loss=0.277249, acc=0.3125\n",
      "batch_id=1600, loss=0.250023, acc=0.5312\n",
      "->epoch: 61, train_loss=0.245677, train_acc=0.5658, val_loss=0.245753, val_acc=0.5653\n",
      "->epoch: 62\n",
      "batch_id=   0, loss=0.234273, acc=0.6562\n",
      "batch_id= 400, loss=0.242182, acc=0.5938\n",
      "batch_id= 800, loss=0.241939, acc=0.5938\n",
      "batch_id=1200, loss=0.254134, acc=0.5000\n",
      "batch_id=1600, loss=0.254242, acc=0.5000\n",
      "->epoch: 62, train_loss=0.245672, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 63\n",
      "batch_id=   0, loss=0.246120, acc=0.5625\n",
      "batch_id= 400, loss=0.257911, acc=0.4688\n",
      "batch_id= 800, loss=0.242190, acc=0.5938\n",
      "batch_id=1200, loss=0.229700, acc=0.6875\n",
      "batch_id=1600, loss=0.233640, acc=0.6562\n",
      "->epoch: 63, train_loss=0.245670, train_acc=0.5658, val_loss=0.245758, val_acc=0.5653\n",
      "->epoch: 64\n",
      "batch_id=   0, loss=0.233391, acc=0.6562\n",
      "batch_id= 400, loss=0.246107, acc=0.5625\n",
      "batch_id= 800, loss=0.233510, acc=0.6562\n",
      "batch_id=1200, loss=0.258512, acc=0.4688\n",
      "batch_id=1600, loss=0.222405, acc=0.7500\n",
      "->epoch: 64, train_loss=0.245678, train_acc=0.5658, val_loss=0.245750, val_acc=0.5653\n",
      "->epoch: 65\n",
      "batch_id=   0, loss=0.258456, acc=0.4688\n",
      "batch_id= 400, loss=0.254276, acc=0.5000\n",
      "batch_id= 800, loss=0.250190, acc=0.5312\n",
      "batch_id=1200, loss=0.230198, acc=0.6875\n",
      "batch_id=1600, loss=0.233996, acc=0.6562\n",
      "->epoch: 65, train_loss=0.245671, train_acc=0.5658, val_loss=0.245753, val_acc=0.5653\n",
      "->epoch: 66\n",
      "batch_id=   0, loss=0.258109, acc=0.4688\n",
      "batch_id= 400, loss=0.238407, acc=0.6250\n",
      "batch_id= 800, loss=0.229567, acc=0.6875\n",
      "batch_id=1200, loss=0.229355, acc=0.6875\n",
      "batch_id=1600, loss=0.230473, acc=0.6875\n",
      "->epoch: 66, train_loss=0.245673, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 67\n",
      "batch_id=   0, loss=0.261872, acc=0.4375\n",
      "batch_id= 400, loss=0.242223, acc=0.5938\n",
      "batch_id= 800, loss=0.261753, acc=0.4375\n",
      "batch_id=1200, loss=0.242063, acc=0.5938\n",
      "batch_id=1600, loss=0.242015, acc=0.5938\n",
      "->epoch: 67, train_loss=0.245669, train_acc=0.5658, val_loss=0.245752, val_acc=0.5653\n",
      "->epoch: 68\n",
      "batch_id=   0, loss=0.233665, acc=0.6562\n",
      "batch_id= 400, loss=0.228955, acc=0.6875\n",
      "batch_id= 800, loss=0.250274, acc=0.5312\n",
      "batch_id=1200, loss=0.230613, acc=0.6875\n",
      "batch_id=1600, loss=0.246115, acc=0.5625\n",
      "->epoch: 68, train_loss=0.245667, train_acc=0.5658, val_loss=0.245752, val_acc=0.5653\n",
      "->epoch: 69\n",
      "batch_id=   0, loss=0.254417, acc=0.5000\n",
      "batch_id= 400, loss=0.237759, acc=0.6250\n",
      "batch_id= 800, loss=0.246113, acc=0.5625\n",
      "batch_id=1200, loss=0.241956, acc=0.5938\n",
      "batch_id=1600, loss=0.233902, acc=0.6562\n",
      "->epoch: 69, train_loss=0.245674, train_acc=0.5658, val_loss=0.245751, val_acc=0.5653\n",
      "->epoch: 70\n",
      "batch_id=   0, loss=0.254169, acc=0.5000\n",
      "batch_id= 400, loss=0.250200, acc=0.5312\n",
      "batch_id= 800, loss=0.230121, acc=0.6875\n",
      "batch_id=1200, loss=0.258867, acc=0.4688\n",
      "batch_id=1600, loss=0.242066, acc=0.5938\n",
      "->epoch: 70, train_loss=0.245664, train_acc=0.5658, val_loss=0.245752, val_acc=0.5653\n",
      "->epoch: 71\n",
      "batch_id=   0, loss=0.237929, acc=0.6250\n",
      "batch_id= 400, loss=0.250226, acc=0.5312\n",
      "batch_id= 800, loss=0.266272, acc=0.4062\n",
      "batch_id=1200, loss=0.262381, acc=0.4375\n",
      "batch_id=1600, loss=0.229482, acc=0.6875\n",
      "->epoch: 71, train_loss=0.245674, train_acc=0.5658, val_loss=0.245752, val_acc=0.5653\n",
      "->epoch: 72\n",
      "batch_id=   0, loss=0.242054, acc=0.5938\n",
      "batch_id= 400, loss=0.246119, acc=0.5625\n",
      "batch_id= 800, loss=0.237923, acc=0.6250\n",
      "batch_id=1200, loss=0.246112, acc=0.5625\n",
      "batch_id=1600, loss=0.242068, acc=0.5938\n",
      "->epoch: 72, train_loss=0.245670, train_acc=0.5658, val_loss=0.245755, val_acc=0.5653\n",
      "->epoch: 73\n",
      "batch_id=   0, loss=0.212736, acc=0.8125\n",
      "batch_id= 400, loss=0.233481, acc=0.6562\n",
      "batch_id= 800, loss=0.229383, acc=0.6875\n",
      "batch_id=1200, loss=0.263046, acc=0.4375\n",
      "batch_id=1600, loss=0.250141, acc=0.5312\n",
      "->epoch: 73, train_loss=0.245666, train_acc=0.5658, val_loss=0.245753, val_acc=0.5653\n",
      "->epoch: 74\n",
      "batch_id=   0, loss=0.246127, acc=0.5625\n",
      "batch_id= 400, loss=0.246106, acc=0.5625\n",
      "batch_id= 800, loss=0.262485, acc=0.4375\n",
      "batch_id=1200, loss=0.233334, acc=0.6562\n",
      "batch_id=1600, loss=0.237607, acc=0.6250\n",
      "->epoch: 74, train_loss=0.245657, train_acc=0.5658, val_loss=0.245759, val_acc=0.5653\n",
      "->epoch: 75\n",
      "batch_id=   0, loss=0.241943, acc=0.5938\n",
      "batch_id= 400, loss=0.224900, acc=0.7188\n",
      "batch_id= 800, loss=0.241953, acc=0.5938\n",
      "batch_id=1200, loss=0.233183, acc=0.6562\n",
      "batch_id=1600, loss=0.238110, acc=0.6250\n",
      "->epoch: 75, train_loss=0.245669, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 76\n",
      "batch_id=   0, loss=0.246117, acc=0.5625\n",
      "batch_id= 400, loss=0.233789, acc=0.6562\n",
      "batch_id= 800, loss=0.241821, acc=0.5938\n",
      "batch_id=1200, loss=0.262429, acc=0.4375\n",
      "batch_id=1600, loss=0.233778, acc=0.6562\n",
      "->epoch: 76, train_loss=0.245668, train_acc=0.5658, val_loss=0.245755, val_acc=0.5653\n",
      "->epoch: 77\n",
      "batch_id=   0, loss=0.230101, acc=0.6875\n",
      "batch_id= 400, loss=0.234263, acc=0.6562\n",
      "batch_id= 800, loss=0.249936, acc=0.5312\n",
      "batch_id=1200, loss=0.225542, acc=0.7188\n",
      "batch_id=1600, loss=0.263004, acc=0.4375\n",
      "->epoch: 77, train_loss=0.245665, train_acc=0.5658, val_loss=0.245761, val_acc=0.5653\n",
      "->epoch: 78\n",
      "batch_id=   0, loss=0.237694, acc=0.6250\n",
      "batch_id= 400, loss=0.258161, acc=0.4688\n",
      "batch_id= 800, loss=0.242139, acc=0.5938\n",
      "batch_id=1200, loss=0.238012, acc=0.6250\n",
      "batch_id=1600, loss=0.254450, acc=0.5000\n",
      "->epoch: 78, train_loss=0.245661, train_acc=0.5658, val_loss=0.245761, val_acc=0.5653\n",
      "->epoch: 79\n",
      "batch_id=   0, loss=0.266994, acc=0.4062\n",
      "batch_id= 400, loss=0.246132, acc=0.5625\n",
      "batch_id= 800, loss=0.238129, acc=0.6250\n",
      "batch_id=1200, loss=0.257978, acc=0.4688\n",
      "batch_id=1600, loss=0.246150, acc=0.5625\n",
      "->epoch: 79, train_loss=0.245665, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n",
      "->epoch: 80\n",
      "batch_id=   0, loss=0.237880, acc=0.6250\n",
      "batch_id= 400, loss=0.246117, acc=0.5625\n",
      "batch_id= 800, loss=0.250304, acc=0.5312\n",
      "batch_id=1200, loss=0.208440, acc=0.8438\n",
      "batch_id=1600, loss=0.237906, acc=0.6250\n",
      "->epoch: 80, train_loss=0.245669, train_acc=0.5659, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 81\n",
      "batch_id=   0, loss=0.242013, acc=0.5938\n",
      "batch_id= 400, loss=0.246106, acc=0.5625\n",
      "batch_id= 800, loss=0.230371, acc=0.6875\n",
      "batch_id=1200, loss=0.250176, acc=0.5312\n",
      "batch_id=1600, loss=0.241930, acc=0.5938\n",
      "->epoch: 81, train_loss=0.245662, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 82\n",
      "batch_id=   0, loss=0.237837, acc=0.6250\n",
      "batch_id= 400, loss=0.242022, acc=0.5938\n",
      "batch_id= 800, loss=0.246131, acc=0.5625\n",
      "batch_id=1200, loss=0.254251, acc=0.5000\n",
      "batch_id=1600, loss=0.225479, acc=0.7188\n",
      "->epoch: 82, train_loss=0.245668, train_acc=0.5658, val_loss=0.245754, val_acc=0.5653\n",
      "->epoch: 83\n",
      "batch_id=   0, loss=0.242076, acc=0.5938\n",
      "batch_id= 400, loss=0.241953, acc=0.5938\n",
      "batch_id= 800, loss=0.246130, acc=0.5625\n",
      "batch_id=1200, loss=0.225553, acc=0.7188\n",
      "batch_id=1600, loss=0.222422, acc=0.7500\n",
      "->epoch: 83, train_loss=0.245662, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n",
      "->epoch: 84\n",
      "batch_id=   0, loss=0.246115, acc=0.5625\n",
      "batch_id= 400, loss=0.254277, acc=0.5000\n",
      "batch_id= 800, loss=0.276588, acc=0.3125\n",
      "batch_id=1200, loss=0.254308, acc=0.5000\n",
      "batch_id=1600, loss=0.254206, acc=0.5000\n",
      "->epoch: 84, train_loss=0.245667, train_acc=0.5658, val_loss=0.245755, val_acc=0.5653\n",
      "->epoch: 85\n",
      "batch_id=   0, loss=0.254257, acc=0.5000\n",
      "batch_id= 400, loss=0.233715, acc=0.6562\n",
      "batch_id= 800, loss=0.234600, acc=0.6562\n",
      "batch_id=1200, loss=0.233585, acc=0.6562\n",
      "batch_id=1600, loss=0.242113, acc=0.5938\n",
      "->epoch: 85, train_loss=0.245672, train_acc=0.5658, val_loss=0.245758, val_acc=0.5653\n",
      "->epoch: 86\n",
      "batch_id=   0, loss=0.250281, acc=0.5312\n",
      "batch_id= 400, loss=0.237804, acc=0.6250\n",
      "batch_id= 800, loss=0.238073, acc=0.6250\n",
      "batch_id=1200, loss=0.254534, acc=0.5000\n",
      "batch_id=1600, loss=0.246126, acc=0.5625\n",
      "->epoch: 86, train_loss=0.245660, train_acc=0.5658, val_loss=0.245755, val_acc=0.5653\n",
      "->epoch: 87\n",
      "batch_id=   0, loss=0.229672, acc=0.6875\n",
      "batch_id= 400, loss=0.208316, acc=0.8438\n",
      "batch_id= 800, loss=0.238171, acc=0.6250\n",
      "batch_id=1200, loss=0.229723, acc=0.6875\n",
      "batch_id=1600, loss=0.233685, acc=0.6562\n",
      "->epoch: 87, train_loss=0.245660, train_acc=0.5658, val_loss=0.245766, val_acc=0.5653\n",
      "->epoch: 88\n",
      "batch_id=   0, loss=0.250370, acc=0.5312\n",
      "batch_id= 400, loss=0.258136, acc=0.4688\n",
      "batch_id= 800, loss=0.233774, acc=0.6562\n",
      "batch_id=1200, loss=0.221604, acc=0.7500\n",
      "batch_id=1600, loss=0.250203, acc=0.5312\n",
      "->epoch: 88, train_loss=0.245660, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n",
      "->epoch: 89\n",
      "batch_id=   0, loss=0.238492, acc=0.6250\n",
      "batch_id= 400, loss=0.241946, acc=0.5938\n",
      "batch_id= 800, loss=0.250273, acc=0.5312\n",
      "batch_id=1200, loss=0.234482, acc=0.6562\n",
      "batch_id=1600, loss=0.274436, acc=0.3438\n",
      "->epoch: 89, train_loss=0.245663, train_acc=0.5658, val_loss=0.245755, val_acc=0.5653\n",
      "->epoch: 90\n",
      "batch_id=   0, loss=0.242021, acc=0.5938\n",
      "batch_id= 400, loss=0.222760, acc=0.7500\n",
      "batch_id= 800, loss=0.246111, acc=0.5625\n",
      "batch_id=1200, loss=0.250222, acc=0.5312\n",
      "batch_id=1600, loss=0.241915, acc=0.5938\n",
      "->epoch: 90, train_loss=0.245657, train_acc=0.5658, val_loss=0.245765, val_acc=0.5653\n",
      "->epoch: 91\n",
      "batch_id=   0, loss=0.254629, acc=0.5000\n",
      "batch_id= 400, loss=0.241862, acc=0.5938\n",
      "batch_id= 800, loss=0.237781, acc=0.6250\n",
      "batch_id=1200, loss=0.246112, acc=0.5625\n",
      "batch_id=1600, loss=0.224407, acc=0.7188\n",
      "->epoch: 91, train_loss=0.245659, train_acc=0.5658, val_loss=0.245758, val_acc=0.5653\n",
      "->epoch: 92\n",
      "batch_id=   0, loss=0.254025, acc=0.5000\n",
      "batch_id= 400, loss=0.254539, acc=0.5000\n",
      "batch_id= 800, loss=0.233151, acc=0.6562\n",
      "batch_id=1200, loss=0.250415, acc=0.5312\n",
      "batch_id=1600, loss=0.246108, acc=0.5625\n",
      "->epoch: 92, train_loss=0.245665, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n",
      "->epoch: 93\n",
      "batch_id=   0, loss=0.250221, acc=0.5312\n",
      "batch_id= 400, loss=0.263543, acc=0.4375\n",
      "batch_id= 800, loss=0.258275, acc=0.4688\n",
      "batch_id=1200, loss=0.241897, acc=0.5938\n",
      "batch_id=1600, loss=0.246115, acc=0.5625\n",
      "->epoch: 93, train_loss=0.245653, train_acc=0.5658, val_loss=0.245757, val_acc=0.5653\n",
      "->epoch: 94\n",
      "batch_id=   0, loss=0.257800, acc=0.4688\n",
      "batch_id= 400, loss=0.246125, acc=0.5625\n",
      "batch_id= 800, loss=0.250238, acc=0.5312\n",
      "batch_id=1200, loss=0.253835, acc=0.5000\n",
      "batch_id=1600, loss=0.242092, acc=0.5938\n",
      "->epoch: 94, train_loss=0.245664, train_acc=0.5658, val_loss=0.245759, val_acc=0.5653\n",
      "->epoch: 95\n",
      "batch_id=   0, loss=0.246120, acc=0.5625\n",
      "batch_id= 400, loss=0.246135, acc=0.5625\n",
      "batch_id= 800, loss=0.250189, acc=0.5312\n",
      "batch_id=1200, loss=0.242188, acc=0.5938\n",
      "batch_id=1600, loss=0.246131, acc=0.5625\n",
      "->epoch: 95, train_loss=0.245660, train_acc=0.5658, val_loss=0.245760, val_acc=0.5653\n",
      "->epoch: 96\n",
      "batch_id=   0, loss=0.237701, acc=0.6250\n",
      "batch_id= 400, loss=0.250276, acc=0.5312\n",
      "batch_id= 800, loss=0.242048, acc=0.5938\n",
      "batch_id=1200, loss=0.246134, acc=0.5625\n",
      "batch_id=1600, loss=0.233175, acc=0.6562\n",
      "->epoch: 96, train_loss=0.245660, train_acc=0.5658, val_loss=0.245759, val_acc=0.5653\n",
      "->epoch: 97\n",
      "batch_id=   0, loss=0.250251, acc=0.5312\n",
      "batch_id= 400, loss=0.259401, acc=0.4688\n",
      "batch_id= 800, loss=0.242055, acc=0.5938\n",
      "batch_id=1200, loss=0.265985, acc=0.4062\n",
      "batch_id=1600, loss=0.253926, acc=0.5000\n",
      "->epoch: 97, train_loss=0.245655, train_acc=0.5658, val_loss=0.245755, val_acc=0.5653\n",
      "->epoch: 98\n",
      "batch_id=   0, loss=0.254162, acc=0.5000\n",
      "batch_id= 400, loss=0.237866, acc=0.6250\n",
      "batch_id= 800, loss=0.254314, acc=0.5000\n",
      "batch_id=1200, loss=0.246127, acc=0.5625\n",
      "batch_id=1600, loss=0.262862, acc=0.4375\n",
      "->epoch: 98, train_loss=0.245661, train_acc=0.5658, val_loss=0.245758, val_acc=0.5653\n",
      "->epoch: 99\n",
      "batch_id=   0, loss=0.246128, acc=0.5625\n",
      "batch_id= 400, loss=0.229145, acc=0.6875\n",
      "batch_id= 800, loss=0.246177, acc=0.5625\n",
      "batch_id=1200, loss=0.250050, acc=0.5312\n",
      "batch_id=1600, loss=0.253889, acc=0.5000\n",
      "->epoch: 99, train_loss=0.245655, train_acc=0.5658, val_loss=0.245756, val_acc=0.5653\n"
     ]
    }
   ],
   "source": [
    "emo_dim = 0\n",
    "self_loop_only = True\n",
    "for epoch in range(100):\n",
    "    print(f'->epoch:{epoch:3d}')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim, self_loop_only)\n",
    "    print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cae90-4a7e-4c67-919c-c4c4e0a60255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
