{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pywt import wavedec\n",
    "import pandas as pd\n",
    "from scipy.fftpack import fft,ifft\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import joblib\n",
    "import pywt.data\n",
    "import pickle as pickle\n",
    "import random\n",
    "import math\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minepy import MINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieList=['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39']\n",
    "movieList_int=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39]\n",
    "channel=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]#前32个通道为脑电信号\n",
    "subjectList= ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "#subjectList= ['01','02','03','04','05','06','07','08']\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 128 #Averaging band power of 2 sec\n",
    "step_size = 128 #Each 0.125 sec update once\n",
    "sample_rate = 128 #Sampling rate of 128 Hz\n",
    "movielenth=7680\n",
    "piecesNumber=((movielenth-window_size)/step_size)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_power(X, Band, Fs):\n",
    "    \n",
    "\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    Power = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        Power[Freq_Index] = sum(\n",
    "            C[int(np.floor(Freq / Fs * len(X))): \n",
    "                int(np.floor(Next_Freq / Fs * len(X)))]\n",
    "        )\n",
    "    Power_Ratio = Power / sum(Power)\n",
    "    return Power, Power_Ratio\n",
    "\n",
    "def FFT_Processing_subbasefeature_selectchannel_firstO(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for t in range(3):\n",
    "                    tmp = 0\n",
    "                    bas=base[t]\n",
    "                    for k in range(0,len(bas)-1):\n",
    "                        tmp = tmp + abs(bas[k+1] - bas[k] )\n",
    "                    Y = tmp / (len(bas)-1)\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    tmp = 0\n",
    "                    for k in range(0,len(X)-1):\n",
    "                        tmp = tmp + abs(X[k+1] - X[k] )\n",
    "                    Y = tmp / (len(X)-1)\n",
    "                    meta_data = meta_data + list(([Y]-allChannelbase[num]))\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "        \n",
    "\n",
    "def FFT_Processing_subbasefeature_selectchannel_kurt(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    tmp = 0\n",
    "                    Y=pd.Series(base[k]).kurt()\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y=pd.Series(X).kurt()\n",
    "                    meta_data = meta_data + list(([Y]-allChannelbase[num]))\n",
    "                    num=num+1\n",
    "                    \n",
    "                    #print(np.array(meta_data).shape)\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "def FFT_Processing_subbasefeature_selectchannel_skew(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    tmp = 0\n",
    "                    Y=pd.Series(base[k]).skew()\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y=pd.Series(X).kurt()\n",
    "                    meta_data = meta_data + list(([Y]-allChannelbase[num]))\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "def FFT_Processing_subbasefeature_selectchannel_var(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    tmp = 0\n",
    "                    Y=np.var(base[k])\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "        \n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y=np.var(X)\n",
    "                    meta_data = meta_data + list(([Y]-allChannelbase[num]))\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                    \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "def FFT_Processing_subbasefeature_selectchannel_secondO(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for t in range(3):\n",
    "                    tmp = 0\n",
    "                    bas=base[t]\n",
    "                    for k in range(0,len(bas)-2):\n",
    "                        tmp = tmp + abs(bas[k+2] - 2 * bas[k+1] + bas[k] )\n",
    "                    Y = tmp / (len(bas)-2)\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "                \n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "\n",
    "                    tmp = 0\n",
    "                    for k in range(0,len(X)-2):\n",
    "                        tmp = tmp + abs(X[k+2] - 2 * X[k+1] + X[k] )\n",
    "                    Y = tmp / (len(X)-2)\n",
    "                    #print([Y]-allChannelbase[num])\n",
    "                    meta_data = meta_data + list(([Y]-allChannelbase[num]))\n",
    "                    num=num+1\n",
    "                    \n",
    "                    #print(np.array(meta_data).shape)\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                \n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "\n",
    "\n",
    "\n",
    "def waveAllEngergy(arr1,arr2,arr3,arr4,arr5):\n",
    "    SUM=sum([waveEngergy(arr1),waveEngergy(arr2),waveEngergy(arr3),waveEngergy(arr4),waveEngergy(arr5)])\n",
    "    return SUM\n",
    "\n",
    "def waveEngergy(engergy):\n",
    "    result=sum([x ** 2 for x in engergy])\n",
    "    return result\n",
    "\n",
    "def waveEntropy(en):\n",
    "    result=-en*np.log2(en)\n",
    "    return result\n",
    "\n",
    "def FFT_Processing_subbasefeature_selectchannel_waven(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    A4,D4,D3,D2,D1 = wavedec(base[k], 'db4',level=4)\n",
    "                    #求相对小波能量\n",
    "                    fp1=waveEngergy(A4)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp2=waveEngergy(D4)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp3=waveEngergy(D3)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp4=waveEngergy(D2)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp5=waveEngergy(D1)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    mean = []\n",
    "                    mean.append(waveEntropy(fp1))\n",
    "                    mean.append(waveEntropy(fp2))\n",
    "                    mean.append(waveEntropy(fp3))\n",
    "                    mean.append(waveEntropy(fp4))\n",
    "                    mean.append(waveEntropy(fp5))\n",
    "                    basefeature.append(mean)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    A4,D4,D3,D2,D1 = wavedec(X, 'db4',level=4)\n",
    "                    #求相对小波能量\n",
    "                    fp1=waveEngergy(A4)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp2=waveEngergy(D4)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp3=waveEngergy(D3)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp4=waveEngergy(D2)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "                    fp5=waveEngergy(D1)/waveAllEngergy(A4,D4,D3,D2,D1)\n",
    "\n",
    "                    mean = []\n",
    "                    mean.append(fp1)\n",
    "                    mean.append(fp2)\n",
    "                    mean.append(fp3)\n",
    "                    mean.append(fp4)\n",
    "                    mean.append(fp5)\n",
    "\n",
    "                    meta_data = meta_data + list(mean-allChannelbase[num])\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "        \n",
    "\n",
    "#能量谱\n",
    "def engergy(X, Band, Fs):\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    en = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        fred_band=C[int(np.floor(Freq / Fs * len(X))): int(np.floor(Next_Freq / Fs * len(X)))]\n",
    "#         Power[Freq_Index] = sum(index*index for index in fred_band)\n",
    "        en[Freq_Index] = sum(index*index for index in fred_band)\n",
    "    return en\n",
    "\n",
    "def de_Entropy(X, Band, Fs):\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    en = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        fred_band=C[int(np.floor(Freq / Fs * len(X))): int(np.floor(Next_Freq / Fs * len(X)))]\n",
    "#         Power[Freq_Index] = sum(index*index for index in fred_band)\n",
    "        en[Freq_Index] = sum(index*index for index in fred_band)\n",
    "    return np.log(en)\n",
    "\n",
    "def FFT_Processing_subbasefeature_selectchannel_de_Entropy(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + subjects + '.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    #Power_Ratio= mybin_power(base[k], band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(base[k], band, sample_rate,Power_Ratio)\n",
    "                    Y =  de_Entropy(base[k],band,sample_rate)\n",
    "                    #Y = bin_power(base[k], band, sample_rate)\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y =  de_Entropy(X,band,sample_rate)\n",
    "                    \n",
    "                    meta_data = meta_data + list((Y)-allChannelbase[num])\n",
    "\n",
    "\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True)\n",
    "        \n",
    "def FFT_Processing_subbasefeature_selectchannel_engergy(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + subjects + '.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    #Power_Ratio= mybin_power(base[k], band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(base[k], band, sample_rate,Power_Ratio)\n",
    "                    Y =  engergy(base[k],band,sample_rate)\n",
    "                    #Y = bin_power(base[k], band, sample_rate)\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y =  engergy(X,band,sample_rate)\n",
    "                    \n",
    "                    meta_data = meta_data + list((Y)-allChannelbase[num])\n",
    "\n",
    "\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True)\n",
    "def FFT_Processing_subbasefeature_selectchannel_newfeature_computetime(sub, channel,band, window_size, step_size, sample_rate):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + subjects + '.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    #Power_Ratio= mybin_power(base[k], band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(base[k], band, sample_rate,Power_Ratio)\n",
    "                    Y = bin_power(base[k], band, sample_rate)\n",
    "                    basefeature.append(Y[1]*np.log2(Y[1]))\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            \n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    #Power_Ratio= mybin_power(X, band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(X, band, sample_rate,Power_Ratio)\n",
    "                    #meta_data = meta_data +[Y-allChannelbase[num]]\n",
    "                    meta_data = meta_data + list((Y[1]*np.log2(Y[1]))-allChannelbase[num])\n",
    "\n",
    "\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        #np.save('log2\\s' + sub, meta_de, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "        \n",
    "def FFT_Processing_subbasefeature_selectchannel_mean(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    Y = np.mean(base[k])\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            \n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y=np.mean(X) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    #print((np.log2(Y[0]).shape))\n",
    "                    meta_data = meta_data + list([Y]-allChannelbase[num])\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                    \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "def FFT_Processing_subbasefeature_selectchannel_std(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for t in range(3):\n",
    "                    tmp = 0\n",
    "                    bas=base[t]\n",
    "                    for k in range(0,len(bas)-1):\n",
    "                        tmp = tmp + abs(bas[k+1] - bas[k])\n",
    "                    Y = tmp / (len(bas)-1)\n",
    "                    Y=Y/np.std(bas);\n",
    "                    basefeature.append(Y)\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "           #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    tmp = 0\n",
    "                    \n",
    "                    for k in range(0,len(X)-1):\n",
    "                        tmp = tmp + abs(X[k+1] - X[k])\n",
    "                        \n",
    "                    Y = tmp / (len(X)-1)\n",
    "                    Y=Y/np.std(X)\n",
    "                    \n",
    "                    meta_data = meta_data + list(([Y]-allChannelbase[num]))\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                    \n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "              \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path2 in range(0,2):\n",
    "    for subjects in subjectList:\n",
    "        \n",
    "        FFT_Processing_subbasefeature_selectchannel_std (subjects, channel, band, window_size, step_size, sample_rate, 'std', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_secondO (subjects, channel, band, window_size, step_size, sample_rate, 'secondO', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_firstOm (subjects, channel, band, window_size, step_size, sample_rate, 'firstOm', path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path2 in range(0,2):\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_wav (subjects, channel, band, window_size, step_size, sample_rate, 'wav', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_mean (subjects, channel, band, window_size, step_size, sample_rate, 'mean', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_newfeature_computetime(subjects, channel, band, window_size, step_size, sample_rate)\n",
    "        FFT_Processing_subbasefeature_selectchannel_skew (subjects, channel, band, window_size, step_size, sample_rate, 'skew', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_var (subjects, channel, band, window_size, step_size, sample_rate, 'var', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_firstOm (subjects, channel, band, window_size, step_size, sample_rate, 'firstOm', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_secondO (subjects, channel, band, window_size, step_size, sample_rate, 'secondO', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_de_Entropy(subjects, channel, band, window_size, step_size, sample_rate, 'de_Entropy', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_engergy(subjects, channel, band, window_size, step_size, sample_rate, 'engergy', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_kurt (subjects, channel, band, window_size, step_size, sample_rate, 'kurt', path2)\n",
    "        FFT_Processing_subbasefeature_selectchannel_std (subjects, channel, band, window_size, step_size, sample_rate, 'std', path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_Processing_subbasefeature_selectchannel_psifeature(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + subjects + '.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:#0A1V\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    #Power_Ratio= mybin_power(base[k], band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(base[k], band, sample_rate,Power_Ratio)\n",
    "                    Y = bin_power(base[k], band, sample_rate)\n",
    "                    basefeature.append(Y[0])\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    #Power_Ratio= mybin_power(X, band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(X, band, sample_rate,Power_Ratio)\n",
    "                    #meta_data = meta_data +[Y-allChannelbase[num]]\n",
    "                    meta_data = meta_data + list(Y[0]-allChannelbase[num])\n",
    "\n",
    "\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "        \n",
    "def FFT_Processing_subbasefeature_selectchannel_rirfeature(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + subjects + '.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:#0A1V\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    #Power_Ratio= mybin_power(base[k], band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(base[k], band, sample_rate,Power_Ratio)\n",
    "                    Y = bin_power(base[k], band, sample_rate)\n",
    "                    basefeature.append(Y[1])\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    #Power_Ratio= mybin_power(X, band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(X, band, sample_rate,Power_Ratio)\n",
    "                    #meta_data = meta_data +[Y-allChannelbase[num]]\n",
    "                    meta_data = meta_data + list(Y[1]-allChannelbase[num])\n",
    "\n",
    "\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) \n",
    "        #np.save('logcontrast\\crir\\c1\\s' + sub, meta_de, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.55128699995112\n",
      "98.95964119990822\n"
     ]
    }
   ],
   "source": [
    "#二次验证\n",
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_newfeature_computetime(subjects, channel, band, window_size, step_size, sample_rate)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.36733879998792\n",
      "99.13749090000056\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_psifeature(subjects, channel, band, window_size, step_size, sample_rate, 'psi', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.14927329996135\n",
      "102.27122550003696\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_rirfeature(subjects, channel, band, window_size, step_size, sample_rate, 'rir', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479.2211471999763\n",
      "1469.2222443000646\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_waven(subjects, channel, band, window_size, step_size, sample_rate, 'waven', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323.3524413000005\n",
      "1325.0179961000003\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_wav (subjects, channel, band, window_size, step_size, sample_rate, 'wav', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.406012200000077\n",
      "19.896240800000214\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_mean (subjects, channel, band, window_size, step_size, sample_rate, 'mean', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0796749000001\n",
      "85.42700369999966\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_newfeature_computetime(subjects, channel, band, window_size, step_size, sample_rate)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327.29057409999996\n",
      "326.88688490000004\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_skew (subjects, channel, band, window_size, step_size, sample_rate, 'skew', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.34474380000029\n",
      "49.59791559999985\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_var (subjects, channel, band, window_size, step_size, sample_rate, 'var', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236.2063103999999\n",
      "265.59714250000025\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_firstOm (subjects, channel, band, window_size, step_size, sample_rate, 'firstOm', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237.3756696000005\n",
      "242.3663421000001\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_secondO (subjects, channel, band, window_size, step_size, sample_rate, 'secondO', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.46899209999992\n",
      "95.42163470000014\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "         FFT_Processing_subbasefeature_selectchannel_de_Entropy(subjects, channel, band, window_size, step_size, sample_rate, 'de_Entropy', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.57124820000081\n",
      "95.63695610000013\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_engergy(subjects, channel, band, window_size, step_size, sample_rate, 'engergy', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331.84152559999984\n",
      "331.0095406999999\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_kurt (subjects, channel, band, window_size, step_size, sample_rate, 'kurt', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359.090139599999\n",
      "351.02651049999986\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for path2 in range(0,2):\n",
    "    start = time.clock()\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_std (subjects, channel, band, window_size, step_size, sample_rate, 'std', path2)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "path: de_Entropy 0\n",
      "acc_v_mean: 0.87984375\n",
      "------------\n",
      "path: engergy 0\n",
      "acc_v_mean: 0.723828125\n",
      "------------\n",
      "path: firstOm 0\n",
      "acc_v_mean: 0.78046875\n",
      "------------\n",
      "path: kurt 0\n",
      "acc_v_mean: 0.760234375\n",
      "------------\n",
      "path: mean 0\n",
      "acc_v_mean: 0.60109375\n",
      "------------\n",
      "path: secondO 0\n",
      "acc_v_mean: 0.8025\n",
      "------------\n",
      "path: skew 0\n",
      "acc_v_mean: 0.66\n",
      "------------\n",
      "path: std 0\n",
      "acc_v_mean: 0.74125\n",
      "------------\n",
      "path: var 0\n",
      "acc_v_mean: 0.6723437499999999\n",
      "------------\n",
      "path: wav 0\n",
      "acc_v_mean: 0.9009375\n",
      "------------\n",
      "path: de_Entropy 1\n",
      "acc_v_mean: 0.881640625\n",
      "------------\n",
      "path: engergy 1\n",
      "acc_v_mean: 0.761484375\n",
      "------------\n",
      "path: firstOm 1\n",
      "acc_v_mean: 0.787109375\n",
      "------------\n",
      "path: kurt 1\n",
      "acc_v_mean: 0.77796875\n",
      "------------\n",
      "path: mean 1\n",
      "acc_v_mean: 0.6553906249999999\n",
      "------------\n",
      "path: secondO 1\n",
      "acc_v_mean: 0.807890625\n",
      "------------\n",
      "path: skew 1\n",
      "acc_v_mean: 0.682734375\n",
      "------------\n",
      "path: std 1\n",
      "acc_v_mean: 0.7633593750000001\n",
      "------------\n",
      "path: var 1\n",
      "acc_v_mean: 0.7022656250000001\n",
      "------------\n",
      "path: wav 1\n",
      "acc_v_mean: 0.9012500000000001\n"
     ]
    }
   ],
   "source": [
    "random_list_Test=[]\n",
    "random_list_Validation=[]\n",
    "random_list_Train=[]\n",
    "with open('log1\\list_train'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Train  = np.load(fileTrain)\n",
    "with open('log1\\list_test'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Test  = np.load(fileTrain)\n",
    "with open('log1\\list_validation'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Validation  = np.load(fileTrain)\n",
    "#print(\"测试集\",random_list_Test)\n",
    "#print(\"验证集\",random_list_Validation)\n",
    "#print(\"训练集\",random_list_Train)\n",
    "namelist=['de_Entropy','engergy','firstOm','kurt','mean','secondO','skew','std','var','wav']\n",
    "\n",
    "for path2 in range(0,2):\n",
    "    for path1 in namelist:\n",
    "        acc_v1 = []\n",
    "        for subjects in subjectList:\n",
    "            dataTest=[]\n",
    "            labelTest=[]\n",
    "            dataValidation=[]\n",
    "            labelValidation=[]\n",
    "            dataTrain=[]\n",
    "            labelTrain=[]\n",
    "            dataTestValidation=[]\n",
    "            labelTestValidation=[]\n",
    "\n",
    "            with open('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\s' + subjects + '.npy', 'rb') as file:\n",
    "                sub = np.load(file, allow_pickle=True)\n",
    "                for i in range (0,sub.shape[0]):\n",
    "                    if i in random_list_Train:\n",
    "                        dataTrain.append(np.ravel(sub[i][0]))\n",
    "                        labelTrain.append(sub[i][1])\n",
    "                    if i in random_list_Test:\n",
    "                        dataTest.append(np.ravel(sub[i][0]))\n",
    "                        labelTest.append(sub[i][1])\n",
    "                    if i in random_list_Validation:\n",
    "                        dataValidation.append(np.ravel(sub[i][0]))\n",
    "                        labelValidation.append(sub[i][1])\n",
    "\n",
    "            #print(\"subjects:\", subjects)\n",
    "\n",
    "            #print('loading data...')\n",
    "            X  = np.array(dataTrain)\n",
    "            Y  = np.array(labelTrain)\n",
    "            X = normalize(X)\n",
    "\n",
    "\n",
    "\n",
    "            M = np.array(dataTest)\n",
    "            N = np.array(labelTest)\n",
    "            M = normalize(M)\n",
    "\n",
    "\n",
    "\n",
    "            O = np.array(dataValidation)\n",
    "            P = np.array(labelValidation)\n",
    "            O = normalize(O)\n",
    "            np.save('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\data_testing'+str(subjects)+'.npy', np.array(dataTest), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\label_testing'+str(subjects)+'.npy', np.array(labelTest), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\data_validationing'+str(subjects)+'.npy', np.array(dataValidation), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\label_validationing'+str(subjects)+'.npy', np.array(labelValidation), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\data_training'+str(subjects)+'.npy', np.array(dataTrain), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrastv\\c'+str(path1)+'\\c'+str(path2)+'\\label_training'+str(subjects)+'.npy', np.array(labelTrain), allow_pickle=True, fix_imports=True)\n",
    "\n",
    "\n",
    "            train_size = np.array(X).shape[0]\n",
    "            #print(np.array(X).shape[0],\"------train_size------\",np.array(X).shape[1])\n",
    "\n",
    "\n",
    "            #print(\"-----------svm1分类----------\")\n",
    "            SV = svm.SVC(kernel='rbf')\n",
    "            SV.fit(X, Y)\n",
    "            acc_matrix=SV.score(M, N)\n",
    "            acc_v1.append(acc_matrix)\n",
    "\n",
    "            #print(acc_matrix)\n",
    "\n",
    "\n",
    "        acc_v_mean1 = np.mean(acc_v1)\n",
    "        #for i in range(len(subjectList)):\n",
    "            #print(\"subjects[\",i,\"]\")\n",
    "            #print(\"acc_v_mean:\",acc_v1[i])\n",
    "        print(\"------------\")\n",
    "        print(\"path:\",path1,path2)\n",
    "        print(\"acc_v_mean:\",acc_v_mean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "path: waven 0\n",
      "acc_v_mean: 0.8790625\n",
      "------------\n",
      "path: rir 0\n",
      "acc_v_mean: 0.85421875\n",
      "------------\n",
      "path: psi 0\n",
      "acc_v_mean: 0.821640625\n",
      "------------\n",
      "path: waven 1\n",
      "acc_v_mean: 0.886015625\n",
      "------------\n",
      "path: rir 1\n",
      "acc_v_mean: 0.859140625\n",
      "------------\n",
      "path: psi 1\n",
      "acc_v_mean: 0.83078125\n"
     ]
    }
   ],
   "source": [
    "random_list_Test=[]\n",
    "random_list_Validation=[]\n",
    "random_list_Train=[]\n",
    "with open('log1\\list_train'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Train  = np.load(fileTrain)\n",
    "with open('log1\\list_test'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Test  = np.load(fileTrain)\n",
    "with open('log1\\list_validation'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Validation  = np.load(fileTrain)\n",
    "#print(\"测试集\",random_list_Test)\n",
    "#print(\"验证集\",random_list_Validation)\n",
    "#print(\"训练集\",random_list_Train)\n",
    "namelist=['waven','rir','psi']\n",
    "\n",
    "for path2 in range(0,2):\n",
    "    for path1 in namelist:\n",
    "        acc_v1 = []\n",
    "        for subjects in subjectList:\n",
    "            dataTest=[]\n",
    "            labelTest=[]\n",
    "            dataValidation=[]\n",
    "            labelValidation=[]\n",
    "            dataTrain=[]\n",
    "            labelTrain=[]\n",
    "            dataTestValidation=[]\n",
    "            labelTestValidation=[]\n",
    "\n",
    "            with open('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\s' + subjects + '.npy', 'rb') as file:\n",
    "                sub = np.load(file, allow_pickle=True)\n",
    "                for i in range (0,sub.shape[0]):\n",
    "                    if i in random_list_Train:\n",
    "                        dataTrain.append(np.ravel(sub[i][0]))\n",
    "                        labelTrain.append(sub[i][1])\n",
    "                    if i in random_list_Test:\n",
    "                        dataTest.append(np.ravel(sub[i][0]))\n",
    "                        labelTest.append(sub[i][1])\n",
    "                    if i in random_list_Validation:\n",
    "                        dataValidation.append(np.ravel(sub[i][0]))\n",
    "                        labelValidation.append(sub[i][1])\n",
    "\n",
    "            #print(\"subjects:\", subjects)\n",
    "\n",
    "            #print('loading data...')\n",
    "            X  = np.array(dataTrain)\n",
    "            Y  = np.array(labelTrain)\n",
    "            X = normalize(X)\n",
    "\n",
    "\n",
    "\n",
    "            M = np.array(dataTest)\n",
    "            N = np.array(labelTest)\n",
    "            M = normalize(M)\n",
    "\n",
    "\n",
    "\n",
    "            O = np.array(dataValidation)\n",
    "            P = np.array(labelValidation)\n",
    "            O = normalize(O)\n",
    "            np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\data_testing'+str(subjects)+'.npy', np.array(dataTest), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\label_testing'+str(subjects)+'.npy', np.array(labelTest), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\data_validationing'+str(subjects)+'.npy', np.array(dataValidation), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\label_validationing'+str(subjects)+'.npy', np.array(labelValidation), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\data_training'+str(subjects)+'.npy', np.array(dataTrain), allow_pickle=True, fix_imports=True)\n",
    "            np.save('logcontrast\\c'+str(path1)+'\\c'+str(path2)+'\\label_training'+str(subjects)+'.npy', np.array(labelTrain), allow_pickle=True, fix_imports=True)\n",
    "\n",
    "\n",
    "            train_size = np.array(X).shape[0]\n",
    "            #print(np.array(X).shape[0],\"------train_size------\",np.array(X).shape[1])\n",
    "\n",
    "\n",
    "            #print(\"-----------svm1分类----------\")\n",
    "            SV = svm.SVC(kernel='rbf')\n",
    "            SV.fit(X, Y)\n",
    "            acc_matrix=SV.score(M, N)\n",
    "            acc_v1.append(acc_matrix)\n",
    "\n",
    "            #print(acc_matrix)\n",
    "\n",
    "\n",
    "        acc_v_mean1 = np.mean(acc_v1)\n",
    "        #for i in range(len(subjectList)):\n",
    "            #print(\"subjects[\",i,\"]\")\n",
    "            #print(\"acc_v_mean:\",acc_v1[i])\n",
    "        print(\"------------\")\n",
    "        print(\"path:\",path1,path2)\n",
    "        print(\"acc_v_mean:\",acc_v_mean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_Processing_subbasefeature_selectchannel_PSE(sub, channel,band, window_size, step_size, sample_rate, path1,path2):\n",
    "    meta_de = []\n",
    "    with open('data\\s' + subjects + '.dat', 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        for i in range (0,40):\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            nlabel = 0\n",
    "            if labels[path2]>=5:\n",
    "                nlabel = 1\n",
    "            start = 384\n",
    "            allChannelbase=[]#保存32个通道的特征基线\n",
    "            for j in channel:\n",
    "                base = np.array(data[j][0:384]).reshape(3,128)\n",
    "                basefeature=[]#3s每个特征的特征基线\n",
    "                for k in range(3):\n",
    "                    #Power_Ratio= mybin_power(base[k], band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(base[k], band, sample_rate,Power_Ratio)\n",
    "                    Y = bin_power(base[k], band, sample_rate)\n",
    "                    basefeature.append(Y[1]*np.log2(Y[1]))\n",
    "                basevalue=np.array(basefeature).mean(axis=0)\n",
    "                allChannelbase.append(basevalue)\n",
    "            #print(np.array(allChannelbase).shape)\n",
    "            while start + window_size <= 8064: # 使用窗口将每个通道的信号分为464份，可以产生464个样本（去掉前3秒的数据）\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                num=0\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    #Power_Ratio= mybin_power(X, band, sample_rate)\n",
    "                    #Y =pyeeg.spectral_entropy(X, band, sample_rate,Power_Ratio)\n",
    "                    #meta_data = meta_data +[Y-allChannelbase[num]]\n",
    "                    meta_data = meta_data + list((Y[1]*np.log2(Y[1]))-allChannelbase[num])\n",
    "\n",
    "\n",
    "                    num=num+1\n",
    "                    #print(np.array(meta_data).shape)\n",
    "                \n",
    "                meta_array.append(np.array(meta_data))\n",
    "                #print(meta_array)\n",
    "                meta_array.append(nlabel)\n",
    "\n",
    "                meta_de.append(np.array(meta_array))      \n",
    "                   \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta_de = np.array(meta_de)\n",
    "        np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\s' + sub, meta_de, allow_pickle=True, fix_imports=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "path: pse 0\n",
      "acc_v_mean: 0.90796875\n",
      "------------\n",
      "path: pse 1\n",
      "acc_v_mean: 0.9117968750000001\n"
     ]
    }
   ],
   "source": [
    "for path2 in range(0,2):\n",
    "    for subjects in subjectList:\n",
    "        FFT_Processing_subbasefeature_selectchannel_PSE (subjects, channel, band, window_size, step_size, sample_rate, 'pse', path2)\n",
    "random_list_Test=[]\n",
    "random_list_Validation=[]\n",
    "random_list_Train=[]\n",
    "with open('log1\\list_train'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Train  = np.load(fileTrain)\n",
    "with open('log1\\list_test'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Test  = np.load(fileTrain)\n",
    "with open('log1\\list_validation'+'.npy', 'rb') as fileTrain:\n",
    "    random_list_Validation  = np.load(fileTrain)\n",
    "#print(\"测试集\",random_list_Test)\n",
    "#print(\"验证集\",random_list_Validation)\n",
    "#print(\"训练集\",random_list_Train)\n",
    "namelist=['pse']\n",
    "\n",
    "for path2 in range(0,2):\n",
    "    for path1 in namelist:\n",
    "        acc_v1 = []\n",
    "        for subjects in subjectList:\n",
    "            dataTest=[]\n",
    "            labelTest=[]\n",
    "            dataValidation=[]\n",
    "            labelValidation=[]\n",
    "            dataTrain=[]\n",
    "            labelTrain=[]\n",
    "            dataTestValidation=[]\n",
    "            labelTestValidation=[]\n",
    "\n",
    "            with open('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\s' + subjects + '.npy', 'rb') as file:\n",
    "                sub = np.load(file, allow_pickle=True)\n",
    "                for i in range (0,sub.shape[0]):\n",
    "                    if i in random_list_Train:\n",
    "                        dataTrain.append(np.ravel(sub[i][0]))\n",
    "                        labelTrain.append(sub[i][1])\n",
    "                    if i in random_list_Test:\n",
    "                        dataTest.append(np.ravel(sub[i][0]))\n",
    "                        labelTest.append(sub[i][1])\n",
    "                    if i in random_list_Validation:\n",
    "                        dataValidation.append(np.ravel(sub[i][0]))\n",
    "                        labelValidation.append(sub[i][1])\n",
    "\n",
    "            #print(\"subjects:\", subjects)\n",
    "\n",
    "            #print('loading data...')\n",
    "            X  = np.array(dataTrain)\n",
    "            Y  = np.array(labelTrain)\n",
    "            X = normalize(X)\n",
    "\n",
    "\n",
    "\n",
    "            M = np.array(dataTest)\n",
    "            N = np.array(labelTest)\n",
    "            M = normalize(M)\n",
    "\n",
    "\n",
    "\n",
    "            O = np.array(dataValidation)\n",
    "            P = np.array(labelValidation)\n",
    "            O = normalize(O)\n",
    "            np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\data_testing'+str(subjects)+'.npy', np.array(dataTest), allow_pickle=True, fix_imports=True)\n",
    "            np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\label_testing'+str(subjects)+'.npy', np.array(labelTest), allow_pickle=True, fix_imports=True)\n",
    "            np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\data_validationing'+str(subjects)+'.npy', np.array(dataValidation), allow_pickle=True, fix_imports=True)\n",
    "            np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\label_validationing'+str(subjects)+'.npy', np.array(labelValidation), allow_pickle=True, fix_imports=True)\n",
    "            np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\data_training'+str(subjects)+'.npy', np.array(dataTrain), allow_pickle=True, fix_imports=True)\n",
    "            np.save('log2p\\c'+str(path1)+'\\c'+str(path2)+'\\label_training'+str(subjects)+'.npy', np.array(labelTrain), allow_pickle=True, fix_imports=True)\n",
    "\n",
    "\n",
    "            train_size = np.array(X).shape[0]\n",
    "            #print(np.array(X).shape[0],\"------train_size------\",np.array(X).shape[1])\n",
    "\n",
    "\n",
    "            #print(\"-----------svm1分类----------\")\n",
    "            SV = svm.SVC(kernel='rbf')\n",
    "            SV.fit(X, Y)\n",
    "            acc_matrix=SV.score(M, N)\n",
    "            acc_v1.append(acc_matrix)\n",
    "\n",
    "            #print(acc_matrix)\n",
    "\n",
    "\n",
    "        acc_v_mean1 = np.mean(acc_v1)\n",
    "        #for i in range(len(subjectList)):\n",
    "            #print(\"subjects[\",i,\"]\")\n",
    "            #print(\"acc_v_mean:\",acc_v1[i])\n",
    "        print(\"------------\")\n",
    "        print(\"path:\",path1,path2)\n",
    "        print(\"acc_v_mean:\",acc_v_mean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
