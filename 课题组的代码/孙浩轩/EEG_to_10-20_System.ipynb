{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a8a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367e8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "deapSubList=['01','02','03','04','05','06','07','08','09','10','11','12','13','14',\\\n",
    "         '15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "#用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "channel_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],9:[5,3],10:[6,3],11:[6,0],12:[7,3],\n",
    "                13:[8,3],14:[7,4],15:[6,4],16:[0,5],17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "                25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b660b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 把EEG信号映射到10-20系统中，并把其他信号提取出来备用\n",
    "# EEG 和 其他特征信号均进行了去基线的操作，以后进行训练的时候更方便\n",
    "# 并把EEG特征标准化在放入，其他特征先不标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d66695-685e-4fc0-bcb9-37db9b1e2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把基线按照1s窗口分割，每秒一次分割，也就是128次一分割\n",
    "def base_split(baseline_data):\n",
    "    deap_baseline_split = np.zeros((40,3,40,128))\n",
    "    for i in range(40):\n",
    "        windows = 128//1\n",
    "        part = 3\n",
    "        baseli = np.stack(np.split(baseline_data[i],part,axis=1))\n",
    "        deap_baseline_split[i] = baseli\n",
    "    return deap_baseline_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5355b5bf-74a5-4d02-950d-a1e1bb53cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把特征按照基线做同样的处理，为了方便去基线\n",
    "def feature_split(feature_data):\n",
    "    features_split = np.zeros((40,60,40,128))\n",
    "    for i in range(40):\n",
    "        windows = 128//1\n",
    "        part = 60\n",
    "        fea = np.stack(np.split(feature_data[i],part,axis=1))\n",
    "        features_split[i] = fea\n",
    "    return features_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c3a122-38d5-4c14-84f9-2ba19c08f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把（40,60,40,128）再变回去（40,40,7680），方便映射到10-20系统中的9*9矩阵中\n",
    "def feature_to_oriShape(features_split_moveBasli):\n",
    "    feature_move_baseline = np.zeros((40,40,7680))\n",
    "    for i in range(40):\n",
    "        gap = 0\n",
    "        for j in range(60):\n",
    "            feature_move_baseline [i][:,gap:gap+128] = features_split_moveBasli[i][j]\n",
    "            gap+=128\n",
    "    return feature_move_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63804cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6248dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def EEG_to_1020():\n",
    "    for subs in deapSubList:\n",
    "        with open(r'D:\\JupyterNotebookWorkSpace\\EEG emotion recognition\\dataSet\\data_preprocessed_python\\s'+ subs + '.dat' , 'rb') as file:\n",
    "\n",
    "            \n",
    "            allData = sub = pickle.load(file , encoding='latin1')\n",
    "            allData = allData['data']\n",
    "            feature_data = allData[:,:,384:]  #取所有信号的特征\n",
    "            baseline_data = allData[:,:,:384] #取所有信号的基线\n",
    "            deap_baseline_split = base_split(baseline_data)  #基线按照1s分割\n",
    "            features_split=feature_split(feature_data)  #特征按照1s分割\n",
    "            # 去掉所有信号的基线再进行保存\n",
    "            temp=np.mean(deap_baseline_split,axis=1)[:,np.newaxis,:,:]\n",
    "            features_split_moveBasli = features_split  - temp\n",
    "            \n",
    "            #把去掉基线的所有特征变回原来的形状（40,40,7680）\n",
    "            all_features=feature_to_oriShape(features_split_moveBasli)\n",
    "            \n",
    "            eeg_data = all_features[:,:32,:]  # 把eeg特征取出，之后映射\n",
    "            other_features = all_features[:,32:,:]\n",
    "            # EEG 数据的形状为 （40,32,7680）\n",
    "            std = StandardScaler()\n",
    "            \n",
    "            #这里我是先进行的标准化，之后再进行的映射到二维帧\n",
    "            # ------------------------------------------------------------------------- 标准化问题\n",
    "            for i in range(40):\n",
    "                eeg_data[i] = std.fit_transform(eeg_data[i])\n",
    "            #下面的for循环把eeg映射到10-20系统的9*9矩阵中\n",
    "            # 原来是（40,32,7680）\n",
    "            eeg_features =np.zeros((40,7680,9,9))\n",
    "            for i in range(40):\n",
    "                for j in range(32):\n",
    "                    index=0\n",
    "                    for k in range(7680):\n",
    "                        row = channel_to_1020[j][0]\n",
    "                        column = channel_to_1020[j][1]\n",
    "                        #把EEG信号进行标准化\n",
    "                        eeg_features[i,index,row,column]=eeg_data[i,j,k]\n",
    "                        index+=1\n",
    "        labels = sub[\"labels\"][:,:2]\n",
    "        out_data_dict={'eeg_features':eeg_features,\n",
    "                       'other_features':other_features,'labels':labels}\n",
    "        np.save('eeg_to_1020/1020sub'+subs+'.npy',out_data_dict)    \n",
    "\n",
    "                    \n",
    "EEG_to_1020()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9430d06-1f5f-4fb1-982a-1c715aa4e554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512f667-3f1b-4e0f-8566-a9c5cbe986fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a613dd3-d09c-49f3-b5db-53f073aa9d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4635a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\DEAP data\\data_preprocessed_python\\s'+ '01' + '.dat' , 'rb') as file:\n",
    "    allData = sub = pickle.load(file , encoding='latin1')\n",
    "    allData = allData['data']\n",
    "    feature_data = allData[:,:,:7680]\n",
    "    baseline_data = allData[:,:,7680:]\n",
    "    deap_baseline_split = base_split(baseline_data)\n",
    "    features_split=feature_split(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8046b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 40, 7680), (40, 40, 384))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#所有 特征 和 基线 的原始数据\n",
    "feature_data.shape,baseline_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8575e28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 3, 40, 128), (40, 60, 40, 128))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#所有 特征 和 基线 进行1s分割的数据形状\n",
    "deap_baseline_split.shape,features_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6376d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面三个代码块用来去基线\n",
    "temp=np.mean(deap_baseline_split,axis=1)[:,np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e51925fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_split_moveBasli = features_split  - temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88109b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 60, 40, 128)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_split_moveBasli.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "201dc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=feature_to_oriShape(features_split_moveBasli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c1cf80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 7680)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
