{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6df28-9919-4bff-8e9a-ea82c35eba11",
   "metadata": {},
   "source": [
    "### 脑电图注意力网络（GAT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def data_split(train_ratio=0.7):\n",
    "#     load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "#     trials = np.load(load_dir + 'trials.npy')\n",
    "#     bases = np.load(load_dir + 'bases.npy')\n",
    "#     labels = np.load(load_dir + 'labels.npy')\n",
    "#     # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "#     # 去基线\n",
    "#     for i, base in enumerate(bases):\n",
    "#         trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "#     # 离散化标签\n",
    "#     labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "#     # 复制标签以对齐样本\n",
    "#     labels = np.repeat(labels, 60, axis = 0)\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     shuffle_list = np.arange(trials.shape[0])\n",
    "#     np.random.shuffle(shuffle_list)\n",
    "#     trials = trials[shuffle_list]\n",
    "#     labels = labels[shuffle_list]\n",
    "    \n",
    "#     cut_point = int(trials.shape[0] * train_ratio)\n",
    "#     train_features, train_labels = trials[:cut_point], labels[:cut_point]\n",
    "#     test_features, test_labels = trials[cut_point:], labels[cut_point:]\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32 * 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32 * 128))\n",
    "    \n",
    "#     mean = train_features.mean(axis = 0)\n",
    "#     std = train_features.std(axis = 0)\n",
    "    \n",
    "#     train_features = (train_features - mean) / std\n",
    "#     test_features = (test_features - mean) / std\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32, 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32, 128))\n",
    "    \n",
    "#     save_dir = 'data/data_split/'\n",
    "#     np.save(save_dir + 'train_features.npy', train_features)\n",
    "#     np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "#     np.save(save_dir + 'test_features.npy', test_features)\n",
    "#     np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "# data_split(train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b5e808-1250-4784-b6b7-a0e58d86017a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if create_complete_graph:\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        return edge_index\n",
    "    \n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd42b5bf-ca24-4f71-95ad-b0b494aaa18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcb8368-b508-41fb-baa4-5e35d16ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index = [[],[]]\n",
    "# weight = []\n",
    "\n",
    "# #用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "# chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "#               9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "#               17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "#                 25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "# maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "# for k, v in chan_to_1020.items():\n",
    "#     maps[v[0]][v[1]] = k + 1\n",
    "# print(maps)\n",
    "# plt.matshow(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    edge_index = None\n",
    "    def __init__(self, root, is_train_data, edge_index):\n",
    "        self.is_train_data = is_train_data\n",
    "        self.edge_index = edge_index\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['train.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor(labels[i].reshape(1, -1), dtype=torch.long)\n",
    "            data = Data(x = x, edge_index=self.edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.temporalMLPs1 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.temporalMLPs1.append(nn.Linear(128, 256, device=device))\n",
    "        \n",
    "        self.temporalMLPs2 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.temporalMLPs2.append(nn.Linear(256, 256, device=device))\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(8192, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.view(batch_size, 32, 128)\n",
    "        \n",
    "        temporalMLPs_out = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            x1 = self.temporalMLPs1[i](x[:, i, :])\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, 0.2)\n",
    "            x1 = self.temporalMLPs2[i](x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, 0.2)\n",
    "            temporalMLPs_out.append(x1)\n",
    "        \n",
    "        # concat\n",
    "        x = torch.concat(temporalMLPs_out, dim = 1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emo_dim):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        train_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_train_sample = len(trainDataLoader.dataset)\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    train_acc = train_acc / num_train_sample\n",
    "    \n",
    "    # check测试集的性能\n",
    "    vali_loss = 0\n",
    "    vali_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in testDataLoader:\n",
    "        batch.to(device)\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        vali_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        vali_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_test_sample = len(testDataLoader.dataset)\n",
    "    vali_loss = vali_loss / num_test_sample\n",
    "    vali_acc = vali_acc / num_test_sample\n",
    "    \n",
    "    print(f'train_loss:{train_loss:.6f}, train_acc:{train_acc:.6f}, test_loss:{vali_loss:.6f}, test_acc:{vali_acc:.6f}')\n",
    "    \n",
    "    return train_loss, train_acc, vali_loss, vali_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9bf77-c20d-484b-932b-8b98917f374b",
   "metadata": {},
   "source": [
    "# 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1e1611-fac8-4210-b800-bf02914ed6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete_graph = False\n",
    "self_loop_only = False\n",
    "emo_dim = 0\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "edge_index = get_edge_index(create_complete_graph=create_complete_graph, self_loop_only=self_loop_only)\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, edge_index=edge_index)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, edge_index=edge_index)\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3f3d55-fc83-475e-a697-9b8a12789830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "crit = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:1, train_loss:0.018787, train_acc:0.668287, test_loss:0.016049, test_acc:0.747135\n",
      "->epoch:2, train_loss:0.013750, train_acc:0.792549, test_loss:0.012537, test_acc:0.817708\n",
      "->epoch:3, train_loss:0.010569, train_acc:0.848669, test_loss:0.011563, test_acc:0.842969\n",
      "->epoch:4, train_loss:0.008929, train_acc:0.877734, test_loss:0.010657, test_acc:0.852734\n",
      "->epoch:5, train_loss:0.007764, train_acc:0.895298, test_loss:0.011569, test_acc:0.847005\n",
      "->epoch:6, train_loss:0.006855, train_acc:0.909505, test_loss:0.009483, test_acc:0.877995\n",
      "->epoch:7, train_loss:0.006265, train_acc:0.916999, test_loss:0.010241, test_acc:0.871484\n",
      "->epoch:8, train_loss:0.005754, train_acc:0.923669, test_loss:0.008613, test_acc:0.887760\n",
      "->epoch:9, train_loss:0.005399, train_acc:0.928472, test_loss:0.008488, test_acc:0.892839\n",
      "->epoch:10, train_loss:0.004973, train_acc:0.934823, test_loss:0.008635, test_acc:0.894141\n",
      "->epoch:11, train_loss:0.004695, train_acc:0.939222, test_loss:0.008537, test_acc:0.895313\n",
      "->epoch:12, train_loss:0.004434, train_acc:0.943258, test_loss:0.009000, test_acc:0.897656\n",
      "->epoch:13, train_loss:0.004224, train_acc:0.945587, test_loss:0.009223, test_acc:0.890755\n",
      "->epoch:14, train_loss:0.004085, train_acc:0.946803, test_loss:0.008431, test_acc:0.894531\n",
      "->epoch:15, train_loss:0.003866, train_acc:0.950362, test_loss:0.008066, test_acc:0.907943\n",
      "->epoch:16, train_loss:0.003800, train_acc:0.952286, test_loss:0.009139, test_acc:0.892188\n",
      "->epoch:17, train_loss:0.003533, train_acc:0.954977, test_loss:0.008577, test_acc:0.897005\n",
      "->epoch:18, train_loss:0.003449, train_acc:0.955613, test_loss:0.008240, test_acc:0.905078\n",
      "->epoch:19, train_loss:0.003326, train_acc:0.957697, test_loss:0.008444, test_acc:0.902083\n",
      "->epoch:20, train_loss:0.003215, train_acc:0.958536, test_loss:0.007689, test_acc:0.902995\n",
      "->epoch:21, train_loss:0.003227, train_acc:0.959086, test_loss:0.007766, test_acc:0.910286\n",
      "->epoch:22, train_loss:0.003026, train_acc:0.962052, test_loss:0.008209, test_acc:0.909766\n",
      "->epoch:23, train_loss:0.002917, train_acc:0.963180, test_loss:0.008858, test_acc:0.905990\n",
      "->epoch:24, train_loss:0.002855, train_acc:0.963932, test_loss:0.007641, test_acc:0.911719\n",
      "->epoch:25, train_loss:0.002716, train_acc:0.966536, test_loss:0.009227, test_acc:0.908073\n",
      "->epoch:26, train_loss:0.002685, train_acc:0.966609, test_loss:0.008584, test_acc:0.910677\n",
      "->epoch:27, train_loss:0.002682, train_acc:0.966970, test_loss:0.008286, test_acc:0.907682\n",
      "->epoch:28, train_loss:0.002589, train_acc:0.968186, test_loss:0.008440, test_acc:0.912891\n",
      "->epoch:29, train_loss:0.002516, train_acc:0.969227, test_loss:0.007736, test_acc:0.914453\n",
      "->epoch:30, train_loss:0.002555, train_acc:0.968996, test_loss:0.008266, test_acc:0.915365\n",
      "->epoch:31, train_loss:0.002369, train_acc:0.971267, test_loss:0.008286, test_acc:0.917188\n",
      "->epoch:32, train_loss:0.002379, train_acc:0.970718, test_loss:0.008401, test_acc:0.911328\n",
      "->epoch:33, train_loss:0.002277, train_acc:0.972106, test_loss:0.007917, test_acc:0.911198\n",
      "->epoch:34, train_loss:0.002265, train_acc:0.972584, test_loss:0.008375, test_acc:0.918359\n",
      "->epoch:35, train_loss:0.002259, train_acc:0.972237, test_loss:0.008022, test_acc:0.917448\n",
      "->epoch:36, train_loss:0.002144, train_acc:0.973915, test_loss:0.008834, test_acc:0.915365\n",
      "->epoch:37, train_loss:0.002110, train_acc:0.974595, test_loss:0.007960, test_acc:0.916276\n",
      "->epoch:38, train_loss:0.002105, train_acc:0.975101, test_loss:0.007758, test_acc:0.914714\n",
      "->epoch:39, train_loss:0.002114, train_acc:0.973683, test_loss:0.008833, test_acc:0.915104\n",
      "->epoch:40, train_loss:0.002043, train_acc:0.975738, test_loss:0.008791, test_acc:0.913802\n",
      "->epoch:41, train_loss:0.002027, train_acc:0.975593, test_loss:0.009037, test_acc:0.912109\n",
      "->epoch:42, train_loss:0.001995, train_acc:0.975347, test_loss:0.008634, test_acc:0.916276\n",
      "->epoch:43, train_loss:0.001960, train_acc:0.976360, test_loss:0.008052, test_acc:0.923828\n",
      "->epoch:44, train_loss:0.001936, train_acc:0.976794, test_loss:0.010042, test_acc:0.915104\n",
      "->epoch:45, train_loss:0.001886, train_acc:0.977575, test_loss:0.008104, test_acc:0.916276\n",
      "->epoch:46, train_loss:0.001827, train_acc:0.977156, test_loss:0.008257, test_acc:0.918620\n",
      "->epoch:47, train_loss:0.001814, train_acc:0.978574, test_loss:0.008314, test_acc:0.917318\n",
      "->epoch:48, train_loss:0.001837, train_acc:0.977749, test_loss:0.008948, test_acc:0.916146\n",
      "->epoch:49, train_loss:0.001819, train_acc:0.978342, test_loss:0.008437, test_acc:0.921354\n",
      "->epoch:50, train_loss:0.001811, train_acc:0.978241, test_loss:0.009205, test_acc:0.916797\n",
      "->epoch:51, train_loss:0.001802, train_acc:0.978356, test_loss:0.009014, test_acc:0.918229\n",
      "->epoch:52, train_loss:0.001682, train_acc:0.979760, test_loss:0.009490, test_acc:0.920443\n",
      "->epoch:53, train_loss:0.001715, train_acc:0.979311, test_loss:0.009575, test_acc:0.916537\n",
      "->epoch:54, train_loss:0.001662, train_acc:0.980064, test_loss:0.009233, test_acc:0.918359\n",
      "->epoch:55, train_loss:0.001631, train_acc:0.980816, test_loss:0.009553, test_acc:0.918620\n",
      "->epoch:56, train_loss:0.001667, train_acc:0.980642, test_loss:0.010876, test_acc:0.918099\n",
      "->epoch:57, train_loss:0.001590, train_acc:0.981236, test_loss:0.008320, test_acc:0.924870\n",
      "->epoch:58, train_loss:0.001596, train_acc:0.981337, test_loss:0.009567, test_acc:0.912240\n",
      "->epoch:59, train_loss:0.001544, train_acc:0.981554, test_loss:0.009634, test_acc:0.925521\n",
      "->epoch:60, train_loss:0.001521, train_acc:0.981887, test_loss:0.009334, test_acc:0.922656\n",
      "->epoch:61, train_loss:0.001549, train_acc:0.981930, test_loss:0.010615, test_acc:0.911849\n",
      "->epoch:62, train_loss:0.001560, train_acc:0.982118, test_loss:0.009438, test_acc:0.919010\n",
      "->epoch:63, train_loss:0.001505, train_acc:0.982422, test_loss:0.009191, test_acc:0.918490\n",
      "->epoch:64, train_loss:0.001525, train_acc:0.982523, test_loss:0.009195, test_acc:0.920052\n",
      "->epoch:65, train_loss:0.001440, train_acc:0.983275, test_loss:0.010102, test_acc:0.920703\n",
      "->epoch:66, train_loss:0.001467, train_acc:0.982885, test_loss:0.009101, test_acc:0.916797\n",
      "->epoch:67, train_loss:0.001484, train_acc:0.983073, test_loss:0.009412, test_acc:0.922656\n",
      "->epoch:68, train_loss:0.001486, train_acc:0.982856, test_loss:0.008785, test_acc:0.921224\n",
      "->epoch:69, train_loss:0.001465, train_acc:0.982986, test_loss:0.008880, test_acc:0.923958\n",
      "->epoch:70, train_loss:0.001450, train_acc:0.982986, test_loss:0.010166, test_acc:0.920833\n",
      "->epoch:71, train_loss:0.001446, train_acc:0.983550, test_loss:0.008775, test_acc:0.922135\n",
      "->epoch:72, train_loss:0.001345, train_acc:0.984303, test_loss:0.010019, test_acc:0.919401\n",
      "->epoch:73, train_loss:0.001373, train_acc:0.984028, test_loss:0.009475, test_acc:0.922396\n",
      "->epoch:74, train_loss:0.001408, train_acc:0.983941, test_loss:0.009062, test_acc:0.922135\n",
      "->epoch:75, train_loss:0.001388, train_acc:0.983926, test_loss:0.009889, test_acc:0.925130\n",
      "->epoch:76, train_loss:0.001329, train_acc:0.984621, test_loss:0.010890, test_acc:0.920703\n",
      "->epoch:77, train_loss:0.001356, train_acc:0.984433, test_loss:0.008756, test_acc:0.924349\n",
      "->epoch:78, train_loss:0.001287, train_acc:0.984925, test_loss:0.009742, test_acc:0.924219\n",
      "->epoch:79, train_loss:0.001384, train_acc:0.984462, test_loss:0.011423, test_acc:0.919792\n",
      "->epoch:80, train_loss:0.001346, train_acc:0.984693, test_loss:0.009881, test_acc:0.922917\n",
      "->epoch:81, train_loss:0.001345, train_acc:0.984404, test_loss:0.008825, test_acc:0.926432\n",
      "->epoch:82, train_loss:0.001335, train_acc:0.985026, test_loss:0.009311, test_acc:0.927214\n",
      "->epoch:83, train_loss:0.001296, train_acc:0.985127, test_loss:0.010262, test_acc:0.921484\n",
      "->epoch:84, train_loss:0.001210, train_acc:0.985937, test_loss:0.009150, test_acc:0.922656\n",
      "->epoch:85, train_loss:0.001273, train_acc:0.984954, test_loss:0.011209, test_acc:0.923568\n",
      "->epoch:86, train_loss:0.001251, train_acc:0.985576, test_loss:0.009152, test_acc:0.928776\n",
      "->epoch:87, train_loss:0.001253, train_acc:0.986053, test_loss:0.010275, test_acc:0.920313\n",
      "->epoch:88, train_loss:0.001268, train_acc:0.985503, test_loss:0.009763, test_acc:0.924609\n",
      "->epoch:89, train_loss:0.001211, train_acc:0.986343, test_loss:0.009002, test_acc:0.925130\n",
      "->epoch:90, train_loss:0.001242, train_acc:0.985822, test_loss:0.009069, test_acc:0.918099\n",
      "->epoch:91, train_loss:0.001206, train_acc:0.986039, test_loss:0.009962, test_acc:0.922135\n",
      "->epoch:92, train_loss:0.001208, train_acc:0.986126, test_loss:0.010182, test_acc:0.924349\n",
      "->epoch:93, train_loss:0.001245, train_acc:0.985749, test_loss:0.009397, test_acc:0.922266\n",
      "->epoch:94, train_loss:0.001166, train_acc:0.986892, test_loss:0.009669, test_acc:0.927344\n",
      "->epoch:95, train_loss:0.001201, train_acc:0.987008, test_loss:0.010173, test_acc:0.922396\n",
      "->epoch:96, train_loss:0.001184, train_acc:0.986965, test_loss:0.009242, test_acc:0.923438\n",
      "->epoch:97, train_loss:0.001215, train_acc:0.986690, test_loss:0.009494, test_acc:0.918359\n",
      "->epoch:98, train_loss:0.001151, train_acc:0.986936, test_loss:0.010087, test_acc:0.923568\n",
      "->epoch:99, train_loss:0.001123, train_acc:0.986979, test_loss:0.010362, test_acc:0.920443\n",
      "->epoch:100, train_loss:0.001101, train_acc:0.987297, test_loss:0.011021, test_acc:0.919271\n",
      "->epoch:101, train_loss:0.001204, train_acc:0.986372, test_loss:0.010354, test_acc:0.922266\n",
      "->epoch:102, train_loss:0.001179, train_acc:0.986603, test_loss:0.009635, test_acc:0.929427\n",
      "->epoch:103, train_loss:0.001088, train_acc:0.987948, test_loss:0.008984, test_acc:0.923698\n",
      "->epoch:104, train_loss:0.001152, train_acc:0.987341, test_loss:0.008648, test_acc:0.923698\n",
      "->epoch:105, train_loss:0.001077, train_acc:0.987572, test_loss:0.013020, test_acc:0.919271\n",
      "->epoch:106, train_loss:0.001094, train_acc:0.987457, test_loss:0.010249, test_acc:0.925911\n",
      "->epoch:107, train_loss:0.001130, train_acc:0.987659, test_loss:0.009841, test_acc:0.920443\n",
      "->epoch:108, train_loss:0.001114, train_acc:0.987384, test_loss:0.010224, test_acc:0.922656\n",
      "->epoch:109, train_loss:0.001079, train_acc:0.987442, test_loss:0.010332, test_acc:0.921354\n",
      "->epoch:110, train_loss:0.001087, train_acc:0.987847, test_loss:0.009891, test_acc:0.924349\n",
      "->epoch:111, train_loss:0.001136, train_acc:0.987457, test_loss:0.009874, test_acc:0.924870\n",
      "->epoch:112, train_loss:0.001058, train_acc:0.988180, test_loss:0.009949, test_acc:0.923177\n",
      "->epoch:113, train_loss:0.001033, train_acc:0.988281, test_loss:0.011527, test_acc:0.919792\n",
      "->epoch:114, train_loss:0.001108, train_acc:0.987326, test_loss:0.009550, test_acc:0.923307\n",
      "->epoch:115, train_loss:0.001047, train_acc:0.988021, test_loss:0.010008, test_acc:0.928255\n",
      "->epoch:116, train_loss:0.001112, train_acc:0.987789, test_loss:0.010441, test_acc:0.921615\n",
      "->epoch:117, train_loss:0.001084, train_acc:0.987847, test_loss:0.010862, test_acc:0.924609\n",
      "->epoch:118, train_loss:0.001054, train_acc:0.988426, test_loss:0.010865, test_acc:0.922526\n",
      "->epoch:119, train_loss:0.001070, train_acc:0.987746, test_loss:0.009890, test_acc:0.920573\n",
      "->epoch:120, train_loss:0.001018, train_acc:0.987948, test_loss:0.010848, test_acc:0.926172\n",
      "->epoch:121, train_loss:0.001001, train_acc:0.988657, test_loss:0.010289, test_acc:0.921745\n",
      "->epoch:122, train_loss:0.001055, train_acc:0.988180, test_loss:0.011396, test_acc:0.920573\n",
      "->epoch:123, train_loss:0.001023, train_acc:0.988614, test_loss:0.009693, test_acc:0.926563\n",
      "->epoch:124, train_loss:0.001033, train_acc:0.989048, test_loss:0.009113, test_acc:0.926953\n",
      "->epoch:125, train_loss:0.001069, train_acc:0.988108, test_loss:0.008939, test_acc:0.925521\n",
      "->epoch:126, train_loss:0.001019, train_acc:0.988180, test_loss:0.010135, test_acc:0.925130\n",
      "->epoch:127, train_loss:0.001001, train_acc:0.989077, test_loss:0.009195, test_acc:0.930208\n",
      "->epoch:128, train_loss:0.001026, train_acc:0.988542, test_loss:0.009938, test_acc:0.923698\n",
      "->epoch:129, train_loss:0.000994, train_acc:0.988845, test_loss:0.011873, test_acc:0.925911\n",
      "->epoch:130, train_loss:0.000983, train_acc:0.989005, test_loss:0.011343, test_acc:0.917969\n",
      "->epoch:131, train_loss:0.001026, train_acc:0.988527, test_loss:0.010473, test_acc:0.923438\n",
      "->epoch:132, train_loss:0.001002, train_acc:0.989193, test_loss:0.009689, test_acc:0.924479\n",
      "->epoch:133, train_loss:0.001009, train_acc:0.988600, test_loss:0.011606, test_acc:0.924089\n",
      "->epoch:134, train_loss:0.000985, train_acc:0.989280, test_loss:0.009856, test_acc:0.925000\n",
      "->epoch:135, train_loss:0.000964, train_acc:0.989091, test_loss:0.009793, test_acc:0.929167\n",
      "->epoch:136, train_loss:0.000942, train_acc:0.989757, test_loss:0.009638, test_acc:0.932162\n",
      "->epoch:137, train_loss:0.000940, train_acc:0.989468, test_loss:0.010542, test_acc:0.925391\n",
      "->epoch:138, train_loss:0.001012, train_acc:0.988238, test_loss:0.008573, test_acc:0.923958\n",
      "->epoch:139, train_loss:0.000991, train_acc:0.988759, test_loss:0.010710, test_acc:0.924349\n",
      "->epoch:140, train_loss:0.000875, train_acc:0.989974, test_loss:0.010850, test_acc:0.931120\n",
      "->epoch:141, train_loss:0.000939, train_acc:0.989265, test_loss:0.010419, test_acc:0.926302\n",
      "->epoch:142, train_loss:0.000993, train_acc:0.989034, test_loss:0.010757, test_acc:0.925781\n",
      "->epoch:143, train_loss:0.000985, train_acc:0.988860, test_loss:0.009468, test_acc:0.926823\n",
      "->epoch:144, train_loss:0.000921, train_acc:0.989453, test_loss:0.010675, test_acc:0.926042\n",
      "->epoch:145, train_loss:0.000971, train_acc:0.989106, test_loss:0.010526, test_acc:0.931641\n",
      "->epoch:146, train_loss:0.000905, train_acc:0.989482, test_loss:0.010637, test_acc:0.926172\n",
      "->epoch:147, train_loss:0.000947, train_acc:0.989771, test_loss:0.010168, test_acc:0.931380\n",
      "->epoch:148, train_loss:0.000920, train_acc:0.989858, test_loss:0.011104, test_acc:0.927474\n",
      "->epoch:149, train_loss:0.000940, train_acc:0.989685, test_loss:0.010653, test_acc:0.925521\n",
      "->epoch:150, train_loss:0.000826, train_acc:0.990697, test_loss:0.010676, test_acc:0.929557\n",
      "->epoch:151, train_loss:0.000931, train_acc:0.990017, test_loss:0.010078, test_acc:0.926172\n",
      "->epoch:152, train_loss:0.000885, train_acc:0.990336, test_loss:0.010516, test_acc:0.926432\n",
      "->epoch:153, train_loss:0.000900, train_acc:0.989945, test_loss:0.010282, test_acc:0.929688\n",
      "->epoch:154, train_loss:0.000913, train_acc:0.989742, test_loss:0.010814, test_acc:0.919010\n",
      "->epoch:155, train_loss:0.000938, train_acc:0.989916, test_loss:0.010379, test_acc:0.925911\n",
      "->epoch:156, train_loss:0.000856, train_acc:0.990249, test_loss:0.010767, test_acc:0.926302\n",
      "->epoch:157, train_loss:0.000984, train_acc:0.989714, test_loss:0.009187, test_acc:0.923958\n",
      "->epoch:158, train_loss:0.000904, train_acc:0.990336, test_loss:0.009415, test_acc:0.926042\n",
      "->epoch:159, train_loss:0.000884, train_acc:0.990148, test_loss:0.013759, test_acc:0.923568\n",
      "->epoch:160, train_loss:0.000925, train_acc:0.989714, test_loss:0.009947, test_acc:0.924479\n",
      "->epoch:161, train_loss:0.000873, train_acc:0.990263, test_loss:0.010143, test_acc:0.926042\n",
      "->epoch:162, train_loss:0.000957, train_acc:0.989034, test_loss:0.009517, test_acc:0.929557\n",
      "->epoch:163, train_loss:0.000871, train_acc:0.990292, test_loss:0.010617, test_acc:0.924870\n",
      "->epoch:164, train_loss:0.000854, train_acc:0.990278, test_loss:0.012671, test_acc:0.925260\n",
      "->epoch:165, train_loss:0.000924, train_acc:0.990321, test_loss:0.010352, test_acc:0.921484\n",
      "->epoch:166, train_loss:0.000932, train_acc:0.989858, test_loss:0.011535, test_acc:0.924479\n",
      "->epoch:167, train_loss:0.000959, train_acc:0.989742, test_loss:0.010028, test_acc:0.928906\n",
      "->epoch:168, train_loss:0.000846, train_acc:0.990712, test_loss:0.010106, test_acc:0.926953\n",
      "->epoch:169, train_loss:0.000886, train_acc:0.990220, test_loss:0.009983, test_acc:0.926693\n",
      "->epoch:170, train_loss:0.000822, train_acc:0.991493, test_loss:0.011059, test_acc:0.929948\n",
      "->epoch:171, train_loss:0.000841, train_acc:0.990394, test_loss:0.011281, test_acc:0.924740\n",
      "->epoch:172, train_loss:0.000878, train_acc:0.990220, test_loss:0.010869, test_acc:0.924349\n",
      "->epoch:173, train_loss:0.000960, train_acc:0.989366, test_loss:0.010290, test_acc:0.923828\n",
      "->epoch:174, train_loss:0.000838, train_acc:0.991073, test_loss:0.010271, test_acc:0.927344\n",
      "->epoch:175, train_loss:0.000881, train_acc:0.990408, test_loss:0.010426, test_acc:0.929948\n",
      "->epoch:176, train_loss:0.000848, train_acc:0.991175, test_loss:0.011722, test_acc:0.928255\n",
      "->epoch:177, train_loss:0.000853, train_acc:0.990639, test_loss:0.013605, test_acc:0.920052\n",
      "->epoch:178, train_loss:0.000866, train_acc:0.990784, test_loss:0.011335, test_acc:0.927604\n",
      "->epoch:179, train_loss:0.000871, train_acc:0.990524, test_loss:0.010449, test_acc:0.931510\n",
      "->epoch:180, train_loss:0.000868, train_acc:0.990350, test_loss:0.010097, test_acc:0.929427\n",
      "->epoch:181, train_loss:0.000826, train_acc:0.991348, test_loss:0.010103, test_acc:0.925391\n",
      "->epoch:182, train_loss:0.000807, train_acc:0.990958, test_loss:0.010615, test_acc:0.930599\n",
      "->epoch:183, train_loss:0.000872, train_acc:0.990509, test_loss:0.010738, test_acc:0.926172\n",
      "->epoch:184, train_loss:0.000814, train_acc:0.991059, test_loss:0.012444, test_acc:0.923958\n",
      "->epoch:185, train_loss:0.000841, train_acc:0.990943, test_loss:0.010057, test_acc:0.926823\n",
      "->epoch:186, train_loss:0.000823, train_acc:0.990813, test_loss:0.010104, test_acc:0.925781\n",
      "->epoch:187, train_loss:0.000878, train_acc:0.990697, test_loss:0.010703, test_acc:0.931250\n",
      "->epoch:188, train_loss:0.000826, train_acc:0.990914, test_loss:0.011441, test_acc:0.921224\n",
      "->epoch:189, train_loss:0.000786, train_acc:0.991392, test_loss:0.011113, test_acc:0.927214\n",
      "->epoch:190, train_loss:0.000833, train_acc:0.990654, test_loss:0.010414, test_acc:0.925260\n",
      "->epoch:191, train_loss:0.000860, train_acc:0.990596, test_loss:0.012102, test_acc:0.925130\n",
      "->epoch:192, train_loss:0.000821, train_acc:0.991204, test_loss:0.009401, test_acc:0.926953\n",
      "->epoch:193, train_loss:0.000759, train_acc:0.991898, test_loss:0.010446, test_acc:0.925521\n",
      "->epoch:194, train_loss:0.000810, train_acc:0.990972, test_loss:0.011208, test_acc:0.930469\n",
      "->epoch:195, train_loss:0.000820, train_acc:0.990856, test_loss:0.009604, test_acc:0.928646\n",
      "->epoch:196, train_loss:0.000839, train_acc:0.990668, test_loss:0.012833, test_acc:0.922396\n",
      "->epoch:197, train_loss:0.000797, train_acc:0.990770, test_loss:0.010782, test_acc:0.925781\n",
      "->epoch:198, train_loss:0.000752, train_acc:0.991710, test_loss:0.011316, test_acc:0.932292\n",
      "->epoch:199, train_loss:0.000790, train_acc:0.991565, test_loss:0.011118, test_acc:0.929557\n",
      "->epoch:200, train_loss:0.000766, train_acc:0.992014, test_loss:0.012976, test_acc:0.928646\n",
      "->epoch:201, train_loss:0.000827, train_acc:0.990697, test_loss:0.012240, test_acc:0.927214\n",
      "->epoch:202, train_loss:0.000788, train_acc:0.991073, test_loss:0.010625, test_acc:0.928516\n",
      "->epoch:203, train_loss:0.000750, train_acc:0.991667, test_loss:0.011394, test_acc:0.929688\n",
      "->epoch:204, train_loss:0.000812, train_acc:0.991334, test_loss:0.010975, test_acc:0.929036\n",
      "->epoch:205, train_loss:0.000738, train_acc:0.991681, test_loss:0.012671, test_acc:0.931250\n",
      "->epoch:206, train_loss:0.000770, train_acc:0.991840, test_loss:0.010706, test_acc:0.927083\n",
      "->epoch:207, train_loss:0.000781, train_acc:0.991898, test_loss:0.010682, test_acc:0.932552\n",
      "->epoch:208, train_loss:0.000807, train_acc:0.991117, test_loss:0.011598, test_acc:0.924349\n",
      "->epoch:209, train_loss:0.000730, train_acc:0.991869, test_loss:0.011019, test_acc:0.927474\n",
      "->epoch:210, train_loss:0.000789, train_acc:0.991363, test_loss:0.010133, test_acc:0.930339\n",
      "->epoch:211, train_loss:0.000803, train_acc:0.991160, test_loss:0.010593, test_acc:0.930469\n",
      "->epoch:212, train_loss:0.000823, train_acc:0.991435, test_loss:0.011547, test_acc:0.924219\n",
      "->epoch:213, train_loss:0.000791, train_acc:0.991175, test_loss:0.012336, test_acc:0.932031\n",
      "->epoch:214, train_loss:0.000802, train_acc:0.991551, test_loss:0.011535, test_acc:0.925781\n",
      "->epoch:215, train_loss:0.000777, train_acc:0.991855, test_loss:0.013474, test_acc:0.924349\n",
      "->epoch:216, train_loss:0.000786, train_acc:0.991493, test_loss:0.011358, test_acc:0.929557\n",
      "->epoch:217, train_loss:0.000806, train_acc:0.991536, test_loss:0.008995, test_acc:0.931250\n",
      "->epoch:218, train_loss:0.000775, train_acc:0.991276, test_loss:0.010853, test_acc:0.928776\n",
      "->epoch:219, train_loss:0.000780, train_acc:0.991479, test_loss:0.011469, test_acc:0.926693\n",
      "->epoch:220, train_loss:0.000746, train_acc:0.991696, test_loss:0.010354, test_acc:0.929167\n",
      "->epoch:221, train_loss:0.000735, train_acc:0.992274, test_loss:0.012728, test_acc:0.925130\n",
      "->epoch:222, train_loss:0.000781, train_acc:0.991855, test_loss:0.011142, test_acc:0.929297\n",
      "->epoch:223, train_loss:0.000776, train_acc:0.992130, test_loss:0.011463, test_acc:0.923438\n",
      "->epoch:224, train_loss:0.000765, train_acc:0.991970, test_loss:0.011206, test_acc:0.932813\n",
      "->epoch:225, train_loss:0.000721, train_acc:0.992477, test_loss:0.011602, test_acc:0.927604\n",
      "->epoch:226, train_loss:0.000731, train_acc:0.991753, test_loss:0.011286, test_acc:0.932162\n",
      "->epoch:227, train_loss:0.000760, train_acc:0.991493, test_loss:0.011922, test_acc:0.926953\n",
      "->epoch:228, train_loss:0.000740, train_acc:0.992014, test_loss:0.014382, test_acc:0.926953\n",
      "->epoch:229, train_loss:0.000748, train_acc:0.991884, test_loss:0.013528, test_acc:0.929036\n",
      "->epoch:230, train_loss:0.000777, train_acc:0.992101, test_loss:0.012470, test_acc:0.930208\n",
      "->epoch:231, train_loss:0.000790, train_acc:0.991580, test_loss:0.011133, test_acc:0.929818\n",
      "->epoch:232, train_loss:0.000749, train_acc:0.991855, test_loss:0.010909, test_acc:0.926953\n",
      "->epoch:233, train_loss:0.000748, train_acc:0.992028, test_loss:0.011629, test_acc:0.926693\n",
      "->epoch:234, train_loss:0.000742, train_acc:0.992159, test_loss:0.011722, test_acc:0.930990\n",
      "->epoch:235, train_loss:0.000756, train_acc:0.992477, test_loss:0.013738, test_acc:0.929036\n",
      "->epoch:236, train_loss:0.000788, train_acc:0.991450, test_loss:0.011133, test_acc:0.930208\n",
      "->epoch:237, train_loss:0.000735, train_acc:0.991927, test_loss:0.012938, test_acc:0.928906\n",
      "->epoch:238, train_loss:0.000765, train_acc:0.992086, test_loss:0.010586, test_acc:0.927734\n",
      "->epoch:239, train_loss:0.000748, train_acc:0.991710, test_loss:0.011635, test_acc:0.928516\n",
      "->epoch:240, train_loss:0.000775, train_acc:0.991985, test_loss:0.011655, test_acc:0.931120\n",
      "->epoch:241, train_loss:0.000817, train_acc:0.991565, test_loss:0.010572, test_acc:0.932813\n",
      "->epoch:242, train_loss:0.000703, train_acc:0.992636, test_loss:0.010282, test_acc:0.926693\n",
      "->epoch:243, train_loss:0.000806, train_acc:0.991536, test_loss:0.012129, test_acc:0.928776\n",
      "->epoch:244, train_loss:0.000784, train_acc:0.991753, test_loss:0.011255, test_acc:0.926693\n",
      "->epoch:245, train_loss:0.000807, train_acc:0.991088, test_loss:0.010889, test_acc:0.930469\n",
      "->epoch:246, train_loss:0.000678, train_acc:0.992491, test_loss:0.010522, test_acc:0.929557\n",
      "->epoch:247, train_loss:0.000690, train_acc:0.992274, test_loss:0.011568, test_acc:0.932943\n",
      "->epoch:248, train_loss:0.000722, train_acc:0.992014, test_loss:0.010683, test_acc:0.932813\n",
      "->epoch:249, train_loss:0.000713, train_acc:0.992390, test_loss:0.012283, test_acc:0.928125\n",
      "->epoch:250, train_loss:0.000787, train_acc:0.991913, test_loss:0.010928, test_acc:0.931380\n",
      "->epoch:251, train_loss:0.000718, train_acc:0.992405, test_loss:0.011153, test_acc:0.930078\n",
      "->epoch:252, train_loss:0.000774, train_acc:0.991898, test_loss:0.008260, test_acc:0.933203\n",
      "->epoch:253, train_loss:0.000758, train_acc:0.992101, test_loss:0.011768, test_acc:0.933073\n",
      "->epoch:254, train_loss:0.000743, train_acc:0.991811, test_loss:0.011055, test_acc:0.925260\n",
      "->epoch:255, train_loss:0.000759, train_acc:0.992028, test_loss:0.010769, test_acc:0.932682\n",
      "->epoch:256, train_loss:0.000714, train_acc:0.992260, test_loss:0.011425, test_acc:0.931901\n",
      "->epoch:257, train_loss:0.000783, train_acc:0.992072, test_loss:0.011866, test_acc:0.930729\n",
      "->epoch:258, train_loss:0.000698, train_acc:0.992665, test_loss:0.009728, test_acc:0.929688\n",
      "->epoch:259, train_loss:0.000718, train_acc:0.992462, test_loss:0.009982, test_acc:0.929557\n",
      "->epoch:260, train_loss:0.000700, train_acc:0.992173, test_loss:0.013818, test_acc:0.926042\n",
      "->epoch:261, train_loss:0.000734, train_acc:0.992173, test_loss:0.014426, test_acc:0.925391\n",
      "->epoch:262, train_loss:0.000772, train_acc:0.992101, test_loss:0.009322, test_acc:0.928125\n",
      "->epoch:263, train_loss:0.000684, train_acc:0.992593, test_loss:0.012128, test_acc:0.924740\n",
      "->epoch:264, train_loss:0.000650, train_acc:0.992839, test_loss:0.012297, test_acc:0.931771\n",
      "->epoch:265, train_loss:0.000671, train_acc:0.992231, test_loss:0.013414, test_acc:0.926432\n",
      "->epoch:266, train_loss:0.000717, train_acc:0.992043, test_loss:0.011919, test_acc:0.925651\n",
      "->epoch:267, train_loss:0.000669, train_acc:0.992781, test_loss:0.013865, test_acc:0.927474\n",
      "->epoch:268, train_loss:0.000739, train_acc:0.992274, test_loss:0.013081, test_acc:0.926693\n",
      "->epoch:269, train_loss:0.000730, train_acc:0.992824, test_loss:0.010890, test_acc:0.930469\n",
      "->epoch:270, train_loss:0.000669, train_acc:0.992607, test_loss:0.011068, test_acc:0.931901\n",
      "->epoch:271, train_loss:0.000760, train_acc:0.992028, test_loss:0.011797, test_acc:0.927474\n",
      "->epoch:272, train_loss:0.000771, train_acc:0.991565, test_loss:0.011472, test_acc:0.924479\n",
      "->epoch:273, train_loss:0.000735, train_acc:0.992130, test_loss:0.011796, test_acc:0.935417\n",
      "->epoch:274, train_loss:0.000680, train_acc:0.993142, test_loss:0.012484, test_acc:0.932162\n",
      "->epoch:275, train_loss:0.000731, train_acc:0.992419, test_loss:0.013720, test_acc:0.926042\n",
      "->epoch:276, train_loss:0.000774, train_acc:0.991869, test_loss:0.011181, test_acc:0.928516\n",
      "->epoch:277, train_loss:0.000705, train_acc:0.992593, test_loss:0.011121, test_acc:0.928516\n",
      "->epoch:278, train_loss:0.000704, train_acc:0.992564, test_loss:0.012435, test_acc:0.929948\n",
      "->epoch:279, train_loss:0.000762, train_acc:0.992274, test_loss:0.012310, test_acc:0.927474\n",
      "->epoch:280, train_loss:0.000693, train_acc:0.992650, test_loss:0.011461, test_acc:0.928646\n",
      "->epoch:281, train_loss:0.000691, train_acc:0.992694, test_loss:0.011538, test_acc:0.930859\n",
      "->epoch:282, train_loss:0.000758, train_acc:0.992231, test_loss:0.010087, test_acc:0.929427\n",
      "->epoch:283, train_loss:0.000683, train_acc:0.993157, test_loss:0.012902, test_acc:0.930729\n",
      "->epoch:284, train_loss:0.000720, train_acc:0.992795, test_loss:0.011575, test_acc:0.926563\n",
      "->epoch:285, train_loss:0.000699, train_acc:0.992477, test_loss:0.012020, test_acc:0.926563\n",
      "->epoch:286, train_loss:0.000674, train_acc:0.992650, test_loss:0.011142, test_acc:0.931641\n",
      "->epoch:287, train_loss:0.000701, train_acc:0.992405, test_loss:0.013419, test_acc:0.925260\n",
      "->epoch:288, train_loss:0.000763, train_acc:0.992144, test_loss:0.011550, test_acc:0.925911\n",
      "->epoch:289, train_loss:0.000711, train_acc:0.992752, test_loss:0.011668, test_acc:0.927604\n",
      "->epoch:290, train_loss:0.000671, train_acc:0.993142, test_loss:0.012111, test_acc:0.923958\n",
      "->epoch:291, train_loss:0.000727, train_acc:0.992737, test_loss:0.014872, test_acc:0.930990\n",
      "->epoch:292, train_loss:0.000725, train_acc:0.992303, test_loss:0.012777, test_acc:0.928385\n",
      "->epoch:293, train_loss:0.000646, train_acc:0.992969, test_loss:0.012896, test_acc:0.929297\n",
      "->epoch:294, train_loss:0.000663, train_acc:0.993084, test_loss:0.012907, test_acc:0.927865\n",
      "->epoch:295, train_loss:0.000689, train_acc:0.992462, test_loss:0.012812, test_acc:0.929297\n",
      "->epoch:296, train_loss:0.000687, train_acc:0.992477, test_loss:0.014104, test_acc:0.927474\n",
      "->epoch:297, train_loss:0.000628, train_acc:0.993186, test_loss:0.012165, test_acc:0.925911\n",
      "->epoch:298, train_loss:0.000705, train_acc:0.992679, test_loss:0.013079, test_acc:0.925521\n",
      "->epoch:299, train_loss:0.000717, train_acc:0.992564, test_loss:0.012018, test_acc:0.931120\n",
      "->epoch:300, train_loss:0.000761, train_acc:0.992202, test_loss:0.013611, test_acc:0.926823\n",
      "->epoch:301, train_loss:0.000674, train_acc:0.992954, test_loss:0.014182, test_acc:0.926172\n",
      "->epoch:302, train_loss:0.000694, train_acc:0.993171, test_loss:0.011301, test_acc:0.927995\n",
      "->epoch:303, train_loss:0.000691, train_acc:0.992781, test_loss:0.011804, test_acc:0.931380\n",
      "->epoch:304, train_loss:0.000695, train_acc:0.992723, test_loss:0.012247, test_acc:0.927604\n",
      "->epoch:305, train_loss:0.000733, train_acc:0.992405, test_loss:0.012173, test_acc:0.929297\n",
      "->epoch:306, train_loss:0.000717, train_acc:0.992578, test_loss:0.011519, test_acc:0.930078\n",
      "->epoch:307, train_loss:0.000731, train_acc:0.991985, test_loss:0.012137, test_acc:0.931120\n",
      "->epoch:308, train_loss:0.000726, train_acc:0.993027, test_loss:0.013052, test_acc:0.923828\n",
      "->epoch:309, train_loss:0.000626, train_acc:0.993490, test_loss:0.012079, test_acc:0.930990\n",
      "->epoch:310, train_loss:0.000669, train_acc:0.992708, test_loss:0.011076, test_acc:0.928646\n",
      "->epoch:311, train_loss:0.000688, train_acc:0.992752, test_loss:0.010518, test_acc:0.933464\n",
      "->epoch:312, train_loss:0.000694, train_acc:0.993113, test_loss:0.010068, test_acc:0.927474\n",
      "->epoch:313, train_loss:0.000668, train_acc:0.992969, test_loss:0.012384, test_acc:0.926563\n",
      "->epoch:314, train_loss:0.000647, train_acc:0.992882, test_loss:0.013489, test_acc:0.927474\n",
      "->epoch:315, train_loss:0.000646, train_acc:0.993374, test_loss:0.010743, test_acc:0.933724\n",
      "->epoch:316, train_loss:0.000663, train_acc:0.993128, test_loss:0.012207, test_acc:0.930339\n",
      "->epoch:317, train_loss:0.000678, train_acc:0.992954, test_loss:0.011556, test_acc:0.929557\n",
      "->epoch:318, train_loss:0.000615, train_acc:0.993244, test_loss:0.012969, test_acc:0.926042\n",
      "->epoch:319, train_loss:0.000628, train_acc:0.993330, test_loss:0.014856, test_acc:0.927083\n",
      "->epoch:320, train_loss:0.000677, train_acc:0.993345, test_loss:0.013018, test_acc:0.927474\n",
      "->epoch:321, train_loss:0.000640, train_acc:0.993273, test_loss:0.011667, test_acc:0.933464\n",
      "->epoch:322, train_loss:0.000691, train_acc:0.992824, test_loss:0.011511, test_acc:0.930859\n",
      "->epoch:323, train_loss:0.000613, train_acc:0.993374, test_loss:0.012805, test_acc:0.928776\n",
      "->epoch:324, train_loss:0.000738, train_acc:0.992535, test_loss:0.010834, test_acc:0.927734\n",
      "->epoch:325, train_loss:0.000700, train_acc:0.992983, test_loss:0.012140, test_acc:0.927214\n",
      "->epoch:326, train_loss:0.000669, train_acc:0.993287, test_loss:0.012400, test_acc:0.935156\n",
      "->epoch:327, train_loss:0.000621, train_acc:0.993866, test_loss:0.012139, test_acc:0.926953\n",
      "->epoch:328, train_loss:0.000630, train_acc:0.993793, test_loss:0.011252, test_acc:0.931901\n",
      "->epoch:329, train_loss:0.000678, train_acc:0.992998, test_loss:0.011204, test_acc:0.929688\n",
      "->epoch:330, train_loss:0.000661, train_acc:0.993258, test_loss:0.011870, test_acc:0.928385\n",
      "->epoch:331, train_loss:0.000700, train_acc:0.992621, test_loss:0.012737, test_acc:0.929948\n",
      "->epoch:332, train_loss:0.000620, train_acc:0.993446, test_loss:0.014058, test_acc:0.929167\n",
      "->epoch:333, train_loss:0.000622, train_acc:0.993215, test_loss:0.011038, test_acc:0.931641\n",
      "->epoch:334, train_loss:0.000686, train_acc:0.992867, test_loss:0.012081, test_acc:0.929427\n",
      "->epoch:335, train_loss:0.000683, train_acc:0.992795, test_loss:0.011236, test_acc:0.930078\n",
      "->epoch:336, train_loss:0.000611, train_acc:0.994054, test_loss:0.015425, test_acc:0.925781\n",
      "->epoch:337, train_loss:0.000702, train_acc:0.993012, test_loss:0.011207, test_acc:0.930599\n",
      "->epoch:338, train_loss:0.000636, train_acc:0.993490, test_loss:0.012072, test_acc:0.926172\n",
      "->epoch:339, train_loss:0.000649, train_acc:0.993113, test_loss:0.012422, test_acc:0.929688\n",
      "->epoch:340, train_loss:0.000683, train_acc:0.992810, test_loss:0.012388, test_acc:0.930990\n",
      "->epoch:341, train_loss:0.000708, train_acc:0.993518, test_loss:0.012036, test_acc:0.931510\n",
      "->epoch:342, train_loss:0.000735, train_acc:0.992188, test_loss:0.013966, test_acc:0.930990\n",
      "->epoch:343, train_loss:0.000651, train_acc:0.993432, test_loss:0.011894, test_acc:0.929818\n",
      "->epoch:344, train_loss:0.000625, train_acc:0.993866, test_loss:0.011604, test_acc:0.932292\n",
      "->epoch:345, train_loss:0.000766, train_acc:0.992766, test_loss:0.013239, test_acc:0.931120\n",
      "->epoch:346, train_loss:0.000646, train_acc:0.993302, test_loss:0.013607, test_acc:0.928776\n",
      "->epoch:347, train_loss:0.000636, train_acc:0.993157, test_loss:0.010688, test_acc:0.933464\n",
      "->epoch:348, train_loss:0.000649, train_acc:0.993287, test_loss:0.013638, test_acc:0.929167\n",
      "->epoch:349, train_loss:0.000718, train_acc:0.993258, test_loss:0.011615, test_acc:0.935287\n",
      "->epoch:350, train_loss:0.000658, train_acc:0.992998, test_loss:0.011142, test_acc:0.931641\n",
      "->epoch:351, train_loss:0.000664, train_acc:0.992998, test_loss:0.011762, test_acc:0.932422\n",
      "->epoch:352, train_loss:0.000628, train_acc:0.993041, test_loss:0.013339, test_acc:0.933333\n",
      "->epoch:353, train_loss:0.000650, train_acc:0.993099, test_loss:0.012640, test_acc:0.927344\n",
      "->epoch:354, train_loss:0.000655, train_acc:0.993547, test_loss:0.011612, test_acc:0.931641\n",
      "->epoch:355, train_loss:0.000646, train_acc:0.993244, test_loss:0.012512, test_acc:0.927995\n",
      "->epoch:356, train_loss:0.000641, train_acc:0.993070, test_loss:0.012709, test_acc:0.926953\n",
      "->epoch:357, train_loss:0.000665, train_acc:0.992925, test_loss:0.013631, test_acc:0.926172\n",
      "->epoch:358, train_loss:0.000614, train_acc:0.993432, test_loss:0.012765, test_acc:0.926432\n",
      "->epoch:359, train_loss:0.000651, train_acc:0.992896, test_loss:0.012737, test_acc:0.928255\n",
      "->epoch:360, train_loss:0.000630, train_acc:0.993403, test_loss:0.012045, test_acc:0.933464\n",
      "->epoch:361, train_loss:0.000603, train_acc:0.993764, test_loss:0.014288, test_acc:0.929688\n",
      "->epoch:362, train_loss:0.000638, train_acc:0.993113, test_loss:0.014571, test_acc:0.927604\n",
      "->epoch:363, train_loss:0.000646, train_acc:0.992954, test_loss:0.011064, test_acc:0.931380\n",
      "->epoch:364, train_loss:0.000629, train_acc:0.993562, test_loss:0.012191, test_acc:0.928906\n",
      "->epoch:365, train_loss:0.000669, train_acc:0.993287, test_loss:0.011839, test_acc:0.933333\n",
      "->epoch:366, train_loss:0.000608, train_acc:0.993620, test_loss:0.013981, test_acc:0.928255\n",
      "->epoch:367, train_loss:0.000646, train_acc:0.993113, test_loss:0.011631, test_acc:0.930339\n",
      "->epoch:368, train_loss:0.000617, train_acc:0.993475, test_loss:0.012444, test_acc:0.932813\n",
      "->epoch:369, train_loss:0.000642, train_acc:0.993403, test_loss:0.011741, test_acc:0.930339\n",
      "->epoch:370, train_loss:0.000615, train_acc:0.993721, test_loss:0.011929, test_acc:0.932422\n",
      "->epoch:371, train_loss:0.000632, train_acc:0.993692, test_loss:0.011393, test_acc:0.935287\n",
      "->epoch:372, train_loss:0.000652, train_acc:0.993692, test_loss:0.012181, test_acc:0.934635\n",
      "->epoch:373, train_loss:0.000586, train_acc:0.993692, test_loss:0.016373, test_acc:0.932813\n",
      "->epoch:374, train_loss:0.000693, train_acc:0.992737, test_loss:0.013022, test_acc:0.928646\n",
      "->epoch:375, train_loss:0.000654, train_acc:0.993070, test_loss:0.011944, test_acc:0.930990\n",
      "->epoch:376, train_loss:0.000580, train_acc:0.993880, test_loss:0.013971, test_acc:0.932422\n",
      "->epoch:377, train_loss:0.000613, train_acc:0.993302, test_loss:0.014001, test_acc:0.928385\n",
      "->epoch:378, train_loss:0.000632, train_acc:0.993099, test_loss:0.011744, test_acc:0.930339\n",
      "->epoch:379, train_loss:0.000684, train_acc:0.993490, test_loss:0.011121, test_acc:0.930729\n",
      "->epoch:380, train_loss:0.000629, train_acc:0.993461, test_loss:0.014280, test_acc:0.927604\n",
      "->epoch:381, train_loss:0.000654, train_acc:0.993200, test_loss:0.013688, test_acc:0.930208\n",
      "->epoch:382, train_loss:0.000659, train_acc:0.993562, test_loss:0.013756, test_acc:0.932162\n",
      "->epoch:383, train_loss:0.000654, train_acc:0.993229, test_loss:0.011768, test_acc:0.933333\n",
      "->epoch:384, train_loss:0.000636, train_acc:0.993490, test_loss:0.014696, test_acc:0.930339\n",
      "->epoch:385, train_loss:0.000681, train_acc:0.993417, test_loss:0.011874, test_acc:0.932682\n",
      "->epoch:386, train_loss:0.000726, train_acc:0.992896, test_loss:0.012491, test_acc:0.930208\n",
      "->epoch:387, train_loss:0.000533, train_acc:0.994170, test_loss:0.013883, test_acc:0.926172\n",
      "->epoch:388, train_loss:0.000628, train_acc:0.993518, test_loss:0.011446, test_acc:0.928255\n",
      "->epoch:389, train_loss:0.000619, train_acc:0.993808, test_loss:0.012092, test_acc:0.929688\n",
      "->epoch:390, train_loss:0.000580, train_acc:0.993967, test_loss:0.010609, test_acc:0.932162\n",
      "->epoch:391, train_loss:0.000597, train_acc:0.993504, test_loss:0.013324, test_acc:0.928646\n",
      "->epoch:392, train_loss:0.000589, train_acc:0.993822, test_loss:0.014150, test_acc:0.934505\n",
      "->epoch:393, train_loss:0.000638, train_acc:0.993287, test_loss:0.012014, test_acc:0.928646\n",
      "->epoch:394, train_loss:0.000656, train_acc:0.992867, test_loss:0.013230, test_acc:0.931510\n",
      "->epoch:395, train_loss:0.000660, train_acc:0.993316, test_loss:0.011345, test_acc:0.923828\n",
      "->epoch:396, train_loss:0.000574, train_acc:0.993938, test_loss:0.015449, test_acc:0.923568\n",
      "->epoch:397, train_loss:0.000686, train_acc:0.993027, test_loss:0.011571, test_acc:0.933724\n",
      "->epoch:398, train_loss:0.000627, train_acc:0.993461, test_loss:0.010788, test_acc:0.928776\n",
      "->epoch:399, train_loss:0.000629, train_acc:0.993432, test_loss:0.012217, test_acc:0.931771\n",
      "->epoch:400, train_loss:0.000665, train_acc:0.993490, test_loss:0.012946, test_acc:0.932943\n",
      "->epoch:401, train_loss:0.000625, train_acc:0.993953, test_loss:0.011970, test_acc:0.928516\n",
      "->epoch:402, train_loss:0.000658, train_acc:0.993504, test_loss:0.010692, test_acc:0.930729\n",
      "->epoch:403, train_loss:0.000638, train_acc:0.993461, test_loss:0.012402, test_acc:0.929427\n",
      "->epoch:404, train_loss:0.000615, train_acc:0.993707, test_loss:0.013986, test_acc:0.928776\n",
      "->epoch:405, train_loss:0.000581, train_acc:0.993793, test_loss:0.012460, test_acc:0.929427\n",
      "->epoch:406, train_loss:0.000592, train_acc:0.993446, test_loss:0.011017, test_acc:0.931250\n",
      "->epoch:407, train_loss:0.000599, train_acc:0.993953, test_loss:0.013329, test_acc:0.927474\n",
      "->epoch:408, train_loss:0.000615, train_acc:0.993808, test_loss:0.014902, test_acc:0.932031\n",
      "->epoch:409, train_loss:0.000605, train_acc:0.993851, test_loss:0.013522, test_acc:0.928776\n",
      "->epoch:410, train_loss:0.000620, train_acc:0.993981, test_loss:0.015847, test_acc:0.926953\n",
      "->epoch:411, train_loss:0.000640, train_acc:0.993678, test_loss:0.012637, test_acc:0.932292\n",
      "->epoch:412, train_loss:0.000621, train_acc:0.993446, test_loss:0.013797, test_acc:0.926432\n",
      "->epoch:413, train_loss:0.000585, train_acc:0.993938, test_loss:0.013361, test_acc:0.935287\n",
      "->epoch:414, train_loss:0.000643, train_acc:0.993244, test_loss:0.013307, test_acc:0.932162\n",
      "->epoch:415, train_loss:0.000587, train_acc:0.994213, test_loss:0.015798, test_acc:0.927344\n",
      "->epoch:416, train_loss:0.000641, train_acc:0.993576, test_loss:0.011980, test_acc:0.931901\n",
      "->epoch:417, train_loss:0.000576, train_acc:0.994314, test_loss:0.015441, test_acc:0.930990\n",
      "->epoch:418, train_loss:0.000585, train_acc:0.993996, test_loss:0.014207, test_acc:0.929167\n",
      "->epoch:419, train_loss:0.000586, train_acc:0.993605, test_loss:0.013570, test_acc:0.929818\n",
      "->epoch:420, train_loss:0.000710, train_acc:0.993374, test_loss:0.011973, test_acc:0.933073\n",
      "->epoch:421, train_loss:0.000598, train_acc:0.994213, test_loss:0.013661, test_acc:0.927214\n",
      "->epoch:422, train_loss:0.000674, train_acc:0.993359, test_loss:0.011948, test_acc:0.931641\n",
      "->epoch:423, train_loss:0.000557, train_acc:0.994025, test_loss:0.013830, test_acc:0.930339\n",
      "->epoch:424, train_loss:0.000612, train_acc:0.993851, test_loss:0.012390, test_acc:0.929297\n",
      "->epoch:425, train_loss:0.000590, train_acc:0.994039, test_loss:0.010736, test_acc:0.929948\n",
      "->epoch:426, train_loss:0.000579, train_acc:0.994213, test_loss:0.012194, test_acc:0.929688\n",
      "->epoch:427, train_loss:0.000579, train_acc:0.993996, test_loss:0.012807, test_acc:0.928646\n",
      "->epoch:428, train_loss:0.000581, train_acc:0.993562, test_loss:0.012933, test_acc:0.932031\n",
      "->epoch:429, train_loss:0.000689, train_acc:0.993186, test_loss:0.013234, test_acc:0.931250\n",
      "->epoch:430, train_loss:0.000612, train_acc:0.993678, test_loss:0.012170, test_acc:0.927995\n",
      "->epoch:431, train_loss:0.000607, train_acc:0.993432, test_loss:0.013405, test_acc:0.931380\n",
      "->epoch:432, train_loss:0.000634, train_acc:0.993649, test_loss:0.012891, test_acc:0.929688\n",
      "->epoch:433, train_loss:0.000587, train_acc:0.993866, test_loss:0.011514, test_acc:0.931901\n",
      "->epoch:434, train_loss:0.000628, train_acc:0.993432, test_loss:0.012687, test_acc:0.927344\n",
      "->epoch:435, train_loss:0.000614, train_acc:0.993605, test_loss:0.011240, test_acc:0.930078\n",
      "->epoch:436, train_loss:0.000543, train_acc:0.994589, test_loss:0.013707, test_acc:0.928516\n",
      "->epoch:437, train_loss:0.000622, train_acc:0.993432, test_loss:0.013202, test_acc:0.933464\n",
      "->epoch:438, train_loss:0.000603, train_acc:0.994010, test_loss:0.013803, test_acc:0.928906\n",
      "->epoch:439, train_loss:0.000578, train_acc:0.993924, test_loss:0.011320, test_acc:0.933984\n",
      "->epoch:440, train_loss:0.000613, train_acc:0.993866, test_loss:0.013059, test_acc:0.930599\n",
      "->epoch:441, train_loss:0.000587, train_acc:0.994300, test_loss:0.011447, test_acc:0.932031\n",
      "->epoch:442, train_loss:0.000612, train_acc:0.994358, test_loss:0.011629, test_acc:0.932031\n",
      "->epoch:443, train_loss:0.000609, train_acc:0.993866, test_loss:0.012525, test_acc:0.933724\n",
      "->epoch:444, train_loss:0.000604, train_acc:0.993981, test_loss:0.011971, test_acc:0.936458\n",
      "->epoch:445, train_loss:0.000575, train_acc:0.994184, test_loss:0.013193, test_acc:0.929297\n",
      "->epoch:446, train_loss:0.000615, train_acc:0.993895, test_loss:0.011448, test_acc:0.934896\n",
      "->epoch:447, train_loss:0.000598, train_acc:0.993620, test_loss:0.012171, test_acc:0.930729\n",
      "->epoch:448, train_loss:0.000613, train_acc:0.993345, test_loss:0.014149, test_acc:0.927995\n",
      "->epoch:449, train_loss:0.000650, train_acc:0.993620, test_loss:0.012671, test_acc:0.930078\n",
      "->epoch:450, train_loss:0.000560, train_acc:0.993981, test_loss:0.014237, test_acc:0.933984\n",
      "->epoch:451, train_loss:0.000603, train_acc:0.993924, test_loss:0.011685, test_acc:0.927995\n",
      "->epoch:452, train_loss:0.000623, train_acc:0.993533, test_loss:0.013181, test_acc:0.933464\n",
      "->epoch:453, train_loss:0.000568, train_acc:0.993735, test_loss:0.012678, test_acc:0.931380\n",
      "->epoch:454, train_loss:0.000595, train_acc:0.994343, test_loss:0.013278, test_acc:0.930078\n",
      "->epoch:455, train_loss:0.000557, train_acc:0.994806, test_loss:0.014394, test_acc:0.927734\n",
      "->epoch:456, train_loss:0.000650, train_acc:0.993735, test_loss:0.012202, test_acc:0.929036\n",
      "->epoch:457, train_loss:0.000595, train_acc:0.993880, test_loss:0.012710, test_acc:0.933724\n",
      "->epoch:458, train_loss:0.000579, train_acc:0.994285, test_loss:0.012977, test_acc:0.929688\n",
      "->epoch:459, train_loss:0.000604, train_acc:0.994039, test_loss:0.013019, test_acc:0.929948\n",
      "->epoch:460, train_loss:0.000606, train_acc:0.993707, test_loss:0.013983, test_acc:0.935677\n",
      "->epoch:461, train_loss:0.000589, train_acc:0.994039, test_loss:0.014960, test_acc:0.923568\n",
      "->epoch:462, train_loss:0.000561, train_acc:0.994126, test_loss:0.012517, test_acc:0.933203\n",
      "->epoch:463, train_loss:0.000541, train_acc:0.994300, test_loss:0.015855, test_acc:0.929427\n",
      "->epoch:464, train_loss:0.000652, train_acc:0.993186, test_loss:0.011516, test_acc:0.930599\n",
      "->epoch:465, train_loss:0.000534, train_acc:0.994430, test_loss:0.012328, test_acc:0.934505\n",
      "->epoch:466, train_loss:0.000594, train_acc:0.993909, test_loss:0.014038, test_acc:0.929036\n",
      "->epoch:467, train_loss:0.000564, train_acc:0.994387, test_loss:0.012295, test_acc:0.931120\n",
      "->epoch:468, train_loss:0.000577, train_acc:0.993981, test_loss:0.011342, test_acc:0.928385\n",
      "->epoch:469, train_loss:0.000580, train_acc:0.994068, test_loss:0.014523, test_acc:0.930990\n",
      "->epoch:470, train_loss:0.000711, train_acc:0.993880, test_loss:0.012548, test_acc:0.931901\n",
      "->epoch:471, train_loss:0.000588, train_acc:0.993779, test_loss:0.011764, test_acc:0.927474\n",
      "->epoch:472, train_loss:0.000587, train_acc:0.993576, test_loss:0.013252, test_acc:0.928776\n",
      "->epoch:473, train_loss:0.000560, train_acc:0.994126, test_loss:0.012078, test_acc:0.932292\n",
      "->epoch:474, train_loss:0.000608, train_acc:0.993895, test_loss:0.013401, test_acc:0.929818\n",
      "->epoch:475, train_loss:0.000590, train_acc:0.993866, test_loss:0.013502, test_acc:0.930208\n",
      "->epoch:476, train_loss:0.000554, train_acc:0.994285, test_loss:0.014209, test_acc:0.930990\n",
      "->epoch:477, train_loss:0.000535, train_acc:0.994227, test_loss:0.013923, test_acc:0.932292\n",
      "->epoch:478, train_loss:0.000537, train_acc:0.994054, test_loss:0.013437, test_acc:0.935417\n",
      "->epoch:479, train_loss:0.000631, train_acc:0.993735, test_loss:0.013641, test_acc:0.931641\n",
      "->epoch:480, train_loss:0.000562, train_acc:0.993953, test_loss:0.012103, test_acc:0.930078\n",
      "->epoch:481, train_loss:0.000566, train_acc:0.994039, test_loss:0.011879, test_acc:0.931901\n",
      "->epoch:482, train_loss:0.000595, train_acc:0.993851, test_loss:0.011819, test_acc:0.931901\n",
      "->epoch:483, train_loss:0.000565, train_acc:0.994126, test_loss:0.012653, test_acc:0.925130\n",
      "->epoch:484, train_loss:0.000727, train_acc:0.993779, test_loss:0.013169, test_acc:0.932682\n",
      "->epoch:485, train_loss:0.000560, train_acc:0.994213, test_loss:0.010530, test_acc:0.933333\n",
      "->epoch:486, train_loss:0.000645, train_acc:0.994126, test_loss:0.010457, test_acc:0.932552\n",
      "->epoch:487, train_loss:0.000535, train_acc:0.994560, test_loss:0.013931, test_acc:0.931120\n",
      "->epoch:488, train_loss:0.000535, train_acc:0.994647, test_loss:0.011053, test_acc:0.928255\n",
      "->epoch:489, train_loss:0.000584, train_acc:0.993880, test_loss:0.013102, test_acc:0.933333\n",
      "->epoch:490, train_loss:0.000550, train_acc:0.994546, test_loss:0.013896, test_acc:0.931250\n",
      "->epoch:491, train_loss:0.000627, train_acc:0.993981, test_loss:0.013887, test_acc:0.930078\n",
      "->epoch:492, train_loss:0.000564, train_acc:0.993996, test_loss:0.011591, test_acc:0.931380\n",
      "->epoch:493, train_loss:0.000565, train_acc:0.994300, test_loss:0.013843, test_acc:0.931510\n",
      "->epoch:494, train_loss:0.000663, train_acc:0.993851, test_loss:0.013122, test_acc:0.929036\n",
      "->epoch:495, train_loss:0.000574, train_acc:0.994170, test_loss:0.011901, test_acc:0.927865\n",
      "->epoch:496, train_loss:0.000572, train_acc:0.994112, test_loss:0.012208, test_acc:0.933073\n",
      "->epoch:497, train_loss:0.000570, train_acc:0.994083, test_loss:0.015102, test_acc:0.931250\n",
      "->epoch:498, train_loss:0.000583, train_acc:0.994068, test_loss:0.009498, test_acc:0.936979\n",
      "->epoch:499, train_loss:0.000610, train_acc:0.993924, test_loss:0.012734, test_acc:0.930469\n",
      "->epoch:500, train_loss:0.000584, train_acc:0.993822, test_loss:0.013618, test_acc:0.927604\n",
      "->epoch:501, train_loss:0.000556, train_acc:0.993996, test_loss:0.013147, test_acc:0.929297\n",
      "->epoch:502, train_loss:0.000595, train_acc:0.993909, test_loss:0.012517, test_acc:0.930339\n",
      "->epoch:503, train_loss:0.000577, train_acc:0.994054, test_loss:0.012457, test_acc:0.936328\n",
      "->epoch:504, train_loss:0.000564, train_acc:0.994039, test_loss:0.014980, test_acc:0.934505\n",
      "->epoch:505, train_loss:0.000520, train_acc:0.994575, test_loss:0.012908, test_acc:0.929557\n",
      "->epoch:506, train_loss:0.000627, train_acc:0.993417, test_loss:0.011094, test_acc:0.929557\n",
      "->epoch:507, train_loss:0.000569, train_acc:0.994227, test_loss:0.014705, test_acc:0.930469\n",
      "->epoch:508, train_loss:0.000594, train_acc:0.994242, test_loss:0.010361, test_acc:0.926953\n",
      "->epoch:509, train_loss:0.000565, train_acc:0.993938, test_loss:0.012466, test_acc:0.934245\n",
      "->epoch:510, train_loss:0.000550, train_acc:0.994213, test_loss:0.013003, test_acc:0.932813\n",
      "->epoch:511, train_loss:0.000576, train_acc:0.994054, test_loss:0.012840, test_acc:0.932422\n",
      "->epoch:512, train_loss:0.000555, train_acc:0.994502, test_loss:0.013176, test_acc:0.932031\n",
      "->epoch:513, train_loss:0.000583, train_acc:0.993793, test_loss:0.011498, test_acc:0.932813\n",
      "->epoch:514, train_loss:0.000552, train_acc:0.994358, test_loss:0.013236, test_acc:0.930208\n",
      "->epoch:515, train_loss:0.000584, train_acc:0.994155, test_loss:0.014936, test_acc:0.932813\n",
      "->epoch:516, train_loss:0.000559, train_acc:0.993996, test_loss:0.011507, test_acc:0.932813\n",
      "->epoch:517, train_loss:0.000549, train_acc:0.994401, test_loss:0.013938, test_acc:0.930208\n",
      "->epoch:518, train_loss:0.000602, train_acc:0.994213, test_loss:0.013394, test_acc:0.929688\n",
      "->epoch:519, train_loss:0.000570, train_acc:0.994199, test_loss:0.011511, test_acc:0.931250\n",
      "->epoch:520, train_loss:0.000472, train_acc:0.995197, test_loss:0.013445, test_acc:0.933333\n",
      "->epoch:521, train_loss:0.000609, train_acc:0.993808, test_loss:0.013082, test_acc:0.930599\n",
      "->epoch:522, train_loss:0.000540, train_acc:0.994821, test_loss:0.014985, test_acc:0.933464\n",
      "->epoch:523, train_loss:0.000613, train_acc:0.994676, test_loss:0.013735, test_acc:0.932682\n",
      "->epoch:524, train_loss:0.000562, train_acc:0.994242, test_loss:0.012833, test_acc:0.927474\n",
      "->epoch:525, train_loss:0.000578, train_acc:0.994459, test_loss:0.014287, test_acc:0.930078\n",
      "->epoch:526, train_loss:0.000544, train_acc:0.994199, test_loss:0.013548, test_acc:0.932682\n",
      "->epoch:527, train_loss:0.000536, train_acc:0.994459, test_loss:0.011657, test_acc:0.934505\n",
      "->epoch:528, train_loss:0.000556, train_acc:0.994285, test_loss:0.013039, test_acc:0.935026\n",
      "->epoch:529, train_loss:0.000540, train_acc:0.994271, test_loss:0.012145, test_acc:0.932813\n",
      "->epoch:530, train_loss:0.000548, train_acc:0.994285, test_loss:0.012263, test_acc:0.929948\n",
      "->epoch:531, train_loss:0.000530, train_acc:0.994893, test_loss:0.013369, test_acc:0.931120\n",
      "->epoch:532, train_loss:0.000580, train_acc:0.994502, test_loss:0.015828, test_acc:0.931510\n",
      "->epoch:533, train_loss:0.000552, train_acc:0.994329, test_loss:0.012492, test_acc:0.932813\n",
      "->epoch:534, train_loss:0.000565, train_acc:0.994155, test_loss:0.012608, test_acc:0.933984\n",
      "->epoch:535, train_loss:0.000575, train_acc:0.994705, test_loss:0.014381, test_acc:0.930208\n",
      "->epoch:536, train_loss:0.000559, train_acc:0.994604, test_loss:0.012154, test_acc:0.932813\n",
      "->epoch:537, train_loss:0.000571, train_acc:0.994227, test_loss:0.011929, test_acc:0.938281\n",
      "->epoch:538, train_loss:0.000554, train_acc:0.994473, test_loss:0.012306, test_acc:0.930859\n",
      "->epoch:539, train_loss:0.000563, train_acc:0.994589, test_loss:0.014900, test_acc:0.930208\n",
      "->epoch:540, train_loss:0.000687, train_acc:0.994300, test_loss:0.013894, test_acc:0.928125\n",
      "->epoch:541, train_loss:0.000465, train_acc:0.995139, test_loss:0.014875, test_acc:0.932813\n",
      "->epoch:542, train_loss:0.000588, train_acc:0.994010, test_loss:0.013047, test_acc:0.930990\n",
      "->epoch:543, train_loss:0.000584, train_acc:0.994401, test_loss:0.010541, test_acc:0.935677\n",
      "->epoch:544, train_loss:0.000606, train_acc:0.993822, test_loss:0.014386, test_acc:0.929948\n",
      "->epoch:545, train_loss:0.000516, train_acc:0.994907, test_loss:0.014671, test_acc:0.932162\n",
      "->epoch:546, train_loss:0.000529, train_acc:0.995182, test_loss:0.012233, test_acc:0.933594\n",
      "->epoch:547, train_loss:0.000527, train_acc:0.994575, test_loss:0.014569, test_acc:0.930078\n",
      "->epoch:548, train_loss:0.000576, train_acc:0.994256, test_loss:0.011569, test_acc:0.933203\n",
      "->epoch:549, train_loss:0.000574, train_acc:0.994343, test_loss:0.015028, test_acc:0.931120\n",
      "->epoch:550, train_loss:0.000574, train_acc:0.994314, test_loss:0.013039, test_acc:0.930339\n",
      "->epoch:551, train_loss:0.000556, train_acc:0.994271, test_loss:0.014478, test_acc:0.931901\n",
      "->epoch:552, train_loss:0.000570, train_acc:0.994560, test_loss:0.009260, test_acc:0.927865\n",
      "->epoch:553, train_loss:0.000528, train_acc:0.994358, test_loss:0.013447, test_acc:0.933594\n",
      "->epoch:554, train_loss:0.000541, train_acc:0.994242, test_loss:0.012604, test_acc:0.930339\n",
      "->epoch:555, train_loss:0.000557, train_acc:0.993924, test_loss:0.013027, test_acc:0.928516\n",
      "->epoch:556, train_loss:0.000525, train_acc:0.994980, test_loss:0.013046, test_acc:0.934635\n",
      "->epoch:557, train_loss:0.000558, train_acc:0.994430, test_loss:0.012122, test_acc:0.932292\n",
      "->epoch:558, train_loss:0.000553, train_acc:0.994329, test_loss:0.012670, test_acc:0.924870\n",
      "->epoch:559, train_loss:0.000553, train_acc:0.994271, test_loss:0.012429, test_acc:0.934505\n",
      "->epoch:560, train_loss:0.000572, train_acc:0.994459, test_loss:0.011352, test_acc:0.931641\n",
      "->epoch:561, train_loss:0.000555, train_acc:0.994401, test_loss:0.014375, test_acc:0.933984\n",
      "->epoch:562, train_loss:0.000531, train_acc:0.994560, test_loss:0.012095, test_acc:0.930859\n",
      "->epoch:563, train_loss:0.000552, train_acc:0.994647, test_loss:0.013649, test_acc:0.933333\n",
      "->epoch:564, train_loss:0.000535, train_acc:0.994271, test_loss:0.013562, test_acc:0.931380\n",
      "->epoch:565, train_loss:0.000565, train_acc:0.994792, test_loss:0.013540, test_acc:0.931771\n",
      "->epoch:566, train_loss:0.000477, train_acc:0.995052, test_loss:0.016155, test_acc:0.931250\n",
      "->epoch:567, train_loss:0.000635, train_acc:0.994054, test_loss:0.012911, test_acc:0.927214\n",
      "->epoch:568, train_loss:0.000522, train_acc:0.994690, test_loss:0.015695, test_acc:0.930339\n",
      "->epoch:569, train_loss:0.000520, train_acc:0.994690, test_loss:0.011770, test_acc:0.933073\n",
      "->epoch:570, train_loss:0.000514, train_acc:0.994661, test_loss:0.012601, test_acc:0.931510\n",
      "->epoch:571, train_loss:0.000591, train_acc:0.993808, test_loss:0.011239, test_acc:0.931771\n",
      "->epoch:572, train_loss:0.000591, train_acc:0.994473, test_loss:0.010365, test_acc:0.936849\n",
      "->epoch:573, train_loss:0.000551, train_acc:0.994719, test_loss:0.014573, test_acc:0.933203\n",
      "->epoch:574, train_loss:0.000555, train_acc:0.994502, test_loss:0.013518, test_acc:0.931641\n",
      "->epoch:575, train_loss:0.000537, train_acc:0.994864, test_loss:0.012662, test_acc:0.936328\n",
      "->epoch:576, train_loss:0.000504, train_acc:0.994676, test_loss:0.013526, test_acc:0.930729\n",
      "->epoch:577, train_loss:0.000536, train_acc:0.994575, test_loss:0.014046, test_acc:0.933594\n",
      "->epoch:578, train_loss:0.000547, train_acc:0.994835, test_loss:0.014327, test_acc:0.931901\n",
      "->epoch:579, train_loss:0.000579, train_acc:0.994560, test_loss:0.011436, test_acc:0.928776\n",
      "->epoch:580, train_loss:0.000488, train_acc:0.995095, test_loss:0.015681, test_acc:0.932422\n",
      "->epoch:581, train_loss:0.000566, train_acc:0.994358, test_loss:0.010333, test_acc:0.931120\n",
      "->epoch:582, train_loss:0.000548, train_acc:0.994734, test_loss:0.011856, test_acc:0.932162\n",
      "->epoch:583, train_loss:0.000499, train_acc:0.994922, test_loss:0.013787, test_acc:0.927474\n",
      "->epoch:584, train_loss:0.000505, train_acc:0.995182, test_loss:0.015319, test_acc:0.931901\n",
      "->epoch:585, train_loss:0.000549, train_acc:0.994473, test_loss:0.012253, test_acc:0.930729\n",
      "->epoch:586, train_loss:0.000511, train_acc:0.994806, test_loss:0.011497, test_acc:0.929818\n",
      "->epoch:587, train_loss:0.000539, train_acc:0.994835, test_loss:0.013553, test_acc:0.929036\n",
      "->epoch:588, train_loss:0.000504, train_acc:0.994661, test_loss:0.013646, test_acc:0.933203\n",
      "->epoch:589, train_loss:0.000525, train_acc:0.994430, test_loss:0.012409, test_acc:0.931771\n",
      "->epoch:590, train_loss:0.000560, train_acc:0.994575, test_loss:0.014422, test_acc:0.929297\n",
      "->epoch:591, train_loss:0.000533, train_acc:0.994690, test_loss:0.015625, test_acc:0.933073\n",
      "->epoch:592, train_loss:0.000555, train_acc:0.994922, test_loss:0.012464, test_acc:0.931250\n",
      "->epoch:593, train_loss:0.000565, train_acc:0.994763, test_loss:0.011424, test_acc:0.931510\n",
      "->epoch:594, train_loss:0.000492, train_acc:0.994748, test_loss:0.013358, test_acc:0.933984\n",
      "->epoch:595, train_loss:0.000546, train_acc:0.994893, test_loss:0.017189, test_acc:0.928906\n",
      "->epoch:596, train_loss:0.000604, train_acc:0.993837, test_loss:0.013567, test_acc:0.931120\n",
      "->epoch:597, train_loss:0.000575, train_acc:0.994618, test_loss:0.012385, test_acc:0.935807\n",
      "->epoch:598, train_loss:0.000533, train_acc:0.994777, test_loss:0.014773, test_acc:0.932813\n",
      "->epoch:599, train_loss:0.000522, train_acc:0.994748, test_loss:0.012543, test_acc:0.935026\n",
      "->epoch:600, train_loss:0.000516, train_acc:0.995298, test_loss:0.013639, test_acc:0.931641\n",
      "->epoch:601, train_loss:0.000528, train_acc:0.994676, test_loss:0.012720, test_acc:0.935156\n",
      "->epoch:602, train_loss:0.000525, train_acc:0.994618, test_loss:0.015919, test_acc:0.934896\n",
      "->epoch:603, train_loss:0.000568, train_acc:0.994502, test_loss:0.012054, test_acc:0.935287\n",
      "->epoch:604, train_loss:0.000538, train_acc:0.994777, test_loss:0.015391, test_acc:0.932552\n",
      "->epoch:605, train_loss:0.000561, train_acc:0.994314, test_loss:0.013236, test_acc:0.931380\n",
      "->epoch:606, train_loss:0.000631, train_acc:0.994343, test_loss:0.012176, test_acc:0.935417\n",
      "->epoch:607, train_loss:0.000615, train_acc:0.994155, test_loss:0.011307, test_acc:0.930859\n",
      "->epoch:608, train_loss:0.000533, train_acc:0.994546, test_loss:0.012622, test_acc:0.931510\n",
      "->epoch:609, train_loss:0.000545, train_acc:0.994430, test_loss:0.014271, test_acc:0.932162\n",
      "->epoch:610, train_loss:0.000545, train_acc:0.994604, test_loss:0.013420, test_acc:0.928646\n",
      "->epoch:611, train_loss:0.000533, train_acc:0.994777, test_loss:0.013750, test_acc:0.932813\n",
      "->epoch:612, train_loss:0.000552, train_acc:0.994387, test_loss:0.012034, test_acc:0.933203\n",
      "->epoch:613, train_loss:0.000558, train_acc:0.994329, test_loss:0.011253, test_acc:0.932422\n",
      "->epoch:614, train_loss:0.000519, train_acc:0.994560, test_loss:0.013986, test_acc:0.934245\n",
      "->epoch:615, train_loss:0.000579, train_acc:0.994473, test_loss:0.011719, test_acc:0.930729\n",
      "->epoch:616, train_loss:0.000521, train_acc:0.994821, test_loss:0.013995, test_acc:0.931120\n",
      "->epoch:617, train_loss:0.000549, train_acc:0.994531, test_loss:0.011737, test_acc:0.932031\n",
      "->epoch:618, train_loss:0.000568, train_acc:0.994835, test_loss:0.016186, test_acc:0.930599\n",
      "->epoch:619, train_loss:0.000559, train_acc:0.994690, test_loss:0.012971, test_acc:0.928776\n",
      "->epoch:620, train_loss:0.000491, train_acc:0.994821, test_loss:0.014049, test_acc:0.934896\n",
      "->epoch:621, train_loss:0.000517, train_acc:0.994444, test_loss:0.013083, test_acc:0.937370\n",
      "->epoch:622, train_loss:0.000534, train_acc:0.994618, test_loss:0.013635, test_acc:0.930469\n",
      "->epoch:623, train_loss:0.000492, train_acc:0.995038, test_loss:0.013133, test_acc:0.934245\n",
      "->epoch:624, train_loss:0.000519, train_acc:0.994690, test_loss:0.011916, test_acc:0.932682\n",
      "->epoch:625, train_loss:0.000585, train_acc:0.994068, test_loss:0.014075, test_acc:0.932031\n",
      "->epoch:626, train_loss:0.000513, train_acc:0.994690, test_loss:0.014317, test_acc:0.933984\n",
      "->epoch:627, train_loss:0.000562, train_acc:0.994893, test_loss:0.016561, test_acc:0.929427\n",
      "->epoch:628, train_loss:0.000511, train_acc:0.994676, test_loss:0.012234, test_acc:0.933854\n",
      "->epoch:629, train_loss:0.000751, train_acc:0.994415, test_loss:0.013913, test_acc:0.938412\n",
      "->epoch:630, train_loss:0.000487, train_acc:0.994922, test_loss:0.015337, test_acc:0.930339\n",
      "->epoch:631, train_loss:0.000492, train_acc:0.994835, test_loss:0.012377, test_acc:0.934635\n",
      "->epoch:632, train_loss:0.000473, train_acc:0.994965, test_loss:0.016249, test_acc:0.929818\n",
      "->epoch:633, train_loss:0.000544, train_acc:0.994387, test_loss:0.014398, test_acc:0.931510\n",
      "->epoch:634, train_loss:0.000561, train_acc:0.994546, test_loss:0.011783, test_acc:0.933984\n",
      "->epoch:635, train_loss:0.000567, train_acc:0.994531, test_loss:0.015183, test_acc:0.933724\n",
      "->epoch:636, train_loss:0.000507, train_acc:0.995067, test_loss:0.013142, test_acc:0.927865\n",
      "->epoch:637, train_loss:0.000464, train_acc:0.995052, test_loss:0.015564, test_acc:0.932682\n",
      "->epoch:638, train_loss:0.000495, train_acc:0.994907, test_loss:0.014997, test_acc:0.933594\n",
      "->epoch:639, train_loss:0.000526, train_acc:0.994358, test_loss:0.013769, test_acc:0.931510\n",
      "->epoch:640, train_loss:0.000550, train_acc:0.994893, test_loss:0.015537, test_acc:0.930859\n",
      "->epoch:641, train_loss:0.000487, train_acc:0.995182, test_loss:0.013357, test_acc:0.931250\n",
      "->epoch:642, train_loss:0.000523, train_acc:0.994575, test_loss:0.011864, test_acc:0.935547\n",
      "->epoch:643, train_loss:0.000528, train_acc:0.994734, test_loss:0.014557, test_acc:0.932813\n",
      "->epoch:644, train_loss:0.000554, train_acc:0.994719, test_loss:0.013953, test_acc:0.929948\n",
      "->epoch:645, train_loss:0.000533, train_acc:0.994965, test_loss:0.014996, test_acc:0.934505\n",
      "->epoch:646, train_loss:0.000509, train_acc:0.994604, test_loss:0.017846, test_acc:0.928906\n",
      "->epoch:647, train_loss:0.000579, train_acc:0.994517, test_loss:0.012518, test_acc:0.932292\n",
      "->epoch:648, train_loss:0.000496, train_acc:0.995023, test_loss:0.011470, test_acc:0.934505\n",
      "->epoch:649, train_loss:0.000554, train_acc:0.994618, test_loss:0.016103, test_acc:0.932943\n",
      "->epoch:650, train_loss:0.000547, train_acc:0.994560, test_loss:0.013248, test_acc:0.931380\n",
      "->epoch:651, train_loss:0.000566, train_acc:0.994531, test_loss:0.014929, test_acc:0.932943\n",
      "->epoch:652, train_loss:0.000520, train_acc:0.994893, test_loss:0.012280, test_acc:0.935807\n",
      "->epoch:653, train_loss:0.000535, train_acc:0.994488, test_loss:0.011866, test_acc:0.930208\n",
      "->epoch:654, train_loss:0.000612, train_acc:0.994951, test_loss:0.012339, test_acc:0.935938\n",
      "->epoch:655, train_loss:0.000524, train_acc:0.994705, test_loss:0.013692, test_acc:0.933724\n",
      "->epoch:656, train_loss:0.000547, train_acc:0.994719, test_loss:0.014348, test_acc:0.932162\n",
      "->epoch:657, train_loss:0.000521, train_acc:0.994575, test_loss:0.012921, test_acc:0.932422\n",
      "->epoch:658, train_loss:0.000493, train_acc:0.994777, test_loss:0.013849, test_acc:0.934896\n",
      "->epoch:659, train_loss:0.000523, train_acc:0.994994, test_loss:0.012324, test_acc:0.932552\n",
      "->epoch:660, train_loss:0.000534, train_acc:0.994792, test_loss:0.015830, test_acc:0.930729\n",
      "->epoch:661, train_loss:0.000593, train_acc:0.994401, test_loss:0.017973, test_acc:0.924479\n",
      "->epoch:662, train_loss:0.000605, train_acc:0.994734, test_loss:0.011813, test_acc:0.936458\n",
      "->epoch:663, train_loss:0.000513, train_acc:0.994864, test_loss:0.014458, test_acc:0.934766\n",
      "->epoch:664, train_loss:0.000478, train_acc:0.995067, test_loss:0.016291, test_acc:0.931250\n",
      "->epoch:665, train_loss:0.000506, train_acc:0.995269, test_loss:0.015149, test_acc:0.933594\n",
      "->epoch:666, train_loss:0.000521, train_acc:0.994647, test_loss:0.017484, test_acc:0.931641\n",
      "->epoch:667, train_loss:0.000510, train_acc:0.994806, test_loss:0.012732, test_acc:0.927734\n",
      "->epoch:668, train_loss:0.000511, train_acc:0.994632, test_loss:0.014305, test_acc:0.933724\n",
      "->epoch:669, train_loss:0.000480, train_acc:0.995269, test_loss:0.013852, test_acc:0.934505\n",
      "->epoch:670, train_loss:0.000506, train_acc:0.994994, test_loss:0.013942, test_acc:0.933203\n",
      "->epoch:671, train_loss:0.000558, train_acc:0.995095, test_loss:0.015589, test_acc:0.934896\n",
      "->epoch:672, train_loss:0.000504, train_acc:0.994907, test_loss:0.013774, test_acc:0.930599\n",
      "->epoch:673, train_loss:0.000518, train_acc:0.994604, test_loss:0.014076, test_acc:0.934766\n",
      "->epoch:674, train_loss:0.000560, train_acc:0.994618, test_loss:0.014240, test_acc:0.929948\n",
      "->epoch:675, train_loss:0.000543, train_acc:0.994907, test_loss:0.015818, test_acc:0.935156\n",
      "->epoch:676, train_loss:0.000509, train_acc:0.995110, test_loss:0.015110, test_acc:0.931901\n",
      "->epoch:677, train_loss:0.000562, train_acc:0.994734, test_loss:0.015117, test_acc:0.929297\n",
      "->epoch:678, train_loss:0.000560, train_acc:0.994618, test_loss:0.013296, test_acc:0.935547\n",
      "->epoch:679, train_loss:0.000499, train_acc:0.994965, test_loss:0.016241, test_acc:0.935026\n",
      "->epoch:680, train_loss:0.000525, train_acc:0.994792, test_loss:0.013326, test_acc:0.930990\n",
      "->epoch:681, train_loss:0.000514, train_acc:0.995124, test_loss:0.013663, test_acc:0.935547\n",
      "->epoch:682, train_loss:0.000515, train_acc:0.995067, test_loss:0.018065, test_acc:0.933333\n",
      "->epoch:683, train_loss:0.000501, train_acc:0.995095, test_loss:0.014667, test_acc:0.934375\n",
      "->epoch:684, train_loss:0.000597, train_acc:0.994473, test_loss:0.014921, test_acc:0.931510\n",
      "->epoch:685, train_loss:0.000478, train_acc:0.995255, test_loss:0.011686, test_acc:0.935287\n",
      "->epoch:686, train_loss:0.000481, train_acc:0.995168, test_loss:0.013605, test_acc:0.935807\n",
      "->epoch:687, train_loss:0.000516, train_acc:0.994951, test_loss:0.015997, test_acc:0.927344\n",
      "->epoch:688, train_loss:0.000492, train_acc:0.994661, test_loss:0.012919, test_acc:0.936849\n",
      "->epoch:689, train_loss:0.000550, train_acc:0.994560, test_loss:0.015676, test_acc:0.933724\n",
      "->epoch:690, train_loss:0.000517, train_acc:0.994517, test_loss:0.013258, test_acc:0.932031\n",
      "->epoch:691, train_loss:0.000528, train_acc:0.994922, test_loss:0.012009, test_acc:0.932422\n",
      "->epoch:692, train_loss:0.000530, train_acc:0.994763, test_loss:0.013333, test_acc:0.937891\n",
      "->epoch:693, train_loss:0.000534, train_acc:0.995038, test_loss:0.013657, test_acc:0.934375\n",
      "->epoch:694, train_loss:0.000527, train_acc:0.994936, test_loss:0.012003, test_acc:0.935026\n",
      "->epoch:695, train_loss:0.000482, train_acc:0.995255, test_loss:0.016072, test_acc:0.929297\n",
      "->epoch:696, train_loss:0.000529, train_acc:0.994734, test_loss:0.013076, test_acc:0.932813\n",
      "->epoch:697, train_loss:0.000448, train_acc:0.995587, test_loss:0.014439, test_acc:0.934896\n",
      "->epoch:698, train_loss:0.000598, train_acc:0.994256, test_loss:0.015498, test_acc:0.935287\n",
      "->epoch:699, train_loss:0.000497, train_acc:0.995226, test_loss:0.012983, test_acc:0.933333\n",
      "->epoch:700, train_loss:0.000535, train_acc:0.995067, test_loss:0.012781, test_acc:0.933203\n",
      "->epoch:701, train_loss:0.000489, train_acc:0.994835, test_loss:0.016832, test_acc:0.934896\n",
      "->epoch:702, train_loss:0.000562, train_acc:0.994705, test_loss:0.013265, test_acc:0.932682\n",
      "->epoch:703, train_loss:0.000497, train_acc:0.994878, test_loss:0.016593, test_acc:0.934505\n",
      "->epoch:704, train_loss:0.000556, train_acc:0.994994, test_loss:0.014542, test_acc:0.934245\n",
      "->epoch:705, train_loss:0.000486, train_acc:0.994864, test_loss:0.012660, test_acc:0.933724\n",
      "->epoch:706, train_loss:0.000518, train_acc:0.994676, test_loss:0.012025, test_acc:0.933854\n",
      "->epoch:707, train_loss:0.000545, train_acc:0.994473, test_loss:0.012704, test_acc:0.935547\n",
      "->epoch:708, train_loss:0.000564, train_acc:0.995081, test_loss:0.013094, test_acc:0.924089\n",
      "->epoch:709, train_loss:0.000549, train_acc:0.994850, test_loss:0.011900, test_acc:0.933333\n",
      "->epoch:710, train_loss:0.000503, train_acc:0.995009, test_loss:0.014739, test_acc:0.928385\n",
      "->epoch:711, train_loss:0.000501, train_acc:0.995182, test_loss:0.014974, test_acc:0.936328\n",
      "->epoch:712, train_loss:0.000520, train_acc:0.995240, test_loss:0.012002, test_acc:0.937891\n",
      "->epoch:713, train_loss:0.000523, train_acc:0.995038, test_loss:0.013795, test_acc:0.931641\n",
      "->epoch:714, train_loss:0.000518, train_acc:0.994878, test_loss:0.012555, test_acc:0.936068\n",
      "->epoch:715, train_loss:0.000530, train_acc:0.995023, test_loss:0.015881, test_acc:0.931120\n",
      "->epoch:716, train_loss:0.000507, train_acc:0.994546, test_loss:0.015025, test_acc:0.932943\n",
      "->epoch:717, train_loss:0.000530, train_acc:0.994763, test_loss:0.015269, test_acc:0.935026\n",
      "->epoch:718, train_loss:0.000524, train_acc:0.994792, test_loss:0.012742, test_acc:0.936849\n",
      "->epoch:719, train_loss:0.000559, train_acc:0.994835, test_loss:0.012744, test_acc:0.934375\n",
      "->epoch:720, train_loss:0.000544, train_acc:0.994835, test_loss:0.015456, test_acc:0.932943\n",
      "->epoch:721, train_loss:0.000568, train_acc:0.994560, test_loss:0.013231, test_acc:0.932552\n",
      "->epoch:722, train_loss:0.000503, train_acc:0.994893, test_loss:0.011850, test_acc:0.932813\n",
      "->epoch:723, train_loss:0.000554, train_acc:0.994560, test_loss:0.015044, test_acc:0.932813\n",
      "->epoch:724, train_loss:0.000527, train_acc:0.994878, test_loss:0.013628, test_acc:0.933203\n",
      "->epoch:725, train_loss:0.000525, train_acc:0.994748, test_loss:0.012037, test_acc:0.934635\n",
      "->epoch:726, train_loss:0.000522, train_acc:0.994792, test_loss:0.012745, test_acc:0.935938\n",
      "->epoch:727, train_loss:0.000489, train_acc:0.995139, test_loss:0.012751, test_acc:0.932813\n",
      "->epoch:728, train_loss:0.000594, train_acc:0.994473, test_loss:0.012610, test_acc:0.933333\n",
      "->epoch:729, train_loss:0.000506, train_acc:0.995182, test_loss:0.019869, test_acc:0.931771\n",
      "->epoch:730, train_loss:0.000548, train_acc:0.994806, test_loss:0.012849, test_acc:0.933203\n",
      "->epoch:731, train_loss:0.000479, train_acc:0.995370, test_loss:0.015001, test_acc:0.929948\n",
      "->epoch:732, train_loss:0.000550, train_acc:0.995023, test_loss:0.014409, test_acc:0.932682\n",
      "->epoch:733, train_loss:0.000475, train_acc:0.995153, test_loss:0.013730, test_acc:0.930208\n",
      "->epoch:734, train_loss:0.000516, train_acc:0.994676, test_loss:0.016127, test_acc:0.932292\n",
      "->epoch:735, train_loss:0.000519, train_acc:0.994734, test_loss:0.013136, test_acc:0.933854\n",
      "->epoch:736, train_loss:0.000621, train_acc:0.994560, test_loss:0.014977, test_acc:0.928125\n",
      "->epoch:737, train_loss:0.000492, train_acc:0.995052, test_loss:0.014044, test_acc:0.931120\n",
      "->epoch:738, train_loss:0.000496, train_acc:0.995399, test_loss:0.016596, test_acc:0.931380\n",
      "->epoch:739, train_loss:0.000481, train_acc:0.995067, test_loss:0.013886, test_acc:0.937500\n",
      "->epoch:740, train_loss:0.000540, train_acc:0.994661, test_loss:0.017630, test_acc:0.931901\n",
      "->epoch:741, train_loss:0.000501, train_acc:0.994777, test_loss:0.017208, test_acc:0.933073\n",
      "->epoch:742, train_loss:0.000474, train_acc:0.995023, test_loss:0.017073, test_acc:0.928125\n",
      "->epoch:743, train_loss:0.000476, train_acc:0.995370, test_loss:0.018569, test_acc:0.935026\n",
      "->epoch:744, train_loss:0.000516, train_acc:0.995472, test_loss:0.013512, test_acc:0.930469\n",
      "->epoch:745, train_loss:0.000544, train_acc:0.994734, test_loss:0.012405, test_acc:0.935026\n",
      "->epoch:746, train_loss:0.000523, train_acc:0.994806, test_loss:0.013877, test_acc:0.934896\n",
      "->epoch:747, train_loss:0.000495, train_acc:0.995269, test_loss:0.016464, test_acc:0.932031\n",
      "->epoch:748, train_loss:0.000468, train_acc:0.995472, test_loss:0.016081, test_acc:0.933724\n",
      "->epoch:749, train_loss:0.000476, train_acc:0.995081, test_loss:0.014724, test_acc:0.931771\n",
      "->epoch:750, train_loss:0.000529, train_acc:0.994878, test_loss:0.013574, test_acc:0.932943\n",
      "->epoch:751, train_loss:0.000515, train_acc:0.994980, test_loss:0.012449, test_acc:0.935807\n",
      "->epoch:752, train_loss:0.000536, train_acc:0.994777, test_loss:0.012830, test_acc:0.935287\n",
      "->epoch:753, train_loss:0.000541, train_acc:0.995067, test_loss:0.012434, test_acc:0.931771\n",
      "->epoch:754, train_loss:0.000471, train_acc:0.994806, test_loss:0.014602, test_acc:0.930859\n",
      "->epoch:755, train_loss:0.000546, train_acc:0.995095, test_loss:0.011309, test_acc:0.938281\n",
      "->epoch:756, train_loss:0.000509, train_acc:0.995298, test_loss:0.015997, test_acc:0.932031\n",
      "->epoch:757, train_loss:0.000535, train_acc:0.994734, test_loss:0.012765, test_acc:0.934896\n",
      "->epoch:758, train_loss:0.000550, train_acc:0.994951, test_loss:0.018119, test_acc:0.931901\n",
      "->epoch:759, train_loss:0.000546, train_acc:0.994618, test_loss:0.011853, test_acc:0.934375\n",
      "->epoch:760, train_loss:0.000518, train_acc:0.994719, test_loss:0.015330, test_acc:0.934766\n",
      "->epoch:761, train_loss:0.000500, train_acc:0.995182, test_loss:0.016277, test_acc:0.936198\n",
      "->epoch:762, train_loss:0.000554, train_acc:0.994994, test_loss:0.015560, test_acc:0.932292\n",
      "->epoch:763, train_loss:0.000537, train_acc:0.995023, test_loss:0.014774, test_acc:0.929688\n",
      "->epoch:764, train_loss:0.000536, train_acc:0.995182, test_loss:0.017435, test_acc:0.931380\n",
      "->epoch:765, train_loss:0.000540, train_acc:0.994980, test_loss:0.012555, test_acc:0.930990\n",
      "->epoch:766, train_loss:0.000489, train_acc:0.995529, test_loss:0.014091, test_acc:0.935677\n",
      "->epoch:767, train_loss:0.000518, train_acc:0.994806, test_loss:0.013640, test_acc:0.934115\n",
      "->epoch:768, train_loss:0.000435, train_acc:0.995703, test_loss:0.016516, test_acc:0.931120\n",
      "->epoch:769, train_loss:0.000520, train_acc:0.994922, test_loss:0.016143, test_acc:0.930208\n",
      "->epoch:770, train_loss:0.000487, train_acc:0.995124, test_loss:0.015524, test_acc:0.935677\n",
      "->epoch:771, train_loss:0.000601, train_acc:0.995038, test_loss:0.011066, test_acc:0.932162\n",
      "->epoch:772, train_loss:0.000564, train_acc:0.994792, test_loss:0.013497, test_acc:0.935677\n",
      "->epoch:773, train_loss:0.000491, train_acc:0.994821, test_loss:0.013794, test_acc:0.934115\n",
      "->epoch:774, train_loss:0.000523, train_acc:0.994864, test_loss:0.015052, test_acc:0.929818\n",
      "->epoch:775, train_loss:0.000520, train_acc:0.994922, test_loss:0.015492, test_acc:0.933984\n",
      "->epoch:776, train_loss:0.000548, train_acc:0.995110, test_loss:0.012596, test_acc:0.932943\n",
      "->epoch:777, train_loss:0.000536, train_acc:0.994661, test_loss:0.012356, test_acc:0.933594\n",
      "->epoch:778, train_loss:0.000571, train_acc:0.994589, test_loss:0.014525, test_acc:0.930729\n",
      "->epoch:779, train_loss:0.000528, train_acc:0.995414, test_loss:0.012702, test_acc:0.935026\n",
      "->epoch:780, train_loss:0.000498, train_acc:0.995081, test_loss:0.015136, test_acc:0.936328\n",
      "->epoch:781, train_loss:0.000497, train_acc:0.995529, test_loss:0.015617, test_acc:0.932422\n",
      "->epoch:782, train_loss:0.000543, train_acc:0.994922, test_loss:0.015954, test_acc:0.936458\n",
      "->epoch:783, train_loss:0.000536, train_acc:0.995197, test_loss:0.017537, test_acc:0.934115\n",
      "->epoch:784, train_loss:0.000489, train_acc:0.995356, test_loss:0.016847, test_acc:0.931380\n",
      "->epoch:785, train_loss:0.000530, train_acc:0.995052, test_loss:0.015178, test_acc:0.933594\n",
      "->epoch:786, train_loss:0.000502, train_acc:0.995645, test_loss:0.013881, test_acc:0.936198\n",
      "->epoch:787, train_loss:0.000538, train_acc:0.995255, test_loss:0.014290, test_acc:0.931641\n",
      "->epoch:788, train_loss:0.000556, train_acc:0.994575, test_loss:0.014594, test_acc:0.934375\n",
      "->epoch:789, train_loss:0.000487, train_acc:0.995602, test_loss:0.014706, test_acc:0.934375\n",
      "->epoch:790, train_loss:0.000506, train_acc:0.994792, test_loss:0.014050, test_acc:0.931380\n",
      "->epoch:791, train_loss:0.000501, train_acc:0.995457, test_loss:0.018316, test_acc:0.931641\n",
      "->epoch:792, train_loss:0.000636, train_acc:0.994401, test_loss:0.012138, test_acc:0.933724\n",
      "->epoch:793, train_loss:0.000456, train_acc:0.995385, test_loss:0.017073, test_acc:0.935547\n",
      "->epoch:794, train_loss:0.000500, train_acc:0.994850, test_loss:0.016580, test_acc:0.931901\n",
      "->epoch:795, train_loss:0.000543, train_acc:0.994502, test_loss:0.016718, test_acc:0.932031\n",
      "->epoch:796, train_loss:0.000499, train_acc:0.995182, test_loss:0.016724, test_acc:0.935287\n",
      "->epoch:797, train_loss:0.000516, train_acc:0.994965, test_loss:0.019959, test_acc:0.934766\n",
      "->epoch:798, train_loss:0.000626, train_acc:0.994792, test_loss:0.013133, test_acc:0.929167\n",
      "->epoch:799, train_loss:0.000543, train_acc:0.994502, test_loss:0.013937, test_acc:0.933594\n",
      "->epoch:800, train_loss:0.000470, train_acc:0.995558, test_loss:0.014062, test_acc:0.929036\n",
      "->epoch:801, train_loss:0.000483, train_acc:0.995023, test_loss:0.012147, test_acc:0.933984\n",
      "->epoch:802, train_loss:0.000500, train_acc:0.995385, test_loss:0.013962, test_acc:0.932813\n",
      "->epoch:803, train_loss:0.000504, train_acc:0.995023, test_loss:0.013408, test_acc:0.932292\n",
      "->epoch:804, train_loss:0.000553, train_acc:0.994777, test_loss:0.012755, test_acc:0.925130\n",
      "->epoch:805, train_loss:0.000538, train_acc:0.995095, test_loss:0.016023, test_acc:0.927734\n",
      "->epoch:806, train_loss:0.000525, train_acc:0.994835, test_loss:0.016824, test_acc:0.930339\n",
      "->epoch:807, train_loss:0.000525, train_acc:0.994821, test_loss:0.013335, test_acc:0.930339\n",
      "->epoch:808, train_loss:0.000523, train_acc:0.995356, test_loss:0.012844, test_acc:0.930599\n",
      "->epoch:809, train_loss:0.000540, train_acc:0.994835, test_loss:0.013225, test_acc:0.936849\n",
      "->epoch:810, train_loss:0.000532, train_acc:0.994661, test_loss:0.012437, test_acc:0.933073\n",
      "->epoch:811, train_loss:0.000466, train_acc:0.995356, test_loss:0.014707, test_acc:0.934245\n",
      "->epoch:812, train_loss:0.000485, train_acc:0.995428, test_loss:0.013044, test_acc:0.935156\n",
      "->epoch:813, train_loss:0.000495, train_acc:0.995081, test_loss:0.014667, test_acc:0.931901\n",
      "->epoch:814, train_loss:0.000441, train_acc:0.995645, test_loss:0.014117, test_acc:0.935287\n",
      "->epoch:815, train_loss:0.000531, train_acc:0.995009, test_loss:0.012081, test_acc:0.934766\n",
      "->epoch:816, train_loss:0.000528, train_acc:0.995052, test_loss:0.012379, test_acc:0.929948\n",
      "->epoch:817, train_loss:0.000528, train_acc:0.995110, test_loss:0.015301, test_acc:0.930729\n",
      "->epoch:818, train_loss:0.000557, train_acc:0.994661, test_loss:0.014867, test_acc:0.931901\n",
      "->epoch:819, train_loss:0.000516, train_acc:0.995081, test_loss:0.014033, test_acc:0.933333\n",
      "->epoch:820, train_loss:0.000448, train_acc:0.995486, test_loss:0.013056, test_acc:0.932031\n",
      "->epoch:821, train_loss:0.000511, train_acc:0.995587, test_loss:0.012775, test_acc:0.929036\n",
      "->epoch:822, train_loss:0.000472, train_acc:0.995370, test_loss:0.014158, test_acc:0.931120\n",
      "->epoch:823, train_loss:0.000509, train_acc:0.995226, test_loss:0.012242, test_acc:0.936589\n",
      "->epoch:824, train_loss:0.000526, train_acc:0.994632, test_loss:0.016211, test_acc:0.935026\n",
      "->epoch:825, train_loss:0.000502, train_acc:0.994994, test_loss:0.018538, test_acc:0.933464\n",
      "->epoch:826, train_loss:0.000509, train_acc:0.995385, test_loss:0.014569, test_acc:0.931641\n",
      "->epoch:827, train_loss:0.000550, train_acc:0.994748, test_loss:0.014762, test_acc:0.931510\n",
      "->epoch:828, train_loss:0.000485, train_acc:0.995631, test_loss:0.016094, test_acc:0.928385\n",
      "->epoch:829, train_loss:0.000442, train_acc:0.995631, test_loss:0.015212, test_acc:0.935547\n",
      "->epoch:830, train_loss:0.000519, train_acc:0.995168, test_loss:0.014719, test_acc:0.937240\n",
      "->epoch:831, train_loss:0.000479, train_acc:0.995182, test_loss:0.015129, test_acc:0.933724\n",
      "->epoch:832, train_loss:0.000485, train_acc:0.995211, test_loss:0.012660, test_acc:0.936589\n",
      "->epoch:833, train_loss:0.000523, train_acc:0.994922, test_loss:0.013679, test_acc:0.932813\n",
      "->epoch:834, train_loss:0.000507, train_acc:0.994821, test_loss:0.018696, test_acc:0.932552\n",
      "->epoch:835, train_loss:0.000502, train_acc:0.995038, test_loss:0.014132, test_acc:0.932422\n",
      "->epoch:836, train_loss:0.000490, train_acc:0.995313, test_loss:0.011706, test_acc:0.937109\n",
      "->epoch:837, train_loss:0.000497, train_acc:0.994965, test_loss:0.013731, test_acc:0.931250\n",
      "->epoch:838, train_loss:0.000488, train_acc:0.995414, test_loss:0.016085, test_acc:0.932682\n",
      "->epoch:839, train_loss:0.000492, train_acc:0.995341, test_loss:0.015659, test_acc:0.931641\n",
      "->epoch:840, train_loss:0.000528, train_acc:0.994936, test_loss:0.013398, test_acc:0.931901\n",
      "->epoch:841, train_loss:0.000555, train_acc:0.994647, test_loss:0.014578, test_acc:0.938542\n",
      "->epoch:842, train_loss:0.000536, train_acc:0.995313, test_loss:0.015325, test_acc:0.931510\n",
      "->epoch:843, train_loss:0.000473, train_acc:0.995443, test_loss:0.015496, test_acc:0.932422\n",
      "->epoch:844, train_loss:0.000507, train_acc:0.994878, test_loss:0.014094, test_acc:0.936068\n",
      "->epoch:845, train_loss:0.000537, train_acc:0.994719, test_loss:0.013960, test_acc:0.937630\n",
      "->epoch:846, train_loss:0.000523, train_acc:0.994676, test_loss:0.017548, test_acc:0.929688\n",
      "->epoch:847, train_loss:0.000554, train_acc:0.995081, test_loss:0.014099, test_acc:0.932162\n",
      "->epoch:848, train_loss:0.000505, train_acc:0.995240, test_loss:0.012871, test_acc:0.932943\n",
      "->epoch:849, train_loss:0.000498, train_acc:0.995255, test_loss:0.012254, test_acc:0.934896\n",
      "->epoch:850, train_loss:0.000526, train_acc:0.995385, test_loss:0.014624, test_acc:0.933333\n",
      "->epoch:851, train_loss:0.000442, train_acc:0.995848, test_loss:0.014525, test_acc:0.927865\n",
      "->epoch:852, train_loss:0.000450, train_acc:0.995848, test_loss:0.015237, test_acc:0.930339\n",
      "->epoch:853, train_loss:0.000517, train_acc:0.994965, test_loss:0.014352, test_acc:0.936719\n",
      "->epoch:854, train_loss:0.000427, train_acc:0.995674, test_loss:0.014397, test_acc:0.932813\n",
      "->epoch:855, train_loss:0.000497, train_acc:0.995240, test_loss:0.014757, test_acc:0.932031\n",
      "->epoch:856, train_loss:0.000531, train_acc:0.995139, test_loss:0.013369, test_acc:0.937109\n",
      "->epoch:857, train_loss:0.000491, train_acc:0.995110, test_loss:0.015364, test_acc:0.935547\n",
      "->epoch:858, train_loss:0.000480, train_acc:0.995414, test_loss:0.015037, test_acc:0.936328\n",
      "->epoch:859, train_loss:0.000471, train_acc:0.995255, test_loss:0.019007, test_acc:0.930729\n",
      "->epoch:860, train_loss:0.000492, train_acc:0.995631, test_loss:0.017390, test_acc:0.934245\n",
      "->epoch:861, train_loss:0.000604, train_acc:0.994835, test_loss:0.014640, test_acc:0.934505\n",
      "->epoch:862, train_loss:0.000546, train_acc:0.995255, test_loss:0.017380, test_acc:0.936068\n",
      "->epoch:863, train_loss:0.000557, train_acc:0.995211, test_loss:0.014028, test_acc:0.927083\n",
      "->epoch:864, train_loss:0.000580, train_acc:0.994821, test_loss:0.020758, test_acc:0.932031\n",
      "->epoch:865, train_loss:0.000541, train_acc:0.995211, test_loss:0.012405, test_acc:0.933594\n",
      "->epoch:866, train_loss:0.000463, train_acc:0.995284, test_loss:0.015542, test_acc:0.937240\n",
      "->epoch:867, train_loss:0.000484, train_acc:0.995255, test_loss:0.015235, test_acc:0.933594\n",
      "->epoch:868, train_loss:0.000541, train_acc:0.995095, test_loss:0.015603, test_acc:0.937240\n",
      "->epoch:869, train_loss:0.000424, train_acc:0.995573, test_loss:0.013898, test_acc:0.934115\n",
      "->epoch:870, train_loss:0.000512, train_acc:0.995284, test_loss:0.016766, test_acc:0.932943\n",
      "->epoch:871, train_loss:0.000517, train_acc:0.995124, test_loss:0.013457, test_acc:0.937370\n",
      "->epoch:872, train_loss:0.000530, train_acc:0.994705, test_loss:0.013614, test_acc:0.936068\n",
      "->epoch:873, train_loss:0.000499, train_acc:0.995110, test_loss:0.017007, test_acc:0.936458\n",
      "->epoch:874, train_loss:0.000471, train_acc:0.995385, test_loss:0.013114, test_acc:0.934766\n",
      "->epoch:875, train_loss:0.000502, train_acc:0.995139, test_loss:0.015822, test_acc:0.933854\n",
      "->epoch:876, train_loss:0.000555, train_acc:0.995139, test_loss:0.014206, test_acc:0.935938\n",
      "->epoch:877, train_loss:0.000525, train_acc:0.994980, test_loss:0.013655, test_acc:0.931510\n",
      "->epoch:878, train_loss:0.000464, train_acc:0.995558, test_loss:0.012022, test_acc:0.934766\n",
      "->epoch:879, train_loss:0.000527, train_acc:0.995356, test_loss:0.014508, test_acc:0.934896\n",
      "->epoch:880, train_loss:0.000582, train_acc:0.995067, test_loss:0.016456, test_acc:0.933203\n",
      "->epoch:881, train_loss:0.000499, train_acc:0.995255, test_loss:0.014188, test_acc:0.934115\n",
      "->epoch:882, train_loss:0.000549, train_acc:0.995269, test_loss:0.013540, test_acc:0.933073\n",
      "->epoch:883, train_loss:0.000554, train_acc:0.994922, test_loss:0.014138, test_acc:0.938151\n",
      "->epoch:884, train_loss:0.000499, train_acc:0.995515, test_loss:0.020369, test_acc:0.928255\n",
      "->epoch:885, train_loss:0.000493, train_acc:0.995284, test_loss:0.014878, test_acc:0.936719\n",
      "->epoch:886, train_loss:0.000556, train_acc:0.994936, test_loss:0.015315, test_acc:0.934896\n",
      "->epoch:887, train_loss:0.000494, train_acc:0.995139, test_loss:0.011794, test_acc:0.934375\n",
      "->epoch:888, train_loss:0.000480, train_acc:0.995587, test_loss:0.015078, test_acc:0.934245\n",
      "->epoch:889, train_loss:0.000460, train_acc:0.995645, test_loss:0.015453, test_acc:0.934896\n",
      "->epoch:890, train_loss:0.000515, train_acc:0.995067, test_loss:0.016697, test_acc:0.931771\n",
      "->epoch:891, train_loss:0.000518, train_acc:0.994965, test_loss:0.014960, test_acc:0.931641\n",
      "->epoch:892, train_loss:0.000532, train_acc:0.995110, test_loss:0.014177, test_acc:0.936198\n",
      "->epoch:893, train_loss:0.000508, train_acc:0.995081, test_loss:0.014626, test_acc:0.931250\n",
      "->epoch:894, train_loss:0.000536, train_acc:0.995110, test_loss:0.013279, test_acc:0.936719\n",
      "->epoch:895, train_loss:0.000532, train_acc:0.995226, test_loss:0.014028, test_acc:0.937760\n",
      "->epoch:896, train_loss:0.000519, train_acc:0.995153, test_loss:0.013461, test_acc:0.933464\n",
      "->epoch:897, train_loss:0.000513, train_acc:0.995298, test_loss:0.018225, test_acc:0.930990\n",
      "->epoch:898, train_loss:0.000462, train_acc:0.995587, test_loss:0.015500, test_acc:0.933854\n",
      "->epoch:899, train_loss:0.000473, train_acc:0.995457, test_loss:0.014661, test_acc:0.935287\n",
      "->epoch:900, train_loss:0.000477, train_acc:0.995486, test_loss:0.013628, test_acc:0.938412\n",
      "->epoch:901, train_loss:0.000478, train_acc:0.995327, test_loss:0.012215, test_acc:0.931510\n",
      "->epoch:902, train_loss:0.000509, train_acc:0.995110, test_loss:0.016149, test_acc:0.934505\n",
      "->epoch:903, train_loss:0.000540, train_acc:0.995023, test_loss:0.015465, test_acc:0.932031\n",
      "->epoch:904, train_loss:0.000548, train_acc:0.994777, test_loss:0.015086, test_acc:0.930729\n",
      "->epoch:905, train_loss:0.000457, train_acc:0.995515, test_loss:0.015590, test_acc:0.935287\n",
      "->epoch:906, train_loss:0.000480, train_acc:0.995067, test_loss:0.014240, test_acc:0.934375\n",
      "->epoch:907, train_loss:0.000532, train_acc:0.994705, test_loss:0.015922, test_acc:0.935677\n",
      "->epoch:908, train_loss:0.000481, train_acc:0.995602, test_loss:0.014973, test_acc:0.935677\n",
      "->epoch:909, train_loss:0.000441, train_acc:0.995370, test_loss:0.014481, test_acc:0.934635\n",
      "->epoch:910, train_loss:0.000518, train_acc:0.994965, test_loss:0.014873, test_acc:0.930729\n",
      "->epoch:911, train_loss:0.000546, train_acc:0.995067, test_loss:0.012914, test_acc:0.933464\n",
      "->epoch:912, train_loss:0.000464, train_acc:0.995182, test_loss:0.015408, test_acc:0.934766\n",
      "->epoch:913, train_loss:0.000531, train_acc:0.995124, test_loss:0.013934, test_acc:0.931510\n",
      "->epoch:914, train_loss:0.000532, train_acc:0.994922, test_loss:0.013690, test_acc:0.935938\n",
      "->epoch:915, train_loss:0.000505, train_acc:0.995226, test_loss:0.013559, test_acc:0.933854\n",
      "->epoch:916, train_loss:0.000560, train_acc:0.995197, test_loss:0.015657, test_acc:0.935026\n",
      "->epoch:917, train_loss:0.000542, train_acc:0.995153, test_loss:0.012007, test_acc:0.923568\n",
      "->epoch:918, train_loss:0.000461, train_acc:0.995370, test_loss:0.014596, test_acc:0.936458\n",
      "->epoch:919, train_loss:0.000493, train_acc:0.995689, test_loss:0.017135, test_acc:0.934375\n",
      "->epoch:920, train_loss:0.000521, train_acc:0.995009, test_loss:0.013056, test_acc:0.931771\n",
      "->epoch:921, train_loss:0.000516, train_acc:0.994893, test_loss:0.013234, test_acc:0.936979\n",
      "->epoch:922, train_loss:0.000462, train_acc:0.995284, test_loss:0.015291, test_acc:0.935417\n",
      "->epoch:923, train_loss:0.000498, train_acc:0.995139, test_loss:0.015453, test_acc:0.931510\n",
      "->epoch:924, train_loss:0.000480, train_acc:0.995255, test_loss:0.017478, test_acc:0.931901\n",
      "->epoch:925, train_loss:0.000488, train_acc:0.995472, test_loss:0.014085, test_acc:0.931380\n",
      "->epoch:926, train_loss:0.000442, train_acc:0.995645, test_loss:0.012071, test_acc:0.928776\n",
      "->epoch:927, train_loss:0.000562, train_acc:0.995501, test_loss:0.015294, test_acc:0.933203\n",
      "->epoch:928, train_loss:0.000626, train_acc:0.995269, test_loss:0.015419, test_acc:0.933854\n",
      "->epoch:929, train_loss:0.000475, train_acc:0.995486, test_loss:0.012912, test_acc:0.937630\n",
      "->epoch:930, train_loss:0.000479, train_acc:0.995414, test_loss:0.013635, test_acc:0.933333\n",
      "->epoch:931, train_loss:0.000534, train_acc:0.995211, test_loss:0.016619, test_acc:0.934505\n",
      "->epoch:932, train_loss:0.000527, train_acc:0.995110, test_loss:0.014491, test_acc:0.937891\n",
      "->epoch:933, train_loss:0.000489, train_acc:0.995110, test_loss:0.014384, test_acc:0.935287\n",
      "->epoch:934, train_loss:0.000456, train_acc:0.995573, test_loss:0.016139, test_acc:0.933854\n",
      "->epoch:935, train_loss:0.000506, train_acc:0.995226, test_loss:0.015337, test_acc:0.936068\n",
      "->epoch:936, train_loss:0.000668, train_acc:0.994748, test_loss:0.015672, test_acc:0.933333\n",
      "->epoch:937, train_loss:0.000475, train_acc:0.995269, test_loss:0.014997, test_acc:0.934896\n",
      "->epoch:938, train_loss:0.000541, train_acc:0.995240, test_loss:0.013485, test_acc:0.930078\n",
      "->epoch:939, train_loss:0.000493, train_acc:0.994936, test_loss:0.013788, test_acc:0.935417\n",
      "->epoch:940, train_loss:0.000578, train_acc:0.994806, test_loss:0.011736, test_acc:0.935417\n",
      "->epoch:941, train_loss:0.000520, train_acc:0.995515, test_loss:0.015945, test_acc:0.932682\n",
      "->epoch:942, train_loss:0.000527, train_acc:0.995052, test_loss:0.013213, test_acc:0.932422\n",
      "->epoch:943, train_loss:0.000472, train_acc:0.995732, test_loss:0.017025, test_acc:0.934375\n",
      "->epoch:944, train_loss:0.000475, train_acc:0.995356, test_loss:0.013027, test_acc:0.935287\n",
      "->epoch:945, train_loss:0.000566, train_acc:0.994922, test_loss:0.014674, test_acc:0.935287\n",
      "->epoch:946, train_loss:0.000498, train_acc:0.995327, test_loss:0.016879, test_acc:0.934115\n",
      "->epoch:947, train_loss:0.000475, train_acc:0.995414, test_loss:0.014175, test_acc:0.936198\n",
      "->epoch:948, train_loss:0.000458, train_acc:0.995992, test_loss:0.017363, test_acc:0.934505\n",
      "->epoch:949, train_loss:0.000477, train_acc:0.995269, test_loss:0.015331, test_acc:0.936719\n",
      "->epoch:950, train_loss:0.000478, train_acc:0.995124, test_loss:0.016525, test_acc:0.933203\n",
      "->epoch:951, train_loss:0.000458, train_acc:0.995587, test_loss:0.014395, test_acc:0.932162\n",
      "->epoch:952, train_loss:0.000526, train_acc:0.995182, test_loss:0.013299, test_acc:0.931771\n",
      "->epoch:953, train_loss:0.000486, train_acc:0.995341, test_loss:0.013890, test_acc:0.934766\n",
      "->epoch:954, train_loss:0.000548, train_acc:0.994994, test_loss:0.013083, test_acc:0.932552\n",
      "->epoch:955, train_loss:0.000473, train_acc:0.995226, test_loss:0.014933, test_acc:0.936068\n",
      "->epoch:956, train_loss:0.000457, train_acc:0.995443, test_loss:0.016671, test_acc:0.932943\n",
      "->epoch:957, train_loss:0.000490, train_acc:0.995341, test_loss:0.014099, test_acc:0.931120\n",
      "->epoch:958, train_loss:0.000477, train_acc:0.995602, test_loss:0.015713, test_acc:0.933073\n",
      "->epoch:959, train_loss:0.000539, train_acc:0.995038, test_loss:0.014007, test_acc:0.932813\n",
      "->epoch:960, train_loss:0.000513, train_acc:0.995327, test_loss:0.012952, test_acc:0.933333\n",
      "->epoch:961, train_loss:0.000450, train_acc:0.995472, test_loss:0.014732, test_acc:0.932292\n",
      "->epoch:962, train_loss:0.000483, train_acc:0.995038, test_loss:0.014346, test_acc:0.935807\n",
      "->epoch:963, train_loss:0.000515, train_acc:0.995486, test_loss:0.017478, test_acc:0.930339\n",
      "->epoch:964, train_loss:0.000486, train_acc:0.995182, test_loss:0.014778, test_acc:0.931510\n",
      "->epoch:965, train_loss:0.000478, train_acc:0.995660, test_loss:0.015375, test_acc:0.930078\n",
      "->epoch:966, train_loss:0.000460, train_acc:0.995602, test_loss:0.013749, test_acc:0.933464\n",
      "->epoch:967, train_loss:0.000435, train_acc:0.995501, test_loss:0.014782, test_acc:0.934245\n",
      "->epoch:968, train_loss:0.000515, train_acc:0.995269, test_loss:0.014570, test_acc:0.933594\n",
      "->epoch:969, train_loss:0.000563, train_acc:0.995009, test_loss:0.015055, test_acc:0.936849\n",
      "->epoch:970, train_loss:0.000602, train_acc:0.994936, test_loss:0.013391, test_acc:0.934635\n",
      "->epoch:971, train_loss:0.000437, train_acc:0.996007, test_loss:0.014451, test_acc:0.931510\n",
      "->epoch:972, train_loss:0.000498, train_acc:0.995313, test_loss:0.015142, test_acc:0.933464\n",
      "->epoch:973, train_loss:0.000524, train_acc:0.995457, test_loss:0.013254, test_acc:0.935417\n",
      "->epoch:974, train_loss:0.000480, train_acc:0.995833, test_loss:0.014464, test_acc:0.934896\n",
      "->epoch:975, train_loss:0.000518, train_acc:0.995313, test_loss:0.014285, test_acc:0.930208\n",
      "->epoch:976, train_loss:0.000453, train_acc:0.995370, test_loss:0.016545, test_acc:0.933203\n",
      "->epoch:977, train_loss:0.000542, train_acc:0.994951, test_loss:0.015450, test_acc:0.932162\n",
      "->epoch:978, train_loss:0.000496, train_acc:0.995182, test_loss:0.013571, test_acc:0.938932\n",
      "->epoch:979, train_loss:0.000471, train_acc:0.995428, test_loss:0.013102, test_acc:0.933333\n",
      "->epoch:980, train_loss:0.000564, train_acc:0.995009, test_loss:0.016613, test_acc:0.929036\n",
      "->epoch:981, train_loss:0.000490, train_acc:0.995240, test_loss:0.017340, test_acc:0.932943\n",
      "->epoch:982, train_loss:0.000431, train_acc:0.995602, test_loss:0.016642, test_acc:0.931380\n",
      "->epoch:983, train_loss:0.000543, train_acc:0.995067, test_loss:0.014142, test_acc:0.935807\n",
      "->epoch:984, train_loss:0.000441, train_acc:0.995732, test_loss:0.013083, test_acc:0.940495\n",
      "->epoch:985, train_loss:0.000516, train_acc:0.995211, test_loss:0.013575, test_acc:0.937240\n",
      "->epoch:986, train_loss:0.000508, train_acc:0.995862, test_loss:0.012306, test_acc:0.934896\n",
      "->epoch:987, train_loss:0.000430, train_acc:0.995544, test_loss:0.012781, test_acc:0.935938\n",
      "->epoch:988, train_loss:0.000500, train_acc:0.995153, test_loss:0.013736, test_acc:0.937109\n",
      "->epoch:989, train_loss:0.000538, train_acc:0.995240, test_loss:0.013802, test_acc:0.935807\n",
      "->epoch:990, train_loss:0.000498, train_acc:0.995327, test_loss:0.012880, test_acc:0.933464\n",
      "->epoch:991, train_loss:0.000474, train_acc:0.995616, test_loss:0.016830, test_acc:0.931771\n",
      "->epoch:992, train_loss:0.000472, train_acc:0.995486, test_loss:0.015465, test_acc:0.935807\n",
      "->epoch:993, train_loss:0.000510, train_acc:0.995139, test_loss:0.014159, test_acc:0.936198\n",
      "->epoch:994, train_loss:0.000509, train_acc:0.995255, test_loss:0.016120, test_acc:0.935547\n",
      "->epoch:995, train_loss:0.000472, train_acc:0.995472, test_loss:0.015749, test_acc:0.938542\n",
      "->epoch:996, train_loss:0.000462, train_acc:0.995168, test_loss:0.013240, test_acc:0.933333\n",
      "->epoch:997, train_loss:0.000600, train_acc:0.994922, test_loss:0.013756, test_acc:0.939323\n",
      "->epoch:998, train_loss:0.000498, train_acc:0.995558, test_loss:0.012833, test_acc:0.932943\n",
      "->epoch:999, train_loss:0.000468, train_acc:0.995313, test_loss:0.014620, test_acc:0.933984\n",
      "->epoch:1000, train_loss:0.000517, train_acc:0.995558, test_loss:0.013547, test_acc:0.934375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f'->epoch:{epoch + 1}', end = ', ')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim)\n",
    "#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaaa6f-c3fb-4b07-91b9-aa185cf2b0ba",
   "metadata": {},
   "source": [
    "- MLP_base ->epoch:195, train_loss:0.000070, train_acc:0.999494, test_loss:0.022968, test_acc:0.933854\n",
    "- MLP_2层dropout（p=0.2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa21ed2-da43-4dc3-ad70-081231e98ef4",
   "metadata": {},
   "source": [
    "# 增加模型容量\n",
    "- ->epoch:86, train_loss:0.002378, train_acc:0.980787, test_loss:0.017223, test_acc:0.908203\n",
    "- 改为heads=3， ->epoch:167, train_loss:0.001960, train_acc:0.985171, test_loss:0.022945, test_acc:0.910417"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964d82-e4ca-40db-ad83-0722b8a6b880",
   "metadata": {},
   "source": [
    "## 比较实验\n",
    "### GCN\n",
    "+ 仅包括自环时，->epoch:25, train_loss:0.000922, train_acc:0.990784, test_loss:0.010875, test_acc:0.919401\n",
    "+ 加上3x3卷积核的邻接边时，->epoch:32, train_loss:0.000819, train_acc:0.992173, test_loss:0.020655, test_acc:0.895313，邻接边设计的不好，限制了模型的发挥\n",
    "+ 别人的方法的准确率：89/90、93/94\n",
    "### GAT\n",
    "+ 仅包括自环时，->epoch:30, train_loss:0.001252, train_acc:0.986531, test_loss:0.015308, test_acc:0.912630\n",
    "+ 使用自己设计的边，->epoch:123, train_loss:0.002104, train_acc:0.982161, test_loss:0.029411, test_acc:0.904688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f4ff-9fbb-4323-ad0d-4dfc7848e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
