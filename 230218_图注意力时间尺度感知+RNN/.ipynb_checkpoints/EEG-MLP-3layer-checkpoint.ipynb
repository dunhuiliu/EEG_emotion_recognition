{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6df28-9919-4bff-8e9a-ea82c35eba11",
   "metadata": {},
   "source": [
    "### 脑电图注意力网络（GAT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def data_split(train_ratio=0.7):\n",
    "#     load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "#     trials = np.load(load_dir + 'trials.npy')\n",
    "#     bases = np.load(load_dir + 'bases.npy')\n",
    "#     labels = np.load(load_dir + 'labels.npy')\n",
    "#     # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "#     # 去基线\n",
    "#     for i, base in enumerate(bases):\n",
    "#         trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "#     # 离散化标签\n",
    "#     labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "#     # 复制标签以对齐样本\n",
    "#     labels = np.repeat(labels, 60, axis = 0)\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     shuffle_list = np.arange(trials.shape[0])\n",
    "#     np.random.shuffle(shuffle_list)\n",
    "#     trials = trials[shuffle_list]\n",
    "#     labels = labels[shuffle_list]\n",
    "    \n",
    "#     cut_point = int(trials.shape[0] * train_ratio)\n",
    "#     train_features, train_labels = trials[:cut_point], labels[:cut_point]\n",
    "#     test_features, test_labels = trials[cut_point:], labels[cut_point:]\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32 * 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32 * 128))\n",
    "    \n",
    "#     mean = train_features.mean(axis = 0)\n",
    "#     std = train_features.std(axis = 0)\n",
    "    \n",
    "#     train_features = (train_features - mean) / std\n",
    "#     test_features = (test_features - mean) / std\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32, 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32, 128))\n",
    "    \n",
    "#     save_dir = 'data/data_split/'\n",
    "#     np.save(save_dir + 'train_features.npy', train_features)\n",
    "#     np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "#     np.save(save_dir + 'test_features.npy', test_features)\n",
    "#     np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "# data_split(train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b5e808-1250-4784-b6b7-a0e58d86017a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if create_complete_graph:\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        return edge_index\n",
    "    \n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd42b5bf-ca24-4f71-95ad-b0b494aaa18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcb8368-b508-41fb-baa4-5e35d16ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index = [[],[]]\n",
    "# weight = []\n",
    "\n",
    "# #用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "# chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "#               9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "#               17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "#                 25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "# maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "# for k, v in chan_to_1020.items():\n",
    "#     maps[v[0]][v[1]] = k + 1\n",
    "# print(maps)\n",
    "# plt.matshow(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    edge_index = None\n",
    "    def __init__(self, root, is_train_data, edge_index):\n",
    "        self.is_train_data = is_train_data\n",
    "        self.edge_index = edge_index\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['train.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor(labels[i].reshape(1, -1), dtype=torch.long)\n",
    "            data = Data(x = x, edge_index=self.edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.temporalMLPs = []\n",
    "        for i in range(32):\n",
    "            self.temporalMLPs.append(nn.Linear(128, 256, device=device))\n",
    "            \n",
    "            \n",
    "        self.temporalMLPs1 = []\n",
    "        for i in range(32):\n",
    "            self.temporalMLPs1.append(nn.Linear(256, 256, device=device))\n",
    "        \n",
    "#         self.bn1 = []\n",
    "#         for i in range(32):\n",
    "#             self.bn1.append(nn.BatchNorm1d(256, device=device))\n",
    "        \n",
    "        self.temporalMLPs2 = []\n",
    "        for i in range(32):\n",
    "            self.temporalMLPs2.append(nn.Linear(256, 256, device=device))\n",
    "            \n",
    "#         self.bn2 = []\n",
    "#         for i in range(32):\n",
    "#             self.bn2.append(nn.BatchNorm1d(256, device=device))\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(8192, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.view(batch_size, 32, 128)\n",
    "        \n",
    "        temporalMLPs_out = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            x1 = self.temporalMLPs[i](x[:, i, :])\n",
    "            x1 = F.relu(x1)\n",
    "            \n",
    "            x1 = self.temporalMLPs1[i](x1)\n",
    "#             x1 = self.bn1[i](x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, 0.2)\n",
    "            \n",
    "            x1 = self.temporalMLPs2[i](x1)\n",
    "#             x1 = self.bn2[i](x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, 0.2)\n",
    "            temporalMLPs_out.append(x1)\n",
    "        \n",
    "        # concat\n",
    "        x = torch.concat(temporalMLPs_out, dim = 1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emo_dim):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        train_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_train_sample = len(trainDataLoader.dataset)\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    train_acc = train_acc / num_train_sample\n",
    "    \n",
    "    # check测试集的性能\n",
    "    vali_loss = 0\n",
    "    vali_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in testDataLoader:\n",
    "        batch.to(device)\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        vali_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        vali_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_test_sample = len(testDataLoader.dataset)\n",
    "    vali_loss = vali_loss / num_test_sample\n",
    "    vali_acc = vali_acc / num_test_sample\n",
    "    \n",
    "    print(f'train_loss:{train_loss:.6f}, train_acc:{train_acc:.6f}, test_loss:{vali_loss:.6f}, test_acc:{vali_acc:.6f}')\n",
    "    \n",
    "    return train_loss, train_acc, vali_loss, vali_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9bf77-c20d-484b-932b-8b98917f374b",
   "metadata": {},
   "source": [
    "# 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1e1611-fac8-4210-b800-bf02914ed6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete_graph = False\n",
    "self_loop_only = False\n",
    "emo_dim = 0\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "edge_index = get_edge_index(create_complete_graph=create_complete_graph, self_loop_only=self_loop_only)\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, edge_index=edge_index)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, edge_index=edge_index)\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3f3d55-fc83-475e-a697-9b8a12789830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "crit = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:1, train_loss:0.020009, train_acc:0.627894, test_loss:0.018319, test_acc:0.684896\n",
      "->epoch:2, train_loss:0.017589, train_acc:0.704167, test_loss:0.016957, test_acc:0.723828\n",
      "->epoch:3, train_loss:0.016217, train_acc:0.740162, test_loss:0.017354, test_acc:0.712630\n",
      "->epoch:4, train_loss:0.014932, train_acc:0.769415, test_loss:0.015476, test_acc:0.764974\n",
      "->epoch:5, train_loss:0.013904, train_acc:0.790220, test_loss:0.014930, test_acc:0.774609\n",
      "->epoch:6, train_loss:0.013051, train_acc:0.804311, test_loss:0.014116, test_acc:0.785286\n",
      "->epoch:7, train_loss:0.012281, train_acc:0.818837, test_loss:0.013133, test_acc:0.809375\n",
      "->epoch:8, train_loss:0.011719, train_acc:0.830049, test_loss:0.012956, test_acc:0.813672\n",
      "->epoch:9, train_loss:0.011228, train_acc:0.839034, test_loss:0.013056, test_acc:0.800911\n",
      "->epoch:10, train_loss:0.010926, train_acc:0.843808, test_loss:0.012348, test_acc:0.824349\n",
      "->epoch:11, train_loss:0.010549, train_acc:0.848741, test_loss:0.012506, test_acc:0.830729\n",
      "->epoch:12, train_loss:0.010284, train_acc:0.854109, test_loss:0.011990, test_acc:0.832682\n",
      "->epoch:13, train_loss:0.010006, train_acc:0.858608, test_loss:0.012288, test_acc:0.834896\n",
      "->epoch:14, train_loss:0.009622, train_acc:0.864236, test_loss:0.011989, test_acc:0.828516\n",
      "->epoch:15, train_loss:0.009487, train_acc:0.867057, test_loss:0.011696, test_acc:0.833984\n",
      "->epoch:16, train_loss:0.009298, train_acc:0.870558, test_loss:0.012180, test_acc:0.833073\n",
      "->epoch:17, train_loss:0.009139, train_acc:0.872497, test_loss:0.012429, test_acc:0.822266\n",
      "->epoch:18, train_loss:0.008953, train_acc:0.875087, test_loss:0.011326, test_acc:0.846875\n",
      "->epoch:19, train_loss:0.008832, train_acc:0.878197, test_loss:0.011406, test_acc:0.840365\n",
      "->epoch:20, train_loss:0.008646, train_acc:0.881351, test_loss:0.011707, test_acc:0.841016\n",
      "->epoch:21, train_loss:0.008450, train_acc:0.883073, test_loss:0.011659, test_acc:0.841667\n",
      "->epoch:22, train_loss:0.008489, train_acc:0.881698, test_loss:0.011871, test_acc:0.839063\n",
      "->epoch:23, train_loss:0.008229, train_acc:0.886777, test_loss:0.011979, test_acc:0.847005\n",
      "->epoch:24, train_loss:0.008337, train_acc:0.884375, test_loss:0.011377, test_acc:0.846615\n",
      "->epoch:25, train_loss:0.008137, train_acc:0.889251, test_loss:0.012153, test_acc:0.841927\n",
      "->epoch:26, train_loss:0.008019, train_acc:0.889656, test_loss:0.011252, test_acc:0.852734\n",
      "->epoch:27, train_loss:0.007959, train_acc:0.892303, test_loss:0.011688, test_acc:0.841667\n",
      "->epoch:28, train_loss:0.007917, train_acc:0.891450, test_loss:0.011272, test_acc:0.850781\n",
      "->epoch:29, train_loss:0.007799, train_acc:0.894401, test_loss:0.011084, test_acc:0.860807\n",
      "->epoch:30, train_loss:0.007758, train_acc:0.894618, test_loss:0.011082, test_acc:0.855990\n",
      "->epoch:31, train_loss:0.007746, train_acc:0.894777, test_loss:0.011893, test_acc:0.840104\n",
      "->epoch:32, train_loss:0.007640, train_acc:0.897135, test_loss:0.011736, test_acc:0.851042\n",
      "->epoch:33, train_loss:0.007627, train_acc:0.896123, test_loss:0.011426, test_acc:0.851172\n",
      "->epoch:34, train_loss:0.007510, train_acc:0.896991, test_loss:0.012460, test_acc:0.837891\n",
      "->epoch:35, train_loss:0.007377, train_acc:0.899117, test_loss:0.011877, test_acc:0.842578\n",
      "->epoch:36, train_loss:0.007329, train_acc:0.900984, test_loss:0.011360, test_acc:0.850781\n",
      "->epoch:37, train_loss:0.007382, train_acc:0.900318, test_loss:0.011572, test_acc:0.855339\n",
      "->epoch:38, train_loss:0.007222, train_acc:0.901837, test_loss:0.010963, test_acc:0.860677\n",
      "->epoch:39, train_loss:0.007172, train_acc:0.902734, test_loss:0.011331, test_acc:0.852865\n",
      "->epoch:40, train_loss:0.007208, train_acc:0.901924, test_loss:0.010988, test_acc:0.857682\n",
      "->epoch:41, train_loss:0.007103, train_acc:0.904731, test_loss:0.011274, test_acc:0.860547\n",
      "->epoch:42, train_loss:0.007060, train_acc:0.904962, test_loss:0.011028, test_acc:0.856901\n",
      "->epoch:43, train_loss:0.007053, train_acc:0.905801, test_loss:0.011567, test_acc:0.853646\n",
      "->epoch:44, train_loss:0.007022, train_acc:0.905498, test_loss:0.011858, test_acc:0.844271\n",
      "->epoch:45, train_loss:0.006973, train_acc:0.906091, test_loss:0.011425, test_acc:0.857031\n",
      "->epoch:46, train_loss:0.006947, train_acc:0.906814, test_loss:0.012487, test_acc:0.851172\n",
      "->epoch:47, train_loss:0.007071, train_acc:0.905295, test_loss:0.010823, test_acc:0.863281\n",
      "->epoch:48, train_loss:0.006913, train_acc:0.906626, test_loss:0.010996, test_acc:0.858073\n",
      "->epoch:49, train_loss:0.006827, train_acc:0.908738, test_loss:0.011924, test_acc:0.855859\n",
      "->epoch:50, train_loss:0.006798, train_acc:0.908883, test_loss:0.011051, test_acc:0.859766\n",
      "->epoch:51, train_loss:0.006786, train_acc:0.909491, test_loss:0.010919, test_acc:0.859115\n",
      "->epoch:52, train_loss:0.006701, train_acc:0.910532, test_loss:0.011848, test_acc:0.852995\n",
      "->epoch:53, train_loss:0.006664, train_acc:0.910778, test_loss:0.010760, test_acc:0.861328\n",
      "->epoch:54, train_loss:0.006705, train_acc:0.909954, test_loss:0.010656, test_acc:0.861068\n",
      "->epoch:55, train_loss:0.006696, train_acc:0.910489, test_loss:0.010962, test_acc:0.861328\n",
      "->epoch:56, train_loss:0.006643, train_acc:0.911994, test_loss:0.010677, test_acc:0.866016\n",
      "->epoch:57, train_loss:0.006607, train_acc:0.912674, test_loss:0.011679, test_acc:0.858594\n",
      "->epoch:58, train_loss:0.006559, train_acc:0.912572, test_loss:0.010762, test_acc:0.866406\n",
      "->epoch:59, train_loss:0.006516, train_acc:0.913484, test_loss:0.010503, test_acc:0.863542\n",
      "->epoch:60, train_loss:0.006543, train_acc:0.912297, test_loss:0.011138, test_acc:0.860026\n",
      "->epoch:61, train_loss:0.006570, train_acc:0.912037, test_loss:0.011402, test_acc:0.867057\n",
      "->epoch:62, train_loss:0.006444, train_acc:0.913947, test_loss:0.012572, test_acc:0.852865\n",
      "->epoch:63, train_loss:0.006351, train_acc:0.914757, test_loss:0.010827, test_acc:0.859635\n",
      "->epoch:64, train_loss:0.006379, train_acc:0.914829, test_loss:0.011678, test_acc:0.862630\n",
      "->epoch:65, train_loss:0.006308, train_acc:0.916276, test_loss:0.010853, test_acc:0.870964\n",
      "->epoch:66, train_loss:0.006334, train_acc:0.916638, test_loss:0.010481, test_acc:0.868880\n",
      "->epoch:67, train_loss:0.006252, train_acc:0.917101, test_loss:0.010980, test_acc:0.866016\n",
      "->epoch:68, train_loss:0.006357, train_acc:0.915249, test_loss:0.011185, test_acc:0.862370\n",
      "->epoch:69, train_loss:0.006243, train_acc:0.916233, test_loss:0.010852, test_acc:0.869271\n",
      "->epoch:70, train_loss:0.006218, train_acc:0.917318, test_loss:0.011192, test_acc:0.864323\n",
      "->epoch:71, train_loss:0.006281, train_acc:0.916869, test_loss:0.011028, test_acc:0.863542\n",
      "->epoch:72, train_loss:0.006270, train_acc:0.916638, test_loss:0.010751, test_acc:0.868229\n",
      "->epoch:73, train_loss:0.006204, train_acc:0.917376, test_loss:0.011078, test_acc:0.870443\n",
      "->epoch:74, train_loss:0.006212, train_acc:0.917347, test_loss:0.010803, test_acc:0.867969\n",
      "->epoch:75, train_loss:0.006275, train_acc:0.916667, test_loss:0.011628, test_acc:0.867839\n",
      "->epoch:76, train_loss:0.006154, train_acc:0.918779, test_loss:0.010328, test_acc:0.874089\n",
      "->epoch:77, train_loss:0.006129, train_acc:0.918287, test_loss:0.011425, test_acc:0.872526\n",
      "->epoch:78, train_loss:0.006146, train_acc:0.919242, test_loss:0.011632, test_acc:0.863802\n",
      "->epoch:79, train_loss:0.006100, train_acc:0.919285, test_loss:0.011294, test_acc:0.862891\n",
      "->epoch:80, train_loss:0.006039, train_acc:0.921079, test_loss:0.011383, test_acc:0.864844\n",
      "->epoch:81, train_loss:0.006058, train_acc:0.919141, test_loss:0.010931, test_acc:0.865234\n",
      "->epoch:82, train_loss:0.006111, train_acc:0.919155, test_loss:0.010975, test_acc:0.861458\n",
      "->epoch:83, train_loss:0.006030, train_acc:0.921137, test_loss:0.011103, test_acc:0.866927\n",
      "->epoch:84, train_loss:0.006032, train_acc:0.920616, test_loss:0.010836, test_acc:0.866016\n",
      "->epoch:85, train_loss:0.006076, train_acc:0.919661, test_loss:0.010995, test_acc:0.866016\n",
      "->epoch:86, train_loss:0.005996, train_acc:0.920341, test_loss:0.012553, test_acc:0.863542\n",
      "->epoch:87, train_loss:0.005994, train_acc:0.921050, test_loss:0.010518, test_acc:0.876563\n",
      "->epoch:88, train_loss:0.005987, train_acc:0.921224, test_loss:0.010455, test_acc:0.873828\n",
      "->epoch:89, train_loss:0.006070, train_acc:0.920645, test_loss:0.011525, test_acc:0.866276\n",
      "->epoch:90, train_loss:0.005878, train_acc:0.923032, test_loss:0.010997, test_acc:0.867839\n",
      "->epoch:91, train_loss:0.005891, train_acc:0.922772, test_loss:0.010582, test_acc:0.869271\n",
      "->epoch:92, train_loss:0.005967, train_acc:0.920385, test_loss:0.011837, test_acc:0.859635\n",
      "->epoch:93, train_loss:0.005928, train_acc:0.921238, test_loss:0.011021, test_acc:0.870052\n",
      "->epoch:94, train_loss:0.005917, train_acc:0.922468, test_loss:0.011559, test_acc:0.861198\n",
      "->epoch:95, train_loss:0.005812, train_acc:0.923857, test_loss:0.010932, test_acc:0.868880\n",
      "->epoch:96, train_loss:0.005858, train_acc:0.923770, test_loss:0.011124, test_acc:0.875781\n",
      "->epoch:97, train_loss:0.005929, train_acc:0.921586, test_loss:0.011789, test_acc:0.859635\n",
      "->epoch:98, train_loss:0.005894, train_acc:0.922815, test_loss:0.011423, test_acc:0.866016\n",
      "->epoch:99, train_loss:0.005877, train_acc:0.922975, test_loss:0.011393, test_acc:0.866146\n",
      "->epoch:100, train_loss:0.005787, train_acc:0.924595, test_loss:0.010544, test_acc:0.874870\n",
      "->epoch:101, train_loss:0.005706, train_acc:0.925593, test_loss:0.010528, test_acc:0.873047\n",
      "->epoch:102, train_loss:0.005702, train_acc:0.925347, test_loss:0.010420, test_acc:0.876693\n",
      "->epoch:103, train_loss:0.005842, train_acc:0.923380, test_loss:0.010264, test_acc:0.876432\n",
      "->epoch:104, train_loss:0.005723, train_acc:0.924884, test_loss:0.010958, test_acc:0.869141\n",
      "->epoch:105, train_loss:0.005725, train_acc:0.925362, test_loss:0.011497, test_acc:0.868750\n",
      "->epoch:106, train_loss:0.005808, train_acc:0.923626, test_loss:0.011045, test_acc:0.867969\n",
      "->epoch:107, train_loss:0.005736, train_acc:0.924971, test_loss:0.010592, test_acc:0.871484\n",
      "->epoch:108, train_loss:0.005740, train_acc:0.924653, test_loss:0.010794, test_acc:0.864974\n",
      "->epoch:109, train_loss:0.005673, train_acc:0.925723, test_loss:0.011248, test_acc:0.866276\n",
      "->epoch:110, train_loss:0.005686, train_acc:0.925579, test_loss:0.011195, test_acc:0.869401\n",
      "->epoch:111, train_loss:0.005555, train_acc:0.927648, test_loss:0.011312, test_acc:0.872005\n",
      "->epoch:112, train_loss:0.005619, train_acc:0.925810, test_loss:0.011404, test_acc:0.866146\n",
      "->epoch:113, train_loss:0.005686, train_acc:0.925579, test_loss:0.011379, test_acc:0.869141\n",
      "->epoch:114, train_loss:0.005639, train_acc:0.926794, test_loss:0.010324, test_acc:0.872917\n",
      "->epoch:115, train_loss:0.005664, train_acc:0.926128, test_loss:0.010775, test_acc:0.877083\n",
      "->epoch:116, train_loss:0.005567, train_acc:0.926649, test_loss:0.011533, test_acc:0.874219\n",
      "->epoch:117, train_loss:0.005617, train_acc:0.926505, test_loss:0.011452, test_acc:0.873177\n",
      "->epoch:118, train_loss:0.005501, train_acc:0.928791, test_loss:0.011971, test_acc:0.864193\n",
      "->epoch:119, train_loss:0.005551, train_acc:0.928038, test_loss:0.010894, test_acc:0.870833\n",
      "->epoch:120, train_loss:0.005636, train_acc:0.926722, test_loss:0.010619, test_acc:0.876823\n",
      "->epoch:121, train_loss:0.005545, train_acc:0.928038, test_loss:0.010596, test_acc:0.876432\n",
      "->epoch:122, train_loss:0.005694, train_acc:0.925550, test_loss:0.010595, test_acc:0.871484\n",
      "->epoch:123, train_loss:0.005549, train_acc:0.927575, test_loss:0.011691, test_acc:0.865234\n",
      "->epoch:124, train_loss:0.005554, train_acc:0.927373, test_loss:0.010738, test_acc:0.872266\n",
      "->epoch:125, train_loss:0.005516, train_acc:0.927648, test_loss:0.011276, test_acc:0.873828\n",
      "->epoch:126, train_loss:0.005551, train_acc:0.927937, test_loss:0.010892, test_acc:0.872656\n",
      "->epoch:127, train_loss:0.005487, train_acc:0.927908, test_loss:0.011055, test_acc:0.869401\n",
      "->epoch:128, train_loss:0.005571, train_acc:0.927257, test_loss:0.010636, test_acc:0.873958\n",
      "->epoch:129, train_loss:0.005501, train_acc:0.927488, test_loss:0.010831, test_acc:0.870964\n",
      "->epoch:130, train_loss:0.005409, train_acc:0.929355, test_loss:0.010648, test_acc:0.876693\n",
      "->epoch:131, train_loss:0.005654, train_acc:0.926924, test_loss:0.010954, test_acc:0.875911\n",
      "->epoch:132, train_loss:0.005403, train_acc:0.928747, test_loss:0.011385, test_acc:0.871094\n",
      "->epoch:133, train_loss:0.005521, train_acc:0.927792, test_loss:0.010319, test_acc:0.872526\n",
      "->epoch:134, train_loss:0.005457, train_acc:0.929022, test_loss:0.011249, test_acc:0.873047\n",
      "->epoch:135, train_loss:0.005569, train_acc:0.927807, test_loss:0.010230, test_acc:0.876302\n",
      "->epoch:136, train_loss:0.005445, train_acc:0.928212, test_loss:0.011034, test_acc:0.875391\n",
      "->epoch:137, train_loss:0.005358, train_acc:0.931033, test_loss:0.010414, test_acc:0.872396\n",
      "->epoch:138, train_loss:0.005384, train_acc:0.929962, test_loss:0.010912, test_acc:0.873307\n",
      "->epoch:139, train_loss:0.005411, train_acc:0.929774, test_loss:0.011516, test_acc:0.867318\n",
      "->epoch:140, train_loss:0.005364, train_acc:0.930122, test_loss:0.010717, test_acc:0.871224\n",
      "->epoch:141, train_loss:0.005409, train_acc:0.929355, test_loss:0.010668, test_acc:0.876693\n",
      "->epoch:142, train_loss:0.005454, train_acc:0.930541, test_loss:0.011302, test_acc:0.874219\n",
      "->epoch:143, train_loss:0.005404, train_acc:0.929572, test_loss:0.012112, test_acc:0.876563\n",
      "->epoch:144, train_loss:0.005306, train_acc:0.931221, test_loss:0.010536, test_acc:0.876042\n",
      "->epoch:145, train_loss:0.005360, train_acc:0.930035, test_loss:0.011110, test_acc:0.870573\n",
      "->epoch:146, train_loss:0.005454, train_acc:0.928718, test_loss:0.011116, test_acc:0.875260\n",
      "->epoch:147, train_loss:0.005392, train_acc:0.929905, test_loss:0.011058, test_acc:0.874609\n",
      "->epoch:148, train_loss:0.005343, train_acc:0.930512, test_loss:0.011044, test_acc:0.872656\n",
      "->epoch:149, train_loss:0.005318, train_acc:0.932089, test_loss:0.012634, test_acc:0.868750\n",
      "->epoch:150, train_loss:0.005340, train_acc:0.931019, test_loss:0.010921, test_acc:0.875391\n",
      "->epoch:151, train_loss:0.005300, train_acc:0.930961, test_loss:0.010675, test_acc:0.872266\n",
      "->epoch:152, train_loss:0.005270, train_acc:0.932147, test_loss:0.011098, test_acc:0.876042\n",
      "->epoch:153, train_loss:0.005284, train_acc:0.932060, test_loss:0.010801, test_acc:0.878776\n",
      "->epoch:154, train_loss:0.005340, train_acc:0.931091, test_loss:0.010649, test_acc:0.876953\n",
      "->epoch:155, train_loss:0.005327, train_acc:0.931568, test_loss:0.011062, test_acc:0.874740\n",
      "->epoch:156, train_loss:0.005405, train_acc:0.931033, test_loss:0.010893, test_acc:0.876953\n",
      "->epoch:157, train_loss:0.005289, train_acc:0.931539, test_loss:0.010962, test_acc:0.875781\n",
      "->epoch:158, train_loss:0.005270, train_acc:0.932248, test_loss:0.011108, test_acc:0.873177\n",
      "->epoch:159, train_loss:0.005311, train_acc:0.931250, test_loss:0.010680, test_acc:0.878646\n",
      "->epoch:160, train_loss:0.005234, train_acc:0.932581, test_loss:0.010881, test_acc:0.875651\n",
      "->epoch:161, train_loss:0.005232, train_acc:0.932928, test_loss:0.011684, test_acc:0.867708\n",
      "->epoch:162, train_loss:0.005206, train_acc:0.932740, test_loss:0.011231, test_acc:0.874349\n",
      "->epoch:163, train_loss:0.005227, train_acc:0.932378, test_loss:0.011412, test_acc:0.871224\n",
      "->epoch:164, train_loss:0.005245, train_acc:0.932451, test_loss:0.011279, test_acc:0.872005\n",
      "->epoch:165, train_loss:0.005254, train_acc:0.932480, test_loss:0.010779, test_acc:0.874870\n",
      "->epoch:166, train_loss:0.005171, train_acc:0.932798, test_loss:0.011198, test_acc:0.875391\n",
      "->epoch:167, train_loss:0.005212, train_acc:0.933304, test_loss:0.011247, test_acc:0.874740\n",
      "->epoch:168, train_loss:0.005175, train_acc:0.932986, test_loss:0.010433, test_acc:0.877995\n",
      "->epoch:169, train_loss:0.005285, train_acc:0.931742, test_loss:0.012115, test_acc:0.871224\n",
      "->epoch:170, train_loss:0.005210, train_acc:0.933189, test_loss:0.010471, test_acc:0.876693\n",
      "->epoch:171, train_loss:0.005199, train_acc:0.932436, test_loss:0.011059, test_acc:0.878906\n",
      "->epoch:172, train_loss:0.005173, train_acc:0.933406, test_loss:0.011335, test_acc:0.874479\n",
      "->epoch:173, train_loss:0.005216, train_acc:0.932190, test_loss:0.011455, test_acc:0.876563\n",
      "->epoch:174, train_loss:0.005178, train_acc:0.933275, test_loss:0.010866, test_acc:0.874870\n",
      "->epoch:175, train_loss:0.005102, train_acc:0.935185, test_loss:0.011332, test_acc:0.873438\n",
      "->epoch:176, train_loss:0.005274, train_acc:0.931916, test_loss:0.010483, test_acc:0.883984\n",
      "->epoch:177, train_loss:0.005200, train_acc:0.933015, test_loss:0.011447, test_acc:0.877214\n",
      "->epoch:178, train_loss:0.005136, train_acc:0.934129, test_loss:0.011418, test_acc:0.871224\n",
      "->epoch:179, train_loss:0.005121, train_acc:0.934751, test_loss:0.010593, test_acc:0.880339\n",
      "->epoch:180, train_loss:0.005151, train_acc:0.933738, test_loss:0.012077, test_acc:0.864453\n",
      "->epoch:181, train_loss:0.005146, train_acc:0.933594, test_loss:0.010130, test_acc:0.880078\n",
      "->epoch:182, train_loss:0.004989, train_acc:0.935026, test_loss:0.011269, test_acc:0.879036\n",
      "->epoch:183, train_loss:0.005166, train_acc:0.933246, test_loss:0.011672, test_acc:0.878646\n",
      "->epoch:184, train_loss:0.005094, train_acc:0.934650, test_loss:0.011004, test_acc:0.871224\n",
      "->epoch:185, train_loss:0.005077, train_acc:0.934158, test_loss:0.011519, test_acc:0.876953\n",
      "->epoch:186, train_loss:0.005170, train_acc:0.932841, test_loss:0.010536, test_acc:0.877865\n",
      "->epoch:187, train_loss:0.005086, train_acc:0.934375, test_loss:0.010907, test_acc:0.875911\n",
      "->epoch:188, train_loss:0.005109, train_acc:0.934100, test_loss:0.010619, test_acc:0.880599\n",
      "->epoch:189, train_loss:0.005012, train_acc:0.934592, test_loss:0.011528, test_acc:0.873047\n",
      "->epoch:190, train_loss:0.005061, train_acc:0.934303, test_loss:0.010950, test_acc:0.876302\n",
      "->epoch:191, train_loss:0.005076, train_acc:0.935373, test_loss:0.011272, test_acc:0.877734\n",
      "->epoch:192, train_loss:0.005123, train_acc:0.934795, test_loss:0.011158, test_acc:0.874349\n",
      "->epoch:193, train_loss:0.005101, train_acc:0.933840, test_loss:0.011910, test_acc:0.874740\n",
      "->epoch:194, train_loss:0.005134, train_acc:0.934288, test_loss:0.011303, test_acc:0.881120\n",
      "->epoch:195, train_loss:0.005007, train_acc:0.935894, test_loss:0.012621, test_acc:0.869401\n",
      "->epoch:196, train_loss:0.005094, train_acc:0.933840, test_loss:0.010557, test_acc:0.884635\n",
      "->epoch:197, train_loss:0.004940, train_acc:0.937558, test_loss:0.011356, test_acc:0.871484\n",
      "->epoch:198, train_loss:0.005115, train_acc:0.934071, test_loss:0.011262, test_acc:0.873828\n",
      "->epoch:199, train_loss:0.005014, train_acc:0.936227, test_loss:0.010829, test_acc:0.878646\n",
      "->epoch:200, train_loss:0.004991, train_acc:0.936675, test_loss:0.011144, test_acc:0.878125\n",
      "->epoch:201, train_loss:0.004997, train_acc:0.935315, test_loss:0.010668, test_acc:0.869922\n",
      "->epoch:202, train_loss:0.005020, train_acc:0.936227, test_loss:0.011012, test_acc:0.874870\n",
      "->epoch:203, train_loss:0.005039, train_acc:0.935388, test_loss:0.010765, test_acc:0.872135\n",
      "->epoch:204, train_loss:0.005141, train_acc:0.933521, test_loss:0.011189, test_acc:0.874609\n",
      "->epoch:205, train_loss:0.005052, train_acc:0.935200, test_loss:0.010643, test_acc:0.874349\n",
      "->epoch:206, train_loss:0.005073, train_acc:0.935720, test_loss:0.012245, test_acc:0.864193\n",
      "->epoch:207, train_loss:0.004996, train_acc:0.935966, test_loss:0.011027, test_acc:0.884635\n",
      "->epoch:208, train_loss:0.005018, train_acc:0.935272, test_loss:0.010390, test_acc:0.883464\n",
      "->epoch:209, train_loss:0.005042, train_acc:0.935807, test_loss:0.012047, test_acc:0.871745\n",
      "->epoch:210, train_loss:0.005073, train_acc:0.934389, test_loss:0.010915, test_acc:0.875130\n",
      "->epoch:211, train_loss:0.004922, train_acc:0.936921, test_loss:0.010701, test_acc:0.881771\n",
      "->epoch:212, train_loss:0.005022, train_acc:0.936357, test_loss:0.010787, test_acc:0.877083\n",
      "->epoch:213, train_loss:0.004902, train_acc:0.936444, test_loss:0.010459, test_acc:0.880339\n",
      "->epoch:214, train_loss:0.004912, train_acc:0.936097, test_loss:0.011011, test_acc:0.876693\n",
      "->epoch:215, train_loss:0.004928, train_acc:0.937370, test_loss:0.013408, test_acc:0.874349\n",
      "->epoch:216, train_loss:0.005004, train_acc:0.935156, test_loss:0.010732, test_acc:0.874870\n",
      "->epoch:217, train_loss:0.004987, train_acc:0.935344, test_loss:0.011170, test_acc:0.872917\n",
      "->epoch:218, train_loss:0.004954, train_acc:0.935648, test_loss:0.011392, test_acc:0.875130\n",
      "->epoch:219, train_loss:0.004998, train_acc:0.936733, test_loss:0.011000, test_acc:0.884635\n",
      "->epoch:220, train_loss:0.004926, train_acc:0.936892, test_loss:0.011975, test_acc:0.874349\n",
      "->epoch:221, train_loss:0.004860, train_acc:0.938281, test_loss:0.011129, test_acc:0.872526\n",
      "->epoch:222, train_loss:0.005010, train_acc:0.935402, test_loss:0.010961, test_acc:0.877474\n",
      "->epoch:223, train_loss:0.004994, train_acc:0.934780, test_loss:0.011607, test_acc:0.875911\n",
      "->epoch:224, train_loss:0.004994, train_acc:0.936111, test_loss:0.011061, test_acc:0.876172\n",
      "->epoch:225, train_loss:0.004918, train_acc:0.937109, test_loss:0.012179, test_acc:0.876823\n",
      "->epoch:226, train_loss:0.004990, train_acc:0.935229, test_loss:0.011253, test_acc:0.874219\n",
      "->epoch:227, train_loss:0.004869, train_acc:0.937558, test_loss:0.011806, test_acc:0.872526\n",
      "->epoch:228, train_loss:0.004822, train_acc:0.937558, test_loss:0.010409, test_acc:0.879167\n",
      "->epoch:229, train_loss:0.004865, train_acc:0.936603, test_loss:0.010633, test_acc:0.875651\n",
      "->epoch:230, train_loss:0.004992, train_acc:0.935590, test_loss:0.010444, test_acc:0.879688\n",
      "->epoch:231, train_loss:0.004826, train_acc:0.937717, test_loss:0.010871, test_acc:0.879948\n",
      "->epoch:232, train_loss:0.004905, train_acc:0.937182, test_loss:0.012291, test_acc:0.874219\n",
      "->epoch:233, train_loss:0.004939, train_acc:0.936198, test_loss:0.011376, test_acc:0.875521\n",
      "->epoch:234, train_loss:0.004934, train_acc:0.937051, test_loss:0.011145, test_acc:0.880339\n",
      "->epoch:235, train_loss:0.004895, train_acc:0.937037, test_loss:0.011199, test_acc:0.879036\n",
      "->epoch:236, train_loss:0.004917, train_acc:0.937196, test_loss:0.011025, test_acc:0.876953\n",
      "->epoch:237, train_loss:0.004996, train_acc:0.935663, test_loss:0.010696, test_acc:0.878516\n",
      "->epoch:238, train_loss:0.004807, train_acc:0.938426, test_loss:0.010871, test_acc:0.880599\n",
      "->epoch:239, train_loss:0.004912, train_acc:0.937442, test_loss:0.011402, test_acc:0.877344\n",
      "->epoch:240, train_loss:0.004850, train_acc:0.937963, test_loss:0.011606, test_acc:0.880339\n",
      "->epoch:241, train_loss:0.004821, train_acc:0.938137, test_loss:0.010287, test_acc:0.881120\n",
      "->epoch:242, train_loss:0.004783, train_acc:0.939294, test_loss:0.011116, test_acc:0.878906\n",
      "->epoch:243, train_loss:0.004938, train_acc:0.937095, test_loss:0.011742, test_acc:0.873047\n",
      "->epoch:244, train_loss:0.004949, train_acc:0.937529, test_loss:0.011399, test_acc:0.877865\n",
      "->epoch:245, train_loss:0.004760, train_acc:0.939612, test_loss:0.010660, test_acc:0.883333\n",
      "->epoch:246, train_loss:0.004804, train_acc:0.938542, test_loss:0.011012, test_acc:0.885156\n",
      "->epoch:247, train_loss:0.004955, train_acc:0.937153, test_loss:0.011294, test_acc:0.872526\n",
      "->epoch:248, train_loss:0.004810, train_acc:0.938455, test_loss:0.010946, test_acc:0.870703\n",
      "->epoch:249, train_loss:0.004839, train_acc:0.938064, test_loss:0.011252, test_acc:0.877995\n",
      "->epoch:250, train_loss:0.004882, train_acc:0.936661, test_loss:0.011376, test_acc:0.876693\n",
      "->epoch:251, train_loss:0.004835, train_acc:0.937659, test_loss:0.009920, test_acc:0.877865\n",
      "->epoch:252, train_loss:0.004833, train_acc:0.938643, test_loss:0.010379, test_acc:0.879948\n",
      "->epoch:253, train_loss:0.004880, train_acc:0.937948, test_loss:0.011169, test_acc:0.875911\n",
      "->epoch:254, train_loss:0.004766, train_acc:0.939337, test_loss:0.012009, test_acc:0.872396\n",
      "->epoch:255, train_loss:0.004924, train_acc:0.936574, test_loss:0.011710, test_acc:0.878125\n",
      "->epoch:256, train_loss:0.004840, train_acc:0.938831, test_loss:0.011285, test_acc:0.880469\n",
      "->epoch:257, train_loss:0.004867, train_acc:0.938165, test_loss:0.010955, test_acc:0.880599\n",
      "->epoch:258, train_loss:0.004812, train_acc:0.938368, test_loss:0.010709, test_acc:0.883203\n",
      "->epoch:259, train_loss:0.004787, train_acc:0.938296, test_loss:0.011790, test_acc:0.877344\n",
      "->epoch:260, train_loss:0.004750, train_acc:0.938874, test_loss:0.011449, test_acc:0.880469\n",
      "->epoch:261, train_loss:0.004768, train_acc:0.939641, test_loss:0.011432, test_acc:0.877865\n",
      "->epoch:262, train_loss:0.004852, train_acc:0.938021, test_loss:0.010466, test_acc:0.877344\n",
      "->epoch:263, train_loss:0.004871, train_acc:0.937486, test_loss:0.011076, test_acc:0.878906\n",
      "->epoch:264, train_loss:0.004775, train_acc:0.938860, test_loss:0.010623, test_acc:0.881901\n",
      "->epoch:265, train_loss:0.004730, train_acc:0.939641, test_loss:0.011155, test_acc:0.876693\n",
      "->epoch:266, train_loss:0.004756, train_acc:0.938600, test_loss:0.011194, test_acc:0.879818\n",
      "->epoch:267, train_loss:0.004830, train_acc:0.937818, test_loss:0.010532, test_acc:0.881250\n",
      "->epoch:268, train_loss:0.004755, train_acc:0.939728, test_loss:0.011100, test_acc:0.876042\n",
      "->epoch:269, train_loss:0.004815, train_acc:0.938397, test_loss:0.010962, test_acc:0.884505\n",
      "->epoch:270, train_loss:0.004731, train_acc:0.939815, test_loss:0.011203, test_acc:0.873958\n",
      "->epoch:271, train_loss:0.004791, train_acc:0.939251, test_loss:0.011425, test_acc:0.873828\n",
      "->epoch:272, train_loss:0.004807, train_acc:0.938527, test_loss:0.011402, test_acc:0.878385\n",
      "->epoch:273, train_loss:0.004672, train_acc:0.941016, test_loss:0.011343, test_acc:0.877734\n",
      "->epoch:274, train_loss:0.004714, train_acc:0.940075, test_loss:0.010867, test_acc:0.877865\n",
      "->epoch:275, train_loss:0.004754, train_acc:0.939627, test_loss:0.010880, test_acc:0.879818\n",
      "->epoch:276, train_loss:0.004860, train_acc:0.938527, test_loss:0.011256, test_acc:0.878125\n",
      "->epoch:277, train_loss:0.004770, train_acc:0.939656, test_loss:0.010915, test_acc:0.878776\n",
      "->epoch:278, train_loss:0.004889, train_acc:0.937760, test_loss:0.011228, test_acc:0.875651\n",
      "->epoch:279, train_loss:0.004764, train_acc:0.939034, test_loss:0.010958, test_acc:0.886328\n",
      "->epoch:280, train_loss:0.004710, train_acc:0.939858, test_loss:0.011276, test_acc:0.875651\n",
      "->epoch:281, train_loss:0.004785, train_acc:0.939511, test_loss:0.011424, test_acc:0.878776\n",
      "->epoch:282, train_loss:0.004767, train_acc:0.939742, test_loss:0.010914, test_acc:0.884245\n",
      "->epoch:283, train_loss:0.004741, train_acc:0.939598, test_loss:0.010631, test_acc:0.876953\n",
      "->epoch:284, train_loss:0.004788, train_acc:0.940321, test_loss:0.011244, test_acc:0.877734\n",
      "->epoch:285, train_loss:0.004697, train_acc:0.940148, test_loss:0.010981, test_acc:0.877734\n",
      "->epoch:286, train_loss:0.004771, train_acc:0.939410, test_loss:0.010951, test_acc:0.878385\n",
      "->epoch:287, train_loss:0.004703, train_acc:0.940437, test_loss:0.010662, test_acc:0.879167\n",
      "->epoch:288, train_loss:0.004746, train_acc:0.939251, test_loss:0.010648, test_acc:0.883724\n",
      "->epoch:289, train_loss:0.004700, train_acc:0.939424, test_loss:0.010972, test_acc:0.877995\n",
      "->epoch:290, train_loss:0.004696, train_acc:0.940625, test_loss:0.010688, test_acc:0.883594\n",
      "->epoch:291, train_loss:0.004722, train_acc:0.938137, test_loss:0.010806, test_acc:0.884505\n",
      "->epoch:292, train_loss:0.004723, train_acc:0.939627, test_loss:0.010203, test_acc:0.884635\n",
      "->epoch:293, train_loss:0.004779, train_acc:0.938151, test_loss:0.010911, test_acc:0.873958\n",
      "->epoch:294, train_loss:0.004711, train_acc:0.939887, test_loss:0.011230, test_acc:0.874219\n",
      "->epoch:295, train_loss:0.004770, train_acc:0.939149, test_loss:0.011066, test_acc:0.877604\n",
      "->epoch:296, train_loss:0.004739, train_acc:0.939193, test_loss:0.010693, test_acc:0.878516\n",
      "->epoch:297, train_loss:0.004769, train_acc:0.939062, test_loss:0.010717, test_acc:0.878255\n",
      "->epoch:298, train_loss:0.004709, train_acc:0.939771, test_loss:0.010587, test_acc:0.885156\n",
      "->epoch:299, train_loss:0.004699, train_acc:0.940075, test_loss:0.010463, test_acc:0.880469\n",
      "->epoch:300, train_loss:0.004677, train_acc:0.939612, test_loss:0.011474, test_acc:0.880208\n",
      "->epoch:301, train_loss:0.004721, train_acc:0.940046, test_loss:0.011052, test_acc:0.885156\n",
      "->epoch:302, train_loss:0.004650, train_acc:0.939931, test_loss:0.011372, test_acc:0.877083\n",
      "->epoch:303, train_loss:0.004676, train_acc:0.939815, test_loss:0.012189, test_acc:0.875781\n",
      "->epoch:304, train_loss:0.004686, train_acc:0.940726, test_loss:0.010130, test_acc:0.879948\n",
      "->epoch:305, train_loss:0.004643, train_acc:0.939815, test_loss:0.011497, test_acc:0.877083\n",
      "->epoch:306, train_loss:0.004698, train_acc:0.939974, test_loss:0.010758, test_acc:0.878906\n",
      "->epoch:307, train_loss:0.004633, train_acc:0.941725, test_loss:0.010073, test_acc:0.883333\n",
      "->epoch:308, train_loss:0.004700, train_acc:0.939525, test_loss:0.010559, test_acc:0.882031\n",
      "->epoch:309, train_loss:0.004635, train_acc:0.942173, test_loss:0.010908, test_acc:0.880339\n",
      "->epoch:310, train_loss:0.004686, train_acc:0.940191, test_loss:0.010412, test_acc:0.881771\n",
      "->epoch:311, train_loss:0.004639, train_acc:0.940900, test_loss:0.010504, test_acc:0.882292\n",
      "->epoch:312, train_loss:0.004633, train_acc:0.942187, test_loss:0.011170, test_acc:0.886198\n",
      "->epoch:313, train_loss:0.004718, train_acc:0.939714, test_loss:0.011388, test_acc:0.879297\n",
      "->epoch:314, train_loss:0.004687, train_acc:0.940437, test_loss:0.010547, test_acc:0.881901\n",
      "->epoch:315, train_loss:0.004584, train_acc:0.940972, test_loss:0.011483, test_acc:0.876693\n",
      "->epoch:316, train_loss:0.004666, train_acc:0.940582, test_loss:0.011128, test_acc:0.886068\n",
      "->epoch:317, train_loss:0.004593, train_acc:0.941768, test_loss:0.010363, test_acc:0.880990\n",
      "->epoch:318, train_loss:0.004634, train_acc:0.940842, test_loss:0.011192, test_acc:0.879948\n",
      "->epoch:319, train_loss:0.004622, train_acc:0.940856, test_loss:0.011008, test_acc:0.874219\n",
      "->epoch:320, train_loss:0.004607, train_acc:0.941175, test_loss:0.010840, test_acc:0.875391\n",
      "->epoch:321, train_loss:0.004544, train_acc:0.941869, test_loss:0.011029, test_acc:0.879297\n",
      "->epoch:322, train_loss:0.004659, train_acc:0.940278, test_loss:0.010636, test_acc:0.881641\n",
      "->epoch:323, train_loss:0.004559, train_acc:0.942130, test_loss:0.011140, test_acc:0.880208\n",
      "->epoch:324, train_loss:0.004604, train_acc:0.941826, test_loss:0.012414, test_acc:0.876823\n",
      "->epoch:325, train_loss:0.004664, train_acc:0.940914, test_loss:0.011421, test_acc:0.879948\n",
      "->epoch:326, train_loss:0.004535, train_acc:0.940900, test_loss:0.010936, test_acc:0.880729\n",
      "->epoch:327, train_loss:0.004492, train_acc:0.942679, test_loss:0.011372, test_acc:0.875781\n",
      "->epoch:328, train_loss:0.004662, train_acc:0.941638, test_loss:0.011333, test_acc:0.877214\n",
      "->epoch:329, train_loss:0.004644, train_acc:0.940596, test_loss:0.010557, test_acc:0.879818\n",
      "->epoch:330, train_loss:0.004598, train_acc:0.941290, test_loss:0.010969, test_acc:0.884896\n",
      "->epoch:331, train_loss:0.004676, train_acc:0.940828, test_loss:0.011433, test_acc:0.880859\n",
      "->epoch:332, train_loss:0.004588, train_acc:0.941638, test_loss:0.010851, test_acc:0.879427\n",
      "->epoch:333, train_loss:0.004521, train_acc:0.942564, test_loss:0.010437, test_acc:0.883073\n",
      "->epoch:334, train_loss:0.004620, train_acc:0.941696, test_loss:0.010805, test_acc:0.880990\n",
      "->epoch:335, train_loss:0.004552, train_acc:0.941493, test_loss:0.011006, test_acc:0.881901\n",
      "->epoch:336, train_loss:0.004532, train_acc:0.942665, test_loss:0.010842, test_acc:0.883724\n",
      "->epoch:337, train_loss:0.004592, train_acc:0.941030, test_loss:0.011055, test_acc:0.879427\n",
      "->epoch:338, train_loss:0.004583, train_acc:0.941247, test_loss:0.011244, test_acc:0.878516\n",
      "->epoch:339, train_loss:0.004574, train_acc:0.941522, test_loss:0.011233, test_acc:0.880208\n",
      "->epoch:340, train_loss:0.004617, train_acc:0.941189, test_loss:0.010870, test_acc:0.880078\n",
      "->epoch:341, train_loss:0.004614, train_acc:0.942419, test_loss:0.011234, test_acc:0.885156\n",
      "->epoch:342, train_loss:0.004629, train_acc:0.941840, test_loss:0.010913, test_acc:0.878906\n",
      "->epoch:343, train_loss:0.004557, train_acc:0.941811, test_loss:0.010624, test_acc:0.888151\n",
      "->epoch:344, train_loss:0.004507, train_acc:0.942216, test_loss:0.012630, test_acc:0.872787\n",
      "->epoch:345, train_loss:0.004580, train_acc:0.941131, test_loss:0.011001, test_acc:0.884766\n",
      "->epoch:346, train_loss:0.004528, train_acc:0.943244, test_loss:0.011876, test_acc:0.881641\n",
      "->epoch:347, train_loss:0.004588, train_acc:0.941392, test_loss:0.010840, test_acc:0.881771\n",
      "->epoch:348, train_loss:0.004580, train_acc:0.941536, test_loss:0.011754, test_acc:0.883594\n",
      "->epoch:349, train_loss:0.004649, train_acc:0.941406, test_loss:0.010789, test_acc:0.882813\n",
      "->epoch:350, train_loss:0.004611, train_acc:0.941753, test_loss:0.010394, test_acc:0.882292\n",
      "->epoch:351, train_loss:0.004563, train_acc:0.942622, test_loss:0.010584, test_acc:0.880990\n",
      "->epoch:352, train_loss:0.004573, train_acc:0.942853, test_loss:0.010567, test_acc:0.881120\n",
      "->epoch:353, train_loss:0.004511, train_acc:0.943142, test_loss:0.010778, test_acc:0.883464\n",
      "->epoch:354, train_loss:0.004528, train_acc:0.942390, test_loss:0.011254, test_acc:0.878776\n",
      "->epoch:355, train_loss:0.004570, train_acc:0.941160, test_loss:0.010573, test_acc:0.878255\n",
      "->epoch:356, train_loss:0.004567, train_acc:0.941999, test_loss:0.011275, test_acc:0.877865\n",
      "->epoch:357, train_loss:0.004567, train_acc:0.942101, test_loss:0.011082, test_acc:0.879818\n",
      "->epoch:358, train_loss:0.004544, train_acc:0.942766, test_loss:0.010998, test_acc:0.877604\n",
      "->epoch:359, train_loss:0.004517, train_acc:0.942245, test_loss:0.011557, test_acc:0.876693\n",
      "->epoch:360, train_loss:0.004521, train_acc:0.941623, test_loss:0.011651, test_acc:0.872266\n",
      "->epoch:361, train_loss:0.004577, train_acc:0.943171, test_loss:0.011794, test_acc:0.874870\n",
      "->epoch:362, train_loss:0.004641, train_acc:0.941869, test_loss:0.011605, test_acc:0.881380\n",
      "->epoch:363, train_loss:0.004515, train_acc:0.942694, test_loss:0.011152, test_acc:0.877734\n",
      "->epoch:364, train_loss:0.004503, train_acc:0.943316, test_loss:0.010816, test_acc:0.883854\n",
      "->epoch:365, train_loss:0.004485, train_acc:0.942708, test_loss:0.011318, test_acc:0.878776\n",
      "->epoch:366, train_loss:0.004515, train_acc:0.942925, test_loss:0.010917, test_acc:0.879948\n",
      "->epoch:367, train_loss:0.004603, train_acc:0.941710, test_loss:0.011444, test_acc:0.883203\n",
      "->epoch:368, train_loss:0.004479, train_acc:0.943186, test_loss:0.011467, test_acc:0.882682\n",
      "->epoch:369, train_loss:0.004564, train_acc:0.941392, test_loss:0.011120, test_acc:0.882552\n",
      "->epoch:370, train_loss:0.004509, train_acc:0.943519, test_loss:0.010636, test_acc:0.889063\n",
      "->epoch:371, train_loss:0.004485, train_acc:0.943301, test_loss:0.011117, test_acc:0.881510\n",
      "->epoch:372, train_loss:0.004444, train_acc:0.944748, test_loss:0.011094, test_acc:0.881901\n",
      "->epoch:373, train_loss:0.004584, train_acc:0.942839, test_loss:0.010415, test_acc:0.883854\n",
      "->epoch:374, train_loss:0.004536, train_acc:0.941725, test_loss:0.010612, test_acc:0.880339\n",
      "->epoch:375, train_loss:0.004606, train_acc:0.940929, test_loss:0.010618, test_acc:0.885807\n",
      "->epoch:376, train_loss:0.004498, train_acc:0.942535, test_loss:0.011313, test_acc:0.881771\n",
      "->epoch:377, train_loss:0.004456, train_acc:0.943301, test_loss:0.011226, test_acc:0.869922\n",
      "->epoch:378, train_loss:0.004524, train_acc:0.942665, test_loss:0.010708, test_acc:0.879688\n",
      "->epoch:379, train_loss:0.004532, train_acc:0.942925, test_loss:0.011305, test_acc:0.877083\n",
      "->epoch:380, train_loss:0.004526, train_acc:0.942462, test_loss:0.011435, test_acc:0.876563\n",
      "->epoch:381, train_loss:0.004498, train_acc:0.941855, test_loss:0.010846, test_acc:0.882161\n",
      "->epoch:382, train_loss:0.004485, train_acc:0.943099, test_loss:0.010412, test_acc:0.882552\n",
      "->epoch:383, train_loss:0.004511, train_acc:0.942810, test_loss:0.011215, test_acc:0.885807\n",
      "->epoch:384, train_loss:0.004511, train_acc:0.942057, test_loss:0.011249, test_acc:0.879297\n",
      "->epoch:385, train_loss:0.004489, train_acc:0.943128, test_loss:0.010742, test_acc:0.883984\n",
      "->epoch:386, train_loss:0.004445, train_acc:0.943663, test_loss:0.011873, test_acc:0.878776\n",
      "->epoch:387, train_loss:0.004537, train_acc:0.943576, test_loss:0.010651, test_acc:0.882031\n",
      "->epoch:388, train_loss:0.004464, train_acc:0.944112, test_loss:0.011293, test_acc:0.885156\n",
      "->epoch:389, train_loss:0.004441, train_acc:0.944010, test_loss:0.011477, test_acc:0.887240\n",
      "->epoch:390, train_loss:0.004463, train_acc:0.942622, test_loss:0.011435, test_acc:0.877474\n",
      "->epoch:391, train_loss:0.004500, train_acc:0.942810, test_loss:0.012404, test_acc:0.873438\n",
      "->epoch:392, train_loss:0.004481, train_acc:0.942766, test_loss:0.011023, test_acc:0.886719\n",
      "->epoch:393, train_loss:0.004554, train_acc:0.942115, test_loss:0.011745, test_acc:0.880469\n",
      "->epoch:394, train_loss:0.004436, train_acc:0.943113, test_loss:0.012032, test_acc:0.883594\n",
      "->epoch:395, train_loss:0.004510, train_acc:0.942636, test_loss:0.010755, test_acc:0.885156\n",
      "->epoch:396, train_loss:0.004523, train_acc:0.943301, test_loss:0.011419, test_acc:0.880990\n",
      "->epoch:397, train_loss:0.004463, train_acc:0.943084, test_loss:0.011500, test_acc:0.882813\n",
      "->epoch:398, train_loss:0.004498, train_acc:0.943244, test_loss:0.011152, test_acc:0.883984\n",
      "->epoch:399, train_loss:0.004474, train_acc:0.943056, test_loss:0.010346, test_acc:0.888151\n",
      "->epoch:400, train_loss:0.004557, train_acc:0.941580, test_loss:0.011104, test_acc:0.879818\n",
      "->epoch:401, train_loss:0.004431, train_acc:0.943417, test_loss:0.010714, test_acc:0.881250\n",
      "->epoch:402, train_loss:0.004454, train_acc:0.944068, test_loss:0.010404, test_acc:0.884115\n",
      "->epoch:403, train_loss:0.004516, train_acc:0.942882, test_loss:0.010578, test_acc:0.881641\n",
      "->epoch:404, train_loss:0.004519, train_acc:0.942867, test_loss:0.010585, test_acc:0.880859\n",
      "->epoch:405, train_loss:0.004494, train_acc:0.943519, test_loss:0.011764, test_acc:0.876823\n",
      "->epoch:406, train_loss:0.004448, train_acc:0.944039, test_loss:0.011192, test_acc:0.876042\n",
      "->epoch:407, train_loss:0.004492, train_acc:0.943576, test_loss:0.010394, test_acc:0.882161\n",
      "->epoch:408, train_loss:0.004407, train_acc:0.944560, test_loss:0.011608, test_acc:0.887109\n",
      "->epoch:409, train_loss:0.004414, train_acc:0.943576, test_loss:0.010996, test_acc:0.885026\n",
      "->epoch:410, train_loss:0.004489, train_acc:0.943895, test_loss:0.010977, test_acc:0.882161\n",
      "->epoch:411, train_loss:0.004507, train_acc:0.943084, test_loss:0.010523, test_acc:0.887109\n",
      "->epoch:412, train_loss:0.004485, train_acc:0.942824, test_loss:0.010851, test_acc:0.881250\n",
      "->epoch:413, train_loss:0.004456, train_acc:0.943822, test_loss:0.010605, test_acc:0.884115\n",
      "->epoch:414, train_loss:0.004380, train_acc:0.944777, test_loss:0.011951, test_acc:0.886719\n",
      "->epoch:415, train_loss:0.004443, train_acc:0.942564, test_loss:0.011634, test_acc:0.882813\n",
      "->epoch:416, train_loss:0.004398, train_acc:0.944126, test_loss:0.012343, test_acc:0.880208\n",
      "->epoch:417, train_loss:0.004504, train_acc:0.942723, test_loss:0.011839, test_acc:0.880078\n",
      "->epoch:418, train_loss:0.004497, train_acc:0.941811, test_loss:0.010887, test_acc:0.885026\n",
      "->epoch:419, train_loss:0.004421, train_acc:0.944502, test_loss:0.011002, test_acc:0.879297\n",
      "->epoch:420, train_loss:0.004505, train_acc:0.943171, test_loss:0.011355, test_acc:0.883984\n",
      "->epoch:421, train_loss:0.004432, train_acc:0.944763, test_loss:0.010851, test_acc:0.885417\n",
      "->epoch:422, train_loss:0.004447, train_acc:0.943316, test_loss:0.010804, test_acc:0.889714\n",
      "->epoch:423, train_loss:0.004419, train_acc:0.943547, test_loss:0.011444, test_acc:0.883724\n",
      "->epoch:424, train_loss:0.004514, train_acc:0.943099, test_loss:0.010572, test_acc:0.881771\n",
      "->epoch:425, train_loss:0.004443, train_acc:0.943244, test_loss:0.011685, test_acc:0.878125\n",
      "->epoch:426, train_loss:0.004467, train_acc:0.944676, test_loss:0.011713, test_acc:0.877604\n",
      "->epoch:427, train_loss:0.004412, train_acc:0.943533, test_loss:0.011570, test_acc:0.881120\n",
      "->epoch:428, train_loss:0.004495, train_acc:0.942245, test_loss:0.010844, test_acc:0.882031\n",
      "->epoch:429, train_loss:0.004433, train_acc:0.943432, test_loss:0.011616, test_acc:0.879818\n",
      "->epoch:430, train_loss:0.004420, train_acc:0.943301, test_loss:0.011107, test_acc:0.884635\n",
      "->epoch:431, train_loss:0.004435, train_acc:0.944517, test_loss:0.010471, test_acc:0.881380\n",
      "->epoch:432, train_loss:0.004433, train_acc:0.944271, test_loss:0.011215, test_acc:0.882682\n",
      "->epoch:433, train_loss:0.004354, train_acc:0.945124, test_loss:0.011226, test_acc:0.884896\n",
      "->epoch:434, train_loss:0.004324, train_acc:0.946094, test_loss:0.011320, test_acc:0.882943\n",
      "->epoch:435, train_loss:0.004356, train_acc:0.945226, test_loss:0.010606, test_acc:0.884766\n",
      "->epoch:436, train_loss:0.004408, train_acc:0.944835, test_loss:0.010822, test_acc:0.883333\n",
      "->epoch:437, train_loss:0.004498, train_acc:0.943287, test_loss:0.010460, test_acc:0.877214\n",
      "->epoch:438, train_loss:0.004397, train_acc:0.945501, test_loss:0.012440, test_acc:0.879688\n",
      "->epoch:439, train_loss:0.004466, train_acc:0.942940, test_loss:0.010977, test_acc:0.880078\n",
      "->epoch:440, train_loss:0.004356, train_acc:0.945153, test_loss:0.011379, test_acc:0.882292\n",
      "->epoch:441, train_loss:0.004458, train_acc:0.944444, test_loss:0.011307, test_acc:0.880078\n",
      "->epoch:442, train_loss:0.004307, train_acc:0.945240, test_loss:0.010887, test_acc:0.885677\n",
      "->epoch:443, train_loss:0.004306, train_acc:0.945312, test_loss:0.010994, test_acc:0.881901\n",
      "->epoch:444, train_loss:0.004404, train_acc:0.944198, test_loss:0.011201, test_acc:0.884115\n",
      "->epoch:445, train_loss:0.004320, train_acc:0.945645, test_loss:0.011093, test_acc:0.885807\n",
      "->epoch:446, train_loss:0.004444, train_acc:0.944170, test_loss:0.011598, test_acc:0.877604\n",
      "->epoch:447, train_loss:0.004327, train_acc:0.945703, test_loss:0.011119, test_acc:0.882552\n",
      "->epoch:448, train_loss:0.004330, train_acc:0.945833, test_loss:0.010926, test_acc:0.881120\n",
      "->epoch:449, train_loss:0.004364, train_acc:0.944777, test_loss:0.010010, test_acc:0.887891\n",
      "->epoch:450, train_loss:0.004419, train_acc:0.943591, test_loss:0.010715, test_acc:0.880078\n",
      "->epoch:451, train_loss:0.004340, train_acc:0.944719, test_loss:0.011300, test_acc:0.883073\n",
      "->epoch:452, train_loss:0.004400, train_acc:0.943461, test_loss:0.011074, test_acc:0.884505\n",
      "->epoch:453, train_loss:0.004375, train_acc:0.944849, test_loss:0.010488, test_acc:0.885156\n",
      "->epoch:454, train_loss:0.004364, train_acc:0.944256, test_loss:0.010845, test_acc:0.883203\n",
      "->epoch:455, train_loss:0.004406, train_acc:0.945052, test_loss:0.010583, test_acc:0.883724\n",
      "->epoch:456, train_loss:0.004398, train_acc:0.944676, test_loss:0.011030, test_acc:0.879948\n",
      "->epoch:457, train_loss:0.004452, train_acc:0.944372, test_loss:0.011467, test_acc:0.882943\n",
      "->epoch:458, train_loss:0.004383, train_acc:0.944936, test_loss:0.010621, test_acc:0.888672\n",
      "->epoch:459, train_loss:0.004384, train_acc:0.944676, test_loss:0.011155, test_acc:0.885156\n",
      "->epoch:460, train_loss:0.004410, train_acc:0.944893, test_loss:0.011311, test_acc:0.884896\n",
      "->epoch:461, train_loss:0.004457, train_acc:0.944300, test_loss:0.011648, test_acc:0.877214\n",
      "->epoch:462, train_loss:0.004445, train_acc:0.944112, test_loss:0.012236, test_acc:0.879557\n",
      "->epoch:463, train_loss:0.004391, train_acc:0.944604, test_loss:0.012616, test_acc:0.879167\n",
      "->epoch:464, train_loss:0.004407, train_acc:0.945067, test_loss:0.010484, test_acc:0.882422\n",
      "->epoch:465, train_loss:0.004361, train_acc:0.944748, test_loss:0.011466, test_acc:0.878125\n",
      "->epoch:466, train_loss:0.004420, train_acc:0.944401, test_loss:0.011692, test_acc:0.878516\n",
      "->epoch:467, train_loss:0.004342, train_acc:0.945197, test_loss:0.011200, test_acc:0.884505\n",
      "->epoch:468, train_loss:0.004315, train_acc:0.946137, test_loss:0.010851, test_acc:0.882943\n",
      "->epoch:469, train_loss:0.004388, train_acc:0.944907, test_loss:0.011139, test_acc:0.882031\n",
      "->epoch:470, train_loss:0.004391, train_acc:0.944314, test_loss:0.010880, test_acc:0.883984\n",
      "->epoch:471, train_loss:0.004366, train_acc:0.945124, test_loss:0.010838, test_acc:0.882292\n",
      "->epoch:472, train_loss:0.004429, train_acc:0.944010, test_loss:0.011202, test_acc:0.877604\n",
      "->epoch:473, train_loss:0.004443, train_acc:0.942954, test_loss:0.011731, test_acc:0.881901\n",
      "->epoch:474, train_loss:0.004331, train_acc:0.946238, test_loss:0.011082, test_acc:0.881250\n",
      "->epoch:475, train_loss:0.004389, train_acc:0.944372, test_loss:0.011517, test_acc:0.881641\n",
      "->epoch:476, train_loss:0.004343, train_acc:0.945269, test_loss:0.011269, test_acc:0.888151\n",
      "->epoch:477, train_loss:0.004293, train_acc:0.945240, test_loss:0.012020, test_acc:0.882552\n",
      "->epoch:478, train_loss:0.004401, train_acc:0.944372, test_loss:0.010363, test_acc:0.883464\n",
      "->epoch:479, train_loss:0.004455, train_acc:0.944329, test_loss:0.010745, test_acc:0.880729\n",
      "->epoch:480, train_loss:0.004408, train_acc:0.944821, test_loss:0.010864, test_acc:0.888281\n",
      "->epoch:481, train_loss:0.004458, train_acc:0.943822, test_loss:0.011861, test_acc:0.880729\n",
      "->epoch:482, train_loss:0.004299, train_acc:0.946209, test_loss:0.011165, test_acc:0.883073\n",
      "->epoch:483, train_loss:0.004339, train_acc:0.945978, test_loss:0.011120, test_acc:0.880599\n",
      "->epoch:484, train_loss:0.004279, train_acc:0.946021, test_loss:0.011704, test_acc:0.880078\n",
      "->epoch:485, train_loss:0.004293, train_acc:0.945949, test_loss:0.012111, test_acc:0.880208\n",
      "->epoch:486, train_loss:0.004396, train_acc:0.945182, test_loss:0.011497, test_acc:0.880208\n",
      "->epoch:487, train_loss:0.004319, train_acc:0.946282, test_loss:0.010512, test_acc:0.881120\n",
      "->epoch:488, train_loss:0.004339, train_acc:0.945038, test_loss:0.010878, test_acc:0.882552\n",
      "->epoch:489, train_loss:0.004373, train_acc:0.943938, test_loss:0.010351, test_acc:0.888932\n",
      "->epoch:490, train_loss:0.004343, train_acc:0.945515, test_loss:0.010599, test_acc:0.883073\n",
      "->epoch:491, train_loss:0.004398, train_acc:0.944025, test_loss:0.010673, test_acc:0.883333\n",
      "->epoch:492, train_loss:0.004309, train_acc:0.944980, test_loss:0.010497, test_acc:0.886458\n",
      "->epoch:493, train_loss:0.004406, train_acc:0.944141, test_loss:0.011112, test_acc:0.881120\n",
      "->epoch:494, train_loss:0.004289, train_acc:0.945573, test_loss:0.010377, test_acc:0.885156\n",
      "->epoch:495, train_loss:0.004389, train_acc:0.945443, test_loss:0.010879, test_acc:0.882943\n",
      "->epoch:496, train_loss:0.004429, train_acc:0.943707, test_loss:0.010855, test_acc:0.885547\n",
      "->epoch:497, train_loss:0.004354, train_acc:0.945877, test_loss:0.010648, test_acc:0.888151\n",
      "->epoch:498, train_loss:0.004346, train_acc:0.945501, test_loss:0.010911, test_acc:0.888802\n",
      "->epoch:499, train_loss:0.004390, train_acc:0.944864, test_loss:0.011107, test_acc:0.882161\n",
      "->epoch:500, train_loss:0.004359, train_acc:0.944502, test_loss:0.011813, test_acc:0.875391\n",
      "->epoch:501, train_loss:0.004421, train_acc:0.944502, test_loss:0.011211, test_acc:0.884245\n",
      "->epoch:502, train_loss:0.004365, train_acc:0.945530, test_loss:0.011856, test_acc:0.881641\n",
      "->epoch:503, train_loss:0.004358, train_acc:0.945544, test_loss:0.012453, test_acc:0.878776\n",
      "->epoch:504, train_loss:0.004346, train_acc:0.945255, test_loss:0.010984, test_acc:0.887891\n",
      "->epoch:505, train_loss:0.004240, train_acc:0.946846, test_loss:0.011491, test_acc:0.882161\n",
      "->epoch:506, train_loss:0.004339, train_acc:0.945009, test_loss:0.012025, test_acc:0.884896\n",
      "->epoch:507, train_loss:0.004409, train_acc:0.943359, test_loss:0.011104, test_acc:0.883984\n",
      "->epoch:508, train_loss:0.004404, train_acc:0.944068, test_loss:0.011017, test_acc:0.881901\n",
      "->epoch:509, train_loss:0.004295, train_acc:0.945312, test_loss:0.011536, test_acc:0.881250\n",
      "->epoch:510, train_loss:0.004349, train_acc:0.945312, test_loss:0.010822, test_acc:0.879818\n",
      "->epoch:511, train_loss:0.004254, train_acc:0.946108, test_loss:0.011924, test_acc:0.879427\n",
      "->epoch:512, train_loss:0.004351, train_acc:0.945457, test_loss:0.010427, test_acc:0.883073\n",
      "->epoch:513, train_loss:0.004362, train_acc:0.945168, test_loss:0.010471, test_acc:0.883984\n",
      "->epoch:514, train_loss:0.004428, train_acc:0.944285, test_loss:0.010920, test_acc:0.880859\n",
      "->epoch:515, train_loss:0.004310, train_acc:0.945689, test_loss:0.011712, test_acc:0.884505\n",
      "->epoch:516, train_loss:0.004297, train_acc:0.946181, test_loss:0.011266, test_acc:0.880729\n",
      "->epoch:517, train_loss:0.004333, train_acc:0.944864, test_loss:0.011304, test_acc:0.887109\n",
      "->epoch:518, train_loss:0.004333, train_acc:0.945631, test_loss:0.011141, test_acc:0.882813\n",
      "->epoch:519, train_loss:0.004360, train_acc:0.945124, test_loss:0.012271, test_acc:0.877995\n",
      "->epoch:520, train_loss:0.004249, train_acc:0.945689, test_loss:0.010795, test_acc:0.883464\n",
      "->epoch:521, train_loss:0.004235, train_acc:0.946615, test_loss:0.010441, test_acc:0.884766\n",
      "->epoch:522, train_loss:0.004354, train_acc:0.945472, test_loss:0.010831, test_acc:0.883203\n",
      "->epoch:523, train_loss:0.004288, train_acc:0.946282, test_loss:0.011095, test_acc:0.880599\n",
      "->epoch:524, train_loss:0.004281, train_acc:0.945356, test_loss:0.011588, test_acc:0.884896\n",
      "->epoch:525, train_loss:0.004352, train_acc:0.944618, test_loss:0.010811, test_acc:0.887109\n",
      "->epoch:526, train_loss:0.004319, train_acc:0.945284, test_loss:0.010799, test_acc:0.883724\n",
      "->epoch:527, train_loss:0.004282, train_acc:0.946354, test_loss:0.011001, test_acc:0.884375\n",
      "->epoch:528, train_loss:0.004262, train_acc:0.946036, test_loss:0.010983, test_acc:0.885677\n",
      "->epoch:529, train_loss:0.004282, train_acc:0.944777, test_loss:0.011307, test_acc:0.878385\n",
      "->epoch:530, train_loss:0.004359, train_acc:0.945587, test_loss:0.011023, test_acc:0.886589\n",
      "->epoch:531, train_loss:0.004316, train_acc:0.945718, test_loss:0.010862, test_acc:0.888151\n",
      "->epoch:532, train_loss:0.004364, train_acc:0.945819, test_loss:0.010329, test_acc:0.886328\n",
      "->epoch:533, train_loss:0.004373, train_acc:0.944994, test_loss:0.010787, test_acc:0.885156\n",
      "->epoch:534, train_loss:0.004223, train_acc:0.946904, test_loss:0.011725, test_acc:0.881120\n",
      "->epoch:535, train_loss:0.004310, train_acc:0.946412, test_loss:0.011686, test_acc:0.888412\n",
      "->epoch:536, train_loss:0.004392, train_acc:0.944878, test_loss:0.010559, test_acc:0.886458\n",
      "->epoch:537, train_loss:0.004272, train_acc:0.946340, test_loss:0.010904, test_acc:0.887240\n",
      "->epoch:538, train_loss:0.004307, train_acc:0.945631, test_loss:0.010526, test_acc:0.883203\n",
      "->epoch:539, train_loss:0.004318, train_acc:0.945587, test_loss:0.011290, test_acc:0.884505\n",
      "->epoch:540, train_loss:0.004346, train_acc:0.945009, test_loss:0.011829, test_acc:0.880208\n",
      "->epoch:541, train_loss:0.004275, train_acc:0.947092, test_loss:0.011219, test_acc:0.883203\n",
      "->epoch:542, train_loss:0.004281, train_acc:0.945472, test_loss:0.010910, test_acc:0.880339\n",
      "->epoch:543, train_loss:0.004263, train_acc:0.946224, test_loss:0.011162, test_acc:0.884635\n",
      "->epoch:544, train_loss:0.004212, train_acc:0.947569, test_loss:0.011403, test_acc:0.885938\n",
      "->epoch:545, train_loss:0.004323, train_acc:0.946441, test_loss:0.011565, test_acc:0.883203\n",
      "->epoch:546, train_loss:0.004306, train_acc:0.944676, test_loss:0.010557, test_acc:0.885547\n",
      "->epoch:547, train_loss:0.004236, train_acc:0.946962, test_loss:0.011306, test_acc:0.888021\n",
      "->epoch:548, train_loss:0.004351, train_acc:0.944097, test_loss:0.010041, test_acc:0.883594\n",
      "->epoch:549, train_loss:0.004274, train_acc:0.945775, test_loss:0.011314, test_acc:0.883333\n",
      "->epoch:550, train_loss:0.004293, train_acc:0.945978, test_loss:0.011140, test_acc:0.883984\n",
      "->epoch:551, train_loss:0.004294, train_acc:0.945718, test_loss:0.011082, test_acc:0.886458\n",
      "->epoch:552, train_loss:0.004283, train_acc:0.945891, test_loss:0.012119, test_acc:0.885938\n",
      "->epoch:553, train_loss:0.004298, train_acc:0.945197, test_loss:0.011000, test_acc:0.878125\n",
      "->epoch:554, train_loss:0.004173, train_acc:0.947251, test_loss:0.011288, test_acc:0.882422\n",
      "->epoch:555, train_loss:0.004347, train_acc:0.945399, test_loss:0.013368, test_acc:0.877995\n",
      "->epoch:556, train_loss:0.004267, train_acc:0.946860, test_loss:0.011255, test_acc:0.883724\n",
      "->epoch:557, train_loss:0.004261, train_acc:0.946586, test_loss:0.011455, test_acc:0.885677\n",
      "->epoch:558, train_loss:0.004282, train_acc:0.946050, test_loss:0.011027, test_acc:0.888412\n",
      "->epoch:559, train_loss:0.004328, train_acc:0.945197, test_loss:0.011966, test_acc:0.883073\n",
      "->epoch:560, train_loss:0.004241, train_acc:0.945341, test_loss:0.011703, test_acc:0.886719\n",
      "->epoch:561, train_loss:0.004234, train_acc:0.946528, test_loss:0.011166, test_acc:0.883724\n",
      "->epoch:562, train_loss:0.004257, train_acc:0.946730, test_loss:0.010469, test_acc:0.887630\n",
      "->epoch:563, train_loss:0.004382, train_acc:0.944864, test_loss:0.011183, test_acc:0.883333\n",
      "->epoch:564, train_loss:0.004287, train_acc:0.946267, test_loss:0.010753, test_acc:0.889714\n",
      "->epoch:565, train_loss:0.004174, train_acc:0.947902, test_loss:0.011574, test_acc:0.887370\n",
      "->epoch:566, train_loss:0.004203, train_acc:0.947020, test_loss:0.011771, test_acc:0.879557\n",
      "->epoch:567, train_loss:0.004247, train_acc:0.947295, test_loss:0.011610, test_acc:0.883203\n",
      "->epoch:568, train_loss:0.004274, train_acc:0.946484, test_loss:0.010873, test_acc:0.885287\n",
      "->epoch:569, train_loss:0.004329, train_acc:0.945964, test_loss:0.010959, test_acc:0.882161\n",
      "->epoch:570, train_loss:0.004263, train_acc:0.945906, test_loss:0.011025, test_acc:0.880339\n",
      "->epoch:571, train_loss:0.004282, train_acc:0.946209, test_loss:0.011814, test_acc:0.884635\n",
      "->epoch:572, train_loss:0.004178, train_acc:0.946962, test_loss:0.011606, test_acc:0.885287\n",
      "->epoch:573, train_loss:0.004272, train_acc:0.945341, test_loss:0.011055, test_acc:0.885807\n",
      "->epoch:574, train_loss:0.004282, train_acc:0.946672, test_loss:0.010792, test_acc:0.882943\n",
      "->epoch:575, train_loss:0.004323, train_acc:0.946311, test_loss:0.010700, test_acc:0.885287\n",
      "->epoch:576, train_loss:0.004222, train_acc:0.947541, test_loss:0.010794, test_acc:0.885156\n",
      "->epoch:577, train_loss:0.004285, train_acc:0.946528, test_loss:0.010449, test_acc:0.883594\n",
      "->epoch:578, train_loss:0.004180, train_acc:0.947598, test_loss:0.011602, test_acc:0.882552\n",
      "->epoch:579, train_loss:0.004273, train_acc:0.946311, test_loss:0.010584, test_acc:0.876953\n",
      "->epoch:580, train_loss:0.004252, train_acc:0.946759, test_loss:0.010684, test_acc:0.882292\n",
      "->epoch:581, train_loss:0.004325, train_acc:0.945718, test_loss:0.011051, test_acc:0.881120\n",
      "->epoch:582, train_loss:0.004350, train_acc:0.944835, test_loss:0.011579, test_acc:0.885417\n",
      "->epoch:583, train_loss:0.004310, train_acc:0.944907, test_loss:0.010315, test_acc:0.887760\n",
      "->epoch:584, train_loss:0.004345, train_acc:0.946007, test_loss:0.011312, test_acc:0.887370\n",
      "->epoch:585, train_loss:0.004243, train_acc:0.946962, test_loss:0.011312, test_acc:0.887760\n",
      "->epoch:586, train_loss:0.004299, train_acc:0.945240, test_loss:0.010286, test_acc:0.889063\n",
      "->epoch:587, train_loss:0.004291, train_acc:0.946282, test_loss:0.010321, test_acc:0.886589\n",
      "->epoch:588, train_loss:0.004238, train_acc:0.946325, test_loss:0.011438, test_acc:0.883333\n",
      "->epoch:589, train_loss:0.004231, train_acc:0.946875, test_loss:0.010344, test_acc:0.883073\n",
      "->epoch:590, train_loss:0.004235, train_acc:0.947237, test_loss:0.010956, test_acc:0.878255\n",
      "->epoch:591, train_loss:0.004327, train_acc:0.945240, test_loss:0.010790, test_acc:0.885938\n",
      "->epoch:592, train_loss:0.004231, train_acc:0.946991, test_loss:0.010554, test_acc:0.883724\n",
      "->epoch:593, train_loss:0.004358, train_acc:0.945761, test_loss:0.011038, test_acc:0.881120\n",
      "->epoch:594, train_loss:0.004172, train_acc:0.947410, test_loss:0.011571, test_acc:0.880208\n",
      "->epoch:595, train_loss:0.004195, train_acc:0.947425, test_loss:0.011738, test_acc:0.883203\n",
      "->epoch:596, train_loss:0.004284, train_acc:0.946152, test_loss:0.010308, test_acc:0.887630\n",
      "->epoch:597, train_loss:0.004332, train_acc:0.945761, test_loss:0.010653, test_acc:0.885156\n",
      "->epoch:598, train_loss:0.004315, train_acc:0.945992, test_loss:0.010718, test_acc:0.885026\n",
      "->epoch:599, train_loss:0.004316, train_acc:0.945168, test_loss:0.010330, test_acc:0.886719\n",
      "->epoch:600, train_loss:0.004158, train_acc:0.947700, test_loss:0.010200, test_acc:0.887500\n",
      "->epoch:601, train_loss:0.004261, train_acc:0.946050, test_loss:0.011599, test_acc:0.885547\n",
      "->epoch:602, train_loss:0.004178, train_acc:0.947367, test_loss:0.011339, test_acc:0.881771\n",
      "->epoch:603, train_loss:0.004295, train_acc:0.946340, test_loss:0.010733, test_acc:0.886068\n",
      "->epoch:604, train_loss:0.004292, train_acc:0.944416, test_loss:0.011224, test_acc:0.881380\n",
      "->epoch:605, train_loss:0.004254, train_acc:0.945616, test_loss:0.009877, test_acc:0.890234\n",
      "->epoch:606, train_loss:0.004302, train_acc:0.945804, test_loss:0.010911, test_acc:0.884375\n",
      "->epoch:607, train_loss:0.004200, train_acc:0.947671, test_loss:0.011631, test_acc:0.881901\n",
      "->epoch:608, train_loss:0.004176, train_acc:0.947237, test_loss:0.011007, test_acc:0.884896\n",
      "->epoch:609, train_loss:0.004188, train_acc:0.948090, test_loss:0.011840, test_acc:0.876693\n",
      "->epoch:610, train_loss:0.004145, train_acc:0.947830, test_loss:0.011202, test_acc:0.884505\n",
      "->epoch:611, train_loss:0.004323, train_acc:0.945804, test_loss:0.012167, test_acc:0.878516\n",
      "->epoch:612, train_loss:0.004240, train_acc:0.947092, test_loss:0.011450, test_acc:0.883333\n",
      "->epoch:613, train_loss:0.004239, train_acc:0.946991, test_loss:0.010732, test_acc:0.886198\n",
      "->epoch:614, train_loss:0.004278, train_acc:0.945906, test_loss:0.011276, test_acc:0.885026\n",
      "->epoch:615, train_loss:0.004199, train_acc:0.947106, test_loss:0.010733, test_acc:0.883984\n",
      "->epoch:616, train_loss:0.004239, train_acc:0.945616, test_loss:0.011013, test_acc:0.891797\n",
      "->epoch:617, train_loss:0.004230, train_acc:0.947222, test_loss:0.010817, test_acc:0.885156\n",
      "->epoch:618, train_loss:0.004171, train_acc:0.947859, test_loss:0.011337, test_acc:0.883464\n",
      "->epoch:619, train_loss:0.004184, train_acc:0.947569, test_loss:0.011886, test_acc:0.885677\n",
      "->epoch:620, train_loss:0.004260, train_acc:0.946571, test_loss:0.010564, test_acc:0.883854\n",
      "->epoch:621, train_loss:0.004233, train_acc:0.948003, test_loss:0.010691, test_acc:0.888672\n",
      "->epoch:622, train_loss:0.004215, train_acc:0.947700, test_loss:0.010720, test_acc:0.888932\n",
      "->epoch:623, train_loss:0.004212, train_acc:0.946036, test_loss:0.011422, test_acc:0.884635\n",
      "->epoch:624, train_loss:0.004215, train_acc:0.946325, test_loss:0.011273, test_acc:0.886328\n",
      "->epoch:625, train_loss:0.004233, train_acc:0.947150, test_loss:0.010790, test_acc:0.888021\n",
      "->epoch:626, train_loss:0.004307, train_acc:0.946803, test_loss:0.010385, test_acc:0.885287\n",
      "->epoch:627, train_loss:0.004271, train_acc:0.945906, test_loss:0.010784, test_acc:0.885417\n",
      "->epoch:628, train_loss:0.004357, train_acc:0.945761, test_loss:0.010890, test_acc:0.883724\n",
      "->epoch:629, train_loss:0.004223, train_acc:0.946152, test_loss:0.011949, test_acc:0.880599\n",
      "->epoch:630, train_loss:0.004243, train_acc:0.946021, test_loss:0.010730, test_acc:0.883203\n",
      "->epoch:631, train_loss:0.004236, train_acc:0.946181, test_loss:0.011269, test_acc:0.881901\n",
      "->epoch:632, train_loss:0.004168, train_acc:0.947975, test_loss:0.010884, test_acc:0.885156\n",
      "->epoch:633, train_loss:0.004192, train_acc:0.947902, test_loss:0.011115, test_acc:0.882552\n",
      "->epoch:634, train_loss:0.004222, train_acc:0.946933, test_loss:0.011294, test_acc:0.885547\n",
      "->epoch:635, train_loss:0.004182, train_acc:0.947396, test_loss:0.011233, test_acc:0.881641\n",
      "->epoch:636, train_loss:0.004164, train_acc:0.947960, test_loss:0.010729, test_acc:0.882422\n",
      "->epoch:637, train_loss:0.004225, train_acc:0.947410, test_loss:0.010502, test_acc:0.884896\n",
      "->epoch:638, train_loss:0.004173, train_acc:0.949074, test_loss:0.011164, test_acc:0.883724\n",
      "->epoch:639, train_loss:0.004155, train_acc:0.947743, test_loss:0.011303, test_acc:0.879948\n",
      "->epoch:640, train_loss:0.004218, train_acc:0.946094, test_loss:0.011159, test_acc:0.879557\n",
      "->epoch:641, train_loss:0.004217, train_acc:0.947150, test_loss:0.010784, test_acc:0.887240\n",
      "->epoch:642, train_loss:0.004162, train_acc:0.947830, test_loss:0.010977, test_acc:0.886068\n",
      "->epoch:643, train_loss:0.004241, train_acc:0.946803, test_loss:0.011010, test_acc:0.889193\n",
      "->epoch:644, train_loss:0.004221, train_acc:0.947541, test_loss:0.010985, test_acc:0.882552\n",
      "->epoch:645, train_loss:0.004148, train_acc:0.947555, test_loss:0.010564, test_acc:0.887370\n",
      "->epoch:646, train_loss:0.004226, train_acc:0.947512, test_loss:0.011276, test_acc:0.885156\n",
      "->epoch:647, train_loss:0.004221, train_acc:0.946455, test_loss:0.010581, test_acc:0.887891\n",
      "->epoch:648, train_loss:0.004223, train_acc:0.947381, test_loss:0.011390, test_acc:0.880599\n",
      "->epoch:649, train_loss:0.004192, train_acc:0.946441, test_loss:0.010980, test_acc:0.879688\n",
      "->epoch:650, train_loss:0.004226, train_acc:0.946325, test_loss:0.010837, test_acc:0.886458\n",
      "->epoch:651, train_loss:0.004254, train_acc:0.946354, test_loss:0.010808, test_acc:0.887370\n",
      "->epoch:652, train_loss:0.004238, train_acc:0.946600, test_loss:0.010056, test_acc:0.889063\n",
      "->epoch:653, train_loss:0.004247, train_acc:0.946586, test_loss:0.011479, test_acc:0.885938\n",
      "->epoch:654, train_loss:0.004222, train_acc:0.946933, test_loss:0.010862, test_acc:0.881380\n",
      "->epoch:655, train_loss:0.004222, train_acc:0.947555, test_loss:0.010710, test_acc:0.880469\n",
      "->epoch:656, train_loss:0.004214, train_acc:0.947324, test_loss:0.011226, test_acc:0.883984\n",
      "->epoch:657, train_loss:0.004224, train_acc:0.947266, test_loss:0.011185, test_acc:0.885287\n",
      "->epoch:658, train_loss:0.004203, train_acc:0.946398, test_loss:0.011243, test_acc:0.885807\n",
      "->epoch:659, train_loss:0.004209, train_acc:0.947078, test_loss:0.011484, test_acc:0.885547\n",
      "->epoch:660, train_loss:0.004202, train_acc:0.946267, test_loss:0.010850, test_acc:0.890625\n",
      "->epoch:661, train_loss:0.004251, train_acc:0.947627, test_loss:0.012387, test_acc:0.883854\n",
      "->epoch:662, train_loss:0.004202, train_acc:0.947757, test_loss:0.011410, test_acc:0.886068\n",
      "->epoch:663, train_loss:0.004250, train_acc:0.947439, test_loss:0.010842, test_acc:0.885287\n",
      "->epoch:664, train_loss:0.004266, train_acc:0.947049, test_loss:0.010817, test_acc:0.884245\n",
      "->epoch:665, train_loss:0.004194, train_acc:0.946701, test_loss:0.011549, test_acc:0.885026\n",
      "->epoch:666, train_loss:0.004228, train_acc:0.948032, test_loss:0.011029, test_acc:0.883203\n",
      "->epoch:667, train_loss:0.004207, train_acc:0.947671, test_loss:0.010840, test_acc:0.880599\n",
      "->epoch:668, train_loss:0.004098, train_acc:0.948727, test_loss:0.012286, test_acc:0.879818\n",
      "->epoch:669, train_loss:0.004206, train_acc:0.946470, test_loss:0.010767, test_acc:0.885938\n",
      "->epoch:670, train_loss:0.004183, train_acc:0.947005, test_loss:0.011022, test_acc:0.883203\n",
      "->epoch:671, train_loss:0.004157, train_acc:0.948220, test_loss:0.011188, test_acc:0.888802\n",
      "->epoch:672, train_loss:0.004136, train_acc:0.947598, test_loss:0.011069, test_acc:0.883984\n",
      "->epoch:673, train_loss:0.004233, train_acc:0.947555, test_loss:0.010644, test_acc:0.882031\n",
      "->epoch:674, train_loss:0.004100, train_acc:0.948915, test_loss:0.012173, test_acc:0.880078\n",
      "->epoch:675, train_loss:0.004170, train_acc:0.947425, test_loss:0.011652, test_acc:0.883464\n",
      "->epoch:676, train_loss:0.004195, train_acc:0.947396, test_loss:0.010980, test_acc:0.888932\n",
      "->epoch:677, train_loss:0.004225, train_acc:0.946311, test_loss:0.010229, test_acc:0.888672\n",
      "->epoch:678, train_loss:0.004088, train_acc:0.948785, test_loss:0.011072, test_acc:0.885938\n",
      "->epoch:679, train_loss:0.004204, train_acc:0.946976, test_loss:0.012174, test_acc:0.881641\n",
      "->epoch:680, train_loss:0.004204, train_acc:0.947888, test_loss:0.011470, test_acc:0.887630\n",
      "->epoch:681, train_loss:0.004145, train_acc:0.948105, test_loss:0.011112, test_acc:0.884245\n",
      "->epoch:682, train_loss:0.004190, train_acc:0.947266, test_loss:0.011266, test_acc:0.882552\n",
      "->epoch:683, train_loss:0.004202, train_acc:0.947671, test_loss:0.011667, test_acc:0.885938\n",
      "->epoch:684, train_loss:0.004184, train_acc:0.947975, test_loss:0.010228, test_acc:0.889193\n",
      "->epoch:685, train_loss:0.004176, train_acc:0.947975, test_loss:0.010910, test_acc:0.884505\n",
      "->epoch:686, train_loss:0.004108, train_acc:0.948929, test_loss:0.011810, test_acc:0.884245\n",
      "->epoch:687, train_loss:0.004119, train_acc:0.948900, test_loss:0.010847, test_acc:0.888802\n",
      "->epoch:688, train_loss:0.004199, train_acc:0.948293, test_loss:0.011287, test_acc:0.883333\n",
      "->epoch:689, train_loss:0.004083, train_acc:0.948756, test_loss:0.011324, test_acc:0.883333\n",
      "->epoch:690, train_loss:0.004168, train_acc:0.947786, test_loss:0.010974, test_acc:0.885417\n",
      "->epoch:691, train_loss:0.004180, train_acc:0.947873, test_loss:0.012546, test_acc:0.882161\n",
      "->epoch:692, train_loss:0.004086, train_acc:0.947931, test_loss:0.011286, test_acc:0.884766\n",
      "->epoch:693, train_loss:0.004085, train_acc:0.948828, test_loss:0.011076, test_acc:0.885287\n",
      "->epoch:694, train_loss:0.004119, train_acc:0.948495, test_loss:0.011604, test_acc:0.886589\n",
      "->epoch:695, train_loss:0.004119, train_acc:0.947946, test_loss:0.011714, test_acc:0.882943\n",
      "->epoch:696, train_loss:0.004130, train_acc:0.949060, test_loss:0.011341, test_acc:0.886849\n",
      "->epoch:697, train_loss:0.004201, train_acc:0.946615, test_loss:0.011889, test_acc:0.882161\n",
      "->epoch:698, train_loss:0.004173, train_acc:0.948119, test_loss:0.011023, test_acc:0.889714\n",
      "->epoch:699, train_loss:0.004177, train_acc:0.948452, test_loss:0.011242, test_acc:0.882422\n",
      "->epoch:700, train_loss:0.004178, train_acc:0.947671, test_loss:0.011199, test_acc:0.883724\n",
      "->epoch:701, train_loss:0.004096, train_acc:0.948264, test_loss:0.011204, test_acc:0.883073\n",
      "->epoch:702, train_loss:0.004203, train_acc:0.947005, test_loss:0.010434, test_acc:0.886198\n",
      "->epoch:703, train_loss:0.004163, train_acc:0.947989, test_loss:0.011206, test_acc:0.884115\n",
      "->epoch:704, train_loss:0.004109, train_acc:0.947598, test_loss:0.012407, test_acc:0.882682\n",
      "->epoch:705, train_loss:0.004133, train_acc:0.948452, test_loss:0.011469, test_acc:0.885547\n",
      "->epoch:706, train_loss:0.004133, train_acc:0.948032, test_loss:0.011263, test_acc:0.886849\n",
      "->epoch:707, train_loss:0.004298, train_acc:0.945501, test_loss:0.011674, test_acc:0.879557\n",
      "->epoch:708, train_loss:0.004167, train_acc:0.947801, test_loss:0.011592, test_acc:0.885026\n",
      "->epoch:709, train_loss:0.004069, train_acc:0.948409, test_loss:0.011158, test_acc:0.888672\n",
      "->epoch:710, train_loss:0.004229, train_acc:0.946600, test_loss:0.011223, test_acc:0.884245\n",
      "->epoch:711, train_loss:0.004156, train_acc:0.947642, test_loss:0.010864, test_acc:0.888542\n",
      "->epoch:712, train_loss:0.004099, train_acc:0.948481, test_loss:0.011183, test_acc:0.885287\n",
      "->epoch:713, train_loss:0.004248, train_acc:0.947078, test_loss:0.011694, test_acc:0.877083\n",
      "->epoch:714, train_loss:0.004080, train_acc:0.948785, test_loss:0.011084, test_acc:0.882813\n",
      "->epoch:715, train_loss:0.004048, train_acc:0.949450, test_loss:0.012126, test_acc:0.885677\n",
      "->epoch:716, train_loss:0.004084, train_acc:0.949349, test_loss:0.011153, test_acc:0.884375\n",
      "->epoch:717, train_loss:0.004117, train_acc:0.948756, test_loss:0.010367, test_acc:0.887760\n",
      "->epoch:718, train_loss:0.004110, train_acc:0.948235, test_loss:0.011159, test_acc:0.888542\n",
      "->epoch:719, train_loss:0.004109, train_acc:0.949132, test_loss:0.011926, test_acc:0.882161\n",
      "->epoch:720, train_loss:0.004032, train_acc:0.948741, test_loss:0.011274, test_acc:0.887370\n",
      "->epoch:721, train_loss:0.004115, train_acc:0.948206, test_loss:0.012133, test_acc:0.885287\n",
      "->epoch:722, train_loss:0.004225, train_acc:0.948192, test_loss:0.011787, test_acc:0.883464\n",
      "->epoch:723, train_loss:0.004150, train_acc:0.949190, test_loss:0.011425, test_acc:0.884115\n",
      "->epoch:724, train_loss:0.004118, train_acc:0.947902, test_loss:0.010508, test_acc:0.887891\n",
      "->epoch:725, train_loss:0.004244, train_acc:0.946962, test_loss:0.010889, test_acc:0.879297\n",
      "->epoch:726, train_loss:0.004040, train_acc:0.949566, test_loss:0.010977, test_acc:0.888021\n",
      "->epoch:727, train_loss:0.004210, train_acc:0.947497, test_loss:0.010171, test_acc:0.888932\n",
      "->epoch:728, train_loss:0.004146, train_acc:0.947772, test_loss:0.011111, test_acc:0.886198\n",
      "->epoch:729, train_loss:0.004152, train_acc:0.948148, test_loss:0.011334, test_acc:0.887240\n",
      "->epoch:730, train_loss:0.004234, train_acc:0.947613, test_loss:0.011885, test_acc:0.876172\n",
      "->epoch:731, train_loss:0.004086, train_acc:0.948524, test_loss:0.011188, test_acc:0.889453\n",
      "->epoch:732, train_loss:0.004034, train_acc:0.949928, test_loss:0.011451, test_acc:0.884245\n",
      "->epoch:733, train_loss:0.004030, train_acc:0.949826, test_loss:0.010942, test_acc:0.888412\n",
      "->epoch:734, train_loss:0.004097, train_acc:0.949841, test_loss:0.011047, test_acc:0.886068\n",
      "->epoch:735, train_loss:0.004096, train_acc:0.948654, test_loss:0.011171, test_acc:0.886849\n",
      "->epoch:736, train_loss:0.004088, train_acc:0.949262, test_loss:0.011657, test_acc:0.885156\n",
      "->epoch:737, train_loss:0.004081, train_acc:0.949653, test_loss:0.010968, test_acc:0.885807\n",
      "->epoch:738, train_loss:0.004192, train_acc:0.947497, test_loss:0.010602, test_acc:0.886589\n",
      "->epoch:739, train_loss:0.004124, train_acc:0.948192, test_loss:0.010964, test_acc:0.886849\n",
      "->epoch:740, train_loss:0.004100, train_acc:0.948235, test_loss:0.011710, test_acc:0.884245\n",
      "->epoch:741, train_loss:0.004192, train_acc:0.948076, test_loss:0.011119, test_acc:0.881510\n",
      "->epoch:742, train_loss:0.004131, train_acc:0.948322, test_loss:0.010304, test_acc:0.886198\n",
      "->epoch:743, train_loss:0.004168, train_acc:0.947439, test_loss:0.010709, test_acc:0.886328\n",
      "->epoch:744, train_loss:0.004141, train_acc:0.948481, test_loss:0.011526, test_acc:0.886328\n",
      "->epoch:745, train_loss:0.004123, train_acc:0.948423, test_loss:0.011068, test_acc:0.887240\n",
      "->epoch:746, train_loss:0.004099, train_acc:0.947975, test_loss:0.010702, test_acc:0.883594\n",
      "->epoch:747, train_loss:0.004069, train_acc:0.949624, test_loss:0.010903, test_acc:0.883594\n",
      "->epoch:748, train_loss:0.004059, train_acc:0.948886, test_loss:0.011132, test_acc:0.883464\n",
      "->epoch:749, train_loss:0.004154, train_acc:0.948003, test_loss:0.010942, test_acc:0.882943\n",
      "->epoch:750, train_loss:0.004086, train_acc:0.948698, test_loss:0.011545, test_acc:0.882813\n",
      "->epoch:751, train_loss:0.004194, train_acc:0.947121, test_loss:0.012216, test_acc:0.876042\n",
      "->epoch:752, train_loss:0.004185, train_acc:0.947381, test_loss:0.012150, test_acc:0.883594\n",
      "->epoch:753, train_loss:0.004094, train_acc:0.948394, test_loss:0.011322, test_acc:0.882943\n",
      "->epoch:754, train_loss:0.004286, train_acc:0.946774, test_loss:0.010939, test_acc:0.880990\n",
      "->epoch:755, train_loss:0.004142, train_acc:0.948452, test_loss:0.010497, test_acc:0.885807\n",
      "->epoch:756, train_loss:0.004082, train_acc:0.949508, test_loss:0.010676, test_acc:0.888802\n",
      "->epoch:757, train_loss:0.004126, train_acc:0.948466, test_loss:0.010338, test_acc:0.888932\n",
      "->epoch:758, train_loss:0.004114, train_acc:0.948568, test_loss:0.010800, test_acc:0.881380\n",
      "->epoch:759, train_loss:0.004091, train_acc:0.949117, test_loss:0.010745, test_acc:0.888281\n",
      "->epoch:760, train_loss:0.004084, train_acc:0.948799, test_loss:0.010899, test_acc:0.885938\n",
      "->epoch:761, train_loss:0.004117, train_acc:0.947541, test_loss:0.010652, test_acc:0.886849\n",
      "->epoch:762, train_loss:0.004053, train_acc:0.949508, test_loss:0.011100, test_acc:0.882813\n",
      "->epoch:763, train_loss:0.004108, train_acc:0.948654, test_loss:0.011625, test_acc:0.887370\n",
      "->epoch:764, train_loss:0.004188, train_acc:0.948105, test_loss:0.010783, test_acc:0.882943\n",
      "->epoch:765, train_loss:0.004082, train_acc:0.949913, test_loss:0.011040, test_acc:0.889453\n",
      "->epoch:766, train_loss:0.004007, train_acc:0.949870, test_loss:0.011177, test_acc:0.885547\n",
      "->epoch:767, train_loss:0.004095, train_acc:0.949407, test_loss:0.011436, test_acc:0.883984\n",
      "->epoch:768, train_loss:0.004086, train_acc:0.948235, test_loss:0.011339, test_acc:0.882292\n",
      "->epoch:769, train_loss:0.004121, train_acc:0.948568, test_loss:0.010619, test_acc:0.887240\n",
      "->epoch:770, train_loss:0.004070, train_acc:0.949580, test_loss:0.010964, test_acc:0.885547\n",
      "->epoch:771, train_loss:0.004132, train_acc:0.948090, test_loss:0.011511, test_acc:0.889193\n",
      "->epoch:772, train_loss:0.004066, train_acc:0.948611, test_loss:0.010835, test_acc:0.885417\n",
      "->epoch:773, train_loss:0.004021, train_acc:0.949248, test_loss:0.010685, test_acc:0.888542\n",
      "->epoch:774, train_loss:0.004089, train_acc:0.947888, test_loss:0.011241, test_acc:0.884115\n",
      "->epoch:775, train_loss:0.004174, train_acc:0.947714, test_loss:0.011397, test_acc:0.878516\n",
      "->epoch:776, train_loss:0.004155, train_acc:0.948553, test_loss:0.011087, test_acc:0.888021\n",
      "->epoch:777, train_loss:0.004063, train_acc:0.949190, test_loss:0.010555, test_acc:0.886198\n",
      "->epoch:778, train_loss:0.004078, train_acc:0.948220, test_loss:0.010412, test_acc:0.890234\n",
      "->epoch:779, train_loss:0.004160, train_acc:0.947917, test_loss:0.011108, test_acc:0.886328\n",
      "->epoch:780, train_loss:0.004166, train_acc:0.947757, test_loss:0.011238, test_acc:0.886068\n",
      "->epoch:781, train_loss:0.004057, train_acc:0.948857, test_loss:0.011097, test_acc:0.888802\n",
      "->epoch:782, train_loss:0.004060, train_acc:0.949711, test_loss:0.011536, test_acc:0.883333\n",
      "->epoch:783, train_loss:0.004151, train_acc:0.948322, test_loss:0.010597, test_acc:0.886328\n",
      "->epoch:784, train_loss:0.004176, train_acc:0.947960, test_loss:0.011124, test_acc:0.885547\n",
      "->epoch:785, train_loss:0.004141, train_acc:0.948539, test_loss:0.010733, test_acc:0.886198\n",
      "->epoch:786, train_loss:0.004089, train_acc:0.948669, test_loss:0.011159, test_acc:0.881120\n",
      "->epoch:787, train_loss:0.004136, train_acc:0.948915, test_loss:0.010730, test_acc:0.885547\n",
      "->epoch:788, train_loss:0.004046, train_acc:0.949465, test_loss:0.010904, test_acc:0.885807\n",
      "->epoch:789, train_loss:0.004117, train_acc:0.947295, test_loss:0.011105, test_acc:0.881510\n",
      "->epoch:790, train_loss:0.004109, train_acc:0.948466, test_loss:0.011011, test_acc:0.885026\n",
      "->epoch:791, train_loss:0.004152, train_acc:0.948409, test_loss:0.010344, test_acc:0.884635\n",
      "->epoch:792, train_loss:0.004073, train_acc:0.948756, test_loss:0.010811, test_acc:0.885938\n",
      "->epoch:793, train_loss:0.004138, train_acc:0.948712, test_loss:0.010139, test_acc:0.884896\n",
      "->epoch:794, train_loss:0.004003, train_acc:0.949146, test_loss:0.010515, test_acc:0.888802\n",
      "->epoch:795, train_loss:0.004097, train_acc:0.948278, test_loss:0.010913, test_acc:0.883073\n",
      "->epoch:796, train_loss:0.004054, train_acc:0.949248, test_loss:0.011444, test_acc:0.886328\n",
      "->epoch:797, train_loss:0.004099, train_acc:0.947092, test_loss:0.011286, test_acc:0.877734\n",
      "->epoch:798, train_loss:0.004053, train_acc:0.948785, test_loss:0.010967, test_acc:0.890234\n",
      "->epoch:799, train_loss:0.004085, train_acc:0.948770, test_loss:0.011489, test_acc:0.881250\n",
      "->epoch:800, train_loss:0.004128, train_acc:0.948206, test_loss:0.012238, test_acc:0.885547\n",
      "->epoch:801, train_loss:0.004062, train_acc:0.949841, test_loss:0.010608, test_acc:0.886328\n",
      "->epoch:802, train_loss:0.004059, train_acc:0.949638, test_loss:0.010287, test_acc:0.891276\n",
      "->epoch:803, train_loss:0.004139, train_acc:0.948336, test_loss:0.010638, test_acc:0.890104\n",
      "->epoch:804, train_loss:0.004056, train_acc:0.949595, test_loss:0.010698, test_acc:0.890755\n",
      "->epoch:805, train_loss:0.004065, train_acc:0.948958, test_loss:0.010813, test_acc:0.885807\n",
      "->epoch:806, train_loss:0.004115, train_acc:0.947598, test_loss:0.011230, test_acc:0.882031\n",
      "->epoch:807, train_loss:0.004034, train_acc:0.949551, test_loss:0.010689, test_acc:0.880208\n",
      "->epoch:808, train_loss:0.004171, train_acc:0.946021, test_loss:0.010790, test_acc:0.883724\n",
      "->epoch:809, train_loss:0.004083, train_acc:0.948582, test_loss:0.011146, test_acc:0.882292\n",
      "->epoch:810, train_loss:0.004008, train_acc:0.949682, test_loss:0.010384, test_acc:0.888542\n",
      "->epoch:811, train_loss:0.004142, train_acc:0.948148, test_loss:0.010245, test_acc:0.888672\n",
      "->epoch:812, train_loss:0.004105, train_acc:0.948654, test_loss:0.010223, test_acc:0.892839\n",
      "->epoch:813, train_loss:0.004090, train_acc:0.949436, test_loss:0.010217, test_acc:0.883724\n",
      "->epoch:814, train_loss:0.004079, train_acc:0.949161, test_loss:0.011203, test_acc:0.885547\n",
      "->epoch:815, train_loss:0.004118, train_acc:0.948220, test_loss:0.011210, test_acc:0.885807\n",
      "->epoch:816, train_loss:0.004029, train_acc:0.949653, test_loss:0.010658, test_acc:0.886458\n",
      "->epoch:817, train_loss:0.004021, train_acc:0.948712, test_loss:0.011564, test_acc:0.885287\n",
      "->epoch:818, train_loss:0.004108, train_acc:0.947627, test_loss:0.010706, test_acc:0.885807\n",
      "->epoch:819, train_loss:0.004159, train_acc:0.947714, test_loss:0.011327, test_acc:0.881250\n",
      "->epoch:820, train_loss:0.003986, train_acc:0.950289, test_loss:0.011241, test_acc:0.882031\n",
      "->epoch:821, train_loss:0.004054, train_acc:0.948900, test_loss:0.010134, test_acc:0.893359\n",
      "->epoch:822, train_loss:0.004071, train_acc:0.948929, test_loss:0.011609, test_acc:0.887760\n",
      "->epoch:823, train_loss:0.004140, train_acc:0.948698, test_loss:0.011177, test_acc:0.889844\n",
      "->epoch:824, train_loss:0.004097, train_acc:0.947902, test_loss:0.012275, test_acc:0.884635\n",
      "->epoch:825, train_loss:0.004030, train_acc:0.949855, test_loss:0.011286, test_acc:0.883854\n",
      "->epoch:826, train_loss:0.004123, train_acc:0.948553, test_loss:0.010782, test_acc:0.887630\n",
      "->epoch:827, train_loss:0.004050, train_acc:0.950217, test_loss:0.010820, test_acc:0.887760\n",
      "->epoch:828, train_loss:0.004059, train_acc:0.949899, test_loss:0.010810, test_acc:0.883984\n",
      "->epoch:829, train_loss:0.004055, train_acc:0.949682, test_loss:0.010177, test_acc:0.886849\n",
      "->epoch:830, train_loss:0.004042, train_acc:0.949277, test_loss:0.010853, test_acc:0.884375\n",
      "->epoch:831, train_loss:0.004041, train_acc:0.949523, test_loss:0.011042, test_acc:0.885807\n",
      "->epoch:832, train_loss:0.004056, train_acc:0.949175, test_loss:0.010737, test_acc:0.889974\n",
      "->epoch:833, train_loss:0.004053, train_acc:0.949407, test_loss:0.010846, test_acc:0.888412\n",
      "->epoch:834, train_loss:0.004081, train_acc:0.948770, test_loss:0.010389, test_acc:0.894271\n",
      "->epoch:835, train_loss:0.003934, train_acc:0.950448, test_loss:0.010660, test_acc:0.889974\n",
      "->epoch:836, train_loss:0.004029, train_acc:0.950203, test_loss:0.010157, test_acc:0.886849\n",
      "->epoch:837, train_loss:0.004019, train_acc:0.949580, test_loss:0.010726, test_acc:0.884766\n",
      "->epoch:838, train_loss:0.004063, train_acc:0.948944, test_loss:0.011211, test_acc:0.880990\n",
      "->epoch:839, train_loss:0.004122, train_acc:0.948481, test_loss:0.010616, test_acc:0.885547\n",
      "->epoch:840, train_loss:0.004031, train_acc:0.949768, test_loss:0.010521, test_acc:0.886719\n",
      "->epoch:841, train_loss:0.004068, train_acc:0.949407, test_loss:0.010907, test_acc:0.884505\n",
      "->epoch:842, train_loss:0.004002, train_acc:0.949016, test_loss:0.010806, test_acc:0.886589\n",
      "->epoch:843, train_loss:0.004086, train_acc:0.949103, test_loss:0.010804, test_acc:0.880208\n",
      "->epoch:844, train_loss:0.004093, train_acc:0.949248, test_loss:0.011012, test_acc:0.883854\n",
      "->epoch:845, train_loss:0.004077, train_acc:0.948712, test_loss:0.011067, test_acc:0.887240\n",
      "->epoch:846, train_loss:0.004041, train_acc:0.949016, test_loss:0.011068, test_acc:0.891537\n",
      "->epoch:847, train_loss:0.003958, train_acc:0.950622, test_loss:0.011094, test_acc:0.883464\n",
      "->epoch:848, train_loss:0.004038, train_acc:0.949349, test_loss:0.011737, test_acc:0.887760\n",
      "->epoch:849, train_loss:0.004036, train_acc:0.949291, test_loss:0.011814, test_acc:0.884375\n",
      "->epoch:850, train_loss:0.004108, train_acc:0.949016, test_loss:0.010786, test_acc:0.884635\n",
      "->epoch:851, train_loss:0.004169, train_acc:0.947786, test_loss:0.010552, test_acc:0.887240\n",
      "->epoch:852, train_loss:0.004176, train_acc:0.948539, test_loss:0.010394, test_acc:0.893880\n",
      "->epoch:853, train_loss:0.004008, train_acc:0.948843, test_loss:0.011342, test_acc:0.883594\n",
      "->epoch:854, train_loss:0.004038, train_acc:0.948727, test_loss:0.010372, test_acc:0.885287\n",
      "->epoch:855, train_loss:0.004041, train_acc:0.948756, test_loss:0.011950, test_acc:0.885417\n",
      "->epoch:856, train_loss:0.004109, train_acc:0.948177, test_loss:0.012098, test_acc:0.883984\n",
      "->epoch:857, train_loss:0.004109, train_acc:0.948278, test_loss:0.010367, test_acc:0.883854\n",
      "->epoch:858, train_loss:0.004064, train_acc:0.948452, test_loss:0.010754, test_acc:0.884115\n",
      "->epoch:859, train_loss:0.004157, train_acc:0.947425, test_loss:0.011517, test_acc:0.889583\n",
      "->epoch:860, train_loss:0.003961, train_acc:0.950825, test_loss:0.010274, test_acc:0.888542\n",
      "->epoch:861, train_loss:0.004027, train_acc:0.950680, test_loss:0.011417, test_acc:0.885677\n",
      "->epoch:862, train_loss:0.004094, train_acc:0.947873, test_loss:0.010365, test_acc:0.887760\n",
      "->epoch:863, train_loss:0.004130, train_acc:0.948597, test_loss:0.011261, test_acc:0.885938\n",
      "->epoch:864, train_loss:0.004004, train_acc:0.950434, test_loss:0.010891, test_acc:0.889323\n",
      "->epoch:865, train_loss:0.004173, train_acc:0.948698, test_loss:0.010876, test_acc:0.886068\n",
      "->epoch:866, train_loss:0.004046, train_acc:0.950130, test_loss:0.011295, test_acc:0.889323\n",
      "->epoch:867, train_loss:0.004042, train_acc:0.949725, test_loss:0.010858, test_acc:0.882031\n",
      "->epoch:868, train_loss:0.004021, train_acc:0.950217, test_loss:0.010328, test_acc:0.886068\n",
      "->epoch:869, train_loss:0.004016, train_acc:0.949928, test_loss:0.010960, test_acc:0.880859\n",
      "->epoch:870, train_loss:0.004089, train_acc:0.948438, test_loss:0.010972, test_acc:0.886719\n",
      "->epoch:871, train_loss:0.004087, train_acc:0.949103, test_loss:0.010831, test_acc:0.880078\n",
      "->epoch:872, train_loss:0.004043, train_acc:0.949436, test_loss:0.010416, test_acc:0.887500\n",
      "->epoch:873, train_loss:0.004106, train_acc:0.947946, test_loss:0.010425, test_acc:0.891016\n",
      "->epoch:874, train_loss:0.004051, train_acc:0.948307, test_loss:0.011561, test_acc:0.885807\n",
      "->epoch:875, train_loss:0.003996, train_acc:0.950665, test_loss:0.010884, test_acc:0.888932\n",
      "->epoch:876, train_loss:0.004137, train_acc:0.948076, test_loss:0.010922, test_acc:0.890885\n",
      "->epoch:877, train_loss:0.004003, train_acc:0.950087, test_loss:0.011205, test_acc:0.888542\n",
      "->epoch:878, train_loss:0.004120, train_acc:0.948727, test_loss:0.011537, test_acc:0.885026\n",
      "->epoch:879, train_loss:0.004076, train_acc:0.948886, test_loss:0.011537, test_acc:0.885156\n",
      "->epoch:880, train_loss:0.004088, train_acc:0.947989, test_loss:0.011270, test_acc:0.886849\n",
      "->epoch:881, train_loss:0.004069, train_acc:0.949248, test_loss:0.011154, test_acc:0.886849\n",
      "->epoch:882, train_loss:0.004027, train_acc:0.950043, test_loss:0.011442, test_acc:0.886068\n",
      "->epoch:883, train_loss:0.004036, train_acc:0.949117, test_loss:0.011112, test_acc:0.884505\n",
      "->epoch:884, train_loss:0.004108, train_acc:0.948669, test_loss:0.010280, test_acc:0.886458\n",
      "->epoch:885, train_loss:0.003979, train_acc:0.949551, test_loss:0.010698, test_acc:0.885547\n",
      "->epoch:886, train_loss:0.004058, train_acc:0.948828, test_loss:0.011143, test_acc:0.884896\n",
      "->epoch:887, train_loss:0.003946, train_acc:0.950694, test_loss:0.010985, test_acc:0.886589\n",
      "->epoch:888, train_loss:0.004100, train_acc:0.948669, test_loss:0.010693, test_acc:0.890104\n",
      "->epoch:889, train_loss:0.004044, train_acc:0.949740, test_loss:0.011383, test_acc:0.883464\n",
      "->epoch:890, train_loss:0.004024, train_acc:0.949306, test_loss:0.010955, test_acc:0.883594\n",
      "->epoch:891, train_loss:0.004012, train_acc:0.949494, test_loss:0.011328, test_acc:0.881250\n",
      "->epoch:892, train_loss:0.004031, train_acc:0.949768, test_loss:0.010918, test_acc:0.888412\n",
      "->epoch:893, train_loss:0.003996, train_acc:0.950376, test_loss:0.010963, test_acc:0.885156\n",
      "->epoch:894, train_loss:0.004110, train_acc:0.949002, test_loss:0.010924, test_acc:0.886198\n",
      "->epoch:895, train_loss:0.004108, train_acc:0.948770, test_loss:0.010118, test_acc:0.889323\n",
      "->epoch:896, train_loss:0.004050, train_acc:0.949624, test_loss:0.011388, test_acc:0.879297\n",
      "->epoch:897, train_loss:0.004071, train_acc:0.949161, test_loss:0.010890, test_acc:0.886328\n",
      "->epoch:898, train_loss:0.004052, train_acc:0.950246, test_loss:0.011472, test_acc:0.886979\n",
      "->epoch:899, train_loss:0.003957, train_acc:0.950680, test_loss:0.011563, test_acc:0.888281\n",
      "->epoch:900, train_loss:0.004055, train_acc:0.949884, test_loss:0.011459, test_acc:0.881510\n",
      "->epoch:901, train_loss:0.004050, train_acc:0.949841, test_loss:0.011280, test_acc:0.887630\n",
      "->epoch:902, train_loss:0.003995, train_acc:0.949986, test_loss:0.011412, test_acc:0.888672\n",
      "->epoch:903, train_loss:0.004013, train_acc:0.950043, test_loss:0.010772, test_acc:0.888802\n",
      "->epoch:904, train_loss:0.004021, train_acc:0.950318, test_loss:0.011079, test_acc:0.885547\n",
      "->epoch:905, train_loss:0.004043, train_acc:0.950203, test_loss:0.010713, test_acc:0.890755\n",
      "->epoch:906, train_loss:0.004055, train_acc:0.948915, test_loss:0.011196, test_acc:0.888281\n",
      "->epoch:907, train_loss:0.003913, train_acc:0.951910, test_loss:0.011833, test_acc:0.888932\n",
      "->epoch:908, train_loss:0.004047, train_acc:0.949913, test_loss:0.011126, test_acc:0.883464\n",
      "->epoch:909, train_loss:0.004061, train_acc:0.949190, test_loss:0.011824, test_acc:0.879818\n",
      "->epoch:910, train_loss:0.004067, train_acc:0.949349, test_loss:0.010417, test_acc:0.887630\n",
      "->epoch:911, train_loss:0.004068, train_acc:0.948510, test_loss:0.011343, test_acc:0.886719\n",
      "->epoch:912, train_loss:0.004013, train_acc:0.949378, test_loss:0.010862, test_acc:0.887891\n",
      "->epoch:913, train_loss:0.003899, train_acc:0.951128, test_loss:0.011298, test_acc:0.888932\n",
      "->epoch:914, train_loss:0.003980, train_acc:0.950174, test_loss:0.010867, test_acc:0.889714\n",
      "->epoch:915, train_loss:0.003978, train_acc:0.950564, test_loss:0.011244, test_acc:0.884635\n",
      "->epoch:916, train_loss:0.004002, train_acc:0.950911, test_loss:0.010949, test_acc:0.880729\n",
      "->epoch:917, train_loss:0.004051, train_acc:0.949363, test_loss:0.010455, test_acc:0.885287\n",
      "->epoch:918, train_loss:0.003984, train_acc:0.950333, test_loss:0.010703, test_acc:0.887760\n",
      "->epoch:919, train_loss:0.004047, train_acc:0.950058, test_loss:0.010502, test_acc:0.882161\n",
      "->epoch:920, train_loss:0.004031, train_acc:0.949161, test_loss:0.011593, test_acc:0.889583\n",
      "->epoch:921, train_loss:0.004045, train_acc:0.950333, test_loss:0.010698, test_acc:0.885026\n",
      "->epoch:922, train_loss:0.004038, train_acc:0.948958, test_loss:0.010885, test_acc:0.889453\n",
      "->epoch:923, train_loss:0.004026, train_acc:0.949870, test_loss:0.011107, test_acc:0.882813\n",
      "->epoch:924, train_loss:0.004046, train_acc:0.949392, test_loss:0.010672, test_acc:0.882943\n",
      "->epoch:925, train_loss:0.004003, train_acc:0.949740, test_loss:0.011035, test_acc:0.882943\n",
      "->epoch:926, train_loss:0.004028, train_acc:0.949971, test_loss:0.011554, test_acc:0.885547\n",
      "->epoch:927, train_loss:0.004049, train_acc:0.948944, test_loss:0.010945, test_acc:0.889193\n",
      "->epoch:928, train_loss:0.003948, train_acc:0.951056, test_loss:0.011504, test_acc:0.888542\n",
      "->epoch:929, train_loss:0.004047, train_acc:0.950174, test_loss:0.011242, test_acc:0.885156\n",
      "->epoch:930, train_loss:0.003954, train_acc:0.950506, test_loss:0.010880, test_acc:0.883594\n",
      "->epoch:931, train_loss:0.004020, train_acc:0.949320, test_loss:0.010913, test_acc:0.884115\n",
      "->epoch:932, train_loss:0.004006, train_acc:0.950564, test_loss:0.011150, test_acc:0.887891\n",
      "->epoch:933, train_loss:0.004095, train_acc:0.948466, test_loss:0.010595, test_acc:0.885547\n",
      "->epoch:934, train_loss:0.003963, train_acc:0.951013, test_loss:0.011777, test_acc:0.881901\n",
      "->epoch:935, train_loss:0.003933, train_acc:0.950752, test_loss:0.010933, test_acc:0.885807\n",
      "->epoch:936, train_loss:0.004045, train_acc:0.948640, test_loss:0.010943, test_acc:0.890755\n",
      "->epoch:937, train_loss:0.004008, train_acc:0.950217, test_loss:0.010792, test_acc:0.885287\n",
      "->epoch:938, train_loss:0.004009, train_acc:0.949219, test_loss:0.010877, test_acc:0.883464\n",
      "->epoch:939, train_loss:0.003944, train_acc:0.950796, test_loss:0.011444, test_acc:0.888412\n",
      "->epoch:940, train_loss:0.004063, train_acc:0.949537, test_loss:0.010517, test_acc:0.886198\n",
      "->epoch:941, train_loss:0.003926, train_acc:0.951100, test_loss:0.010335, test_acc:0.895443\n",
      "->epoch:942, train_loss:0.004037, train_acc:0.949826, test_loss:0.011401, test_acc:0.884766\n",
      "->epoch:943, train_loss:0.004019, train_acc:0.949175, test_loss:0.011132, test_acc:0.888932\n",
      "->epoch:944, train_loss:0.003954, train_acc:0.950477, test_loss:0.011033, test_acc:0.889714\n",
      "->epoch:945, train_loss:0.003958, train_acc:0.951114, test_loss:0.010452, test_acc:0.889974\n",
      "->epoch:946, train_loss:0.003905, train_acc:0.951866, test_loss:0.011440, test_acc:0.886719\n",
      "->epoch:947, train_loss:0.004073, train_acc:0.949682, test_loss:0.010419, test_acc:0.888672\n",
      "->epoch:948, train_loss:0.003924, train_acc:0.951215, test_loss:0.010187, test_acc:0.889844\n",
      "->epoch:949, train_loss:0.004053, train_acc:0.949392, test_loss:0.010835, test_acc:0.887240\n",
      "->epoch:950, train_loss:0.004017, train_acc:0.950072, test_loss:0.010694, test_acc:0.893359\n",
      "->epoch:951, train_loss:0.003909, train_acc:0.950593, test_loss:0.010988, test_acc:0.889063\n",
      "->epoch:952, train_loss:0.003945, train_acc:0.950839, test_loss:0.010357, test_acc:0.889714\n",
      "->epoch:953, train_loss:0.004086, train_acc:0.948568, test_loss:0.010640, test_acc:0.887240\n",
      "->epoch:954, train_loss:0.004006, train_acc:0.950680, test_loss:0.010759, test_acc:0.882813\n",
      "->epoch:955, train_loss:0.004101, train_acc:0.949146, test_loss:0.011166, test_acc:0.890104\n",
      "->epoch:956, train_loss:0.003990, train_acc:0.950477, test_loss:0.010217, test_acc:0.886458\n",
      "->epoch:957, train_loss:0.004012, train_acc:0.949566, test_loss:0.011212, test_acc:0.883724\n",
      "->epoch:958, train_loss:0.003975, train_acc:0.950159, test_loss:0.010948, test_acc:0.885677\n",
      "->epoch:959, train_loss:0.004002, train_acc:0.949335, test_loss:0.010924, test_acc:0.881510\n",
      "->epoch:960, train_loss:0.004003, train_acc:0.950101, test_loss:0.010748, test_acc:0.887891\n",
      "->epoch:961, train_loss:0.003905, train_acc:0.952141, test_loss:0.011613, test_acc:0.886849\n",
      "->epoch:962, train_loss:0.004006, train_acc:0.949986, test_loss:0.010963, test_acc:0.889453\n",
      "->epoch:963, train_loss:0.004020, train_acc:0.949797, test_loss:0.010960, test_acc:0.890104\n",
      "->epoch:964, train_loss:0.003965, train_acc:0.950825, test_loss:0.011112, test_acc:0.884766\n",
      "->epoch:965, train_loss:0.004133, train_acc:0.947700, test_loss:0.010451, test_acc:0.886068\n",
      "->epoch:966, train_loss:0.003986, train_acc:0.951143, test_loss:0.011342, test_acc:0.881641\n",
      "->epoch:967, train_loss:0.003955, train_acc:0.951244, test_loss:0.010736, test_acc:0.886719\n",
      "->epoch:968, train_loss:0.003992, train_acc:0.949812, test_loss:0.011238, test_acc:0.882422\n",
      "->epoch:969, train_loss:0.003970, train_acc:0.950318, test_loss:0.011406, test_acc:0.889323\n",
      "->epoch:970, train_loss:0.003957, train_acc:0.950260, test_loss:0.010212, test_acc:0.890234\n",
      "->epoch:971, train_loss:0.004003, train_acc:0.950029, test_loss:0.010781, test_acc:0.883464\n",
      "->epoch:972, train_loss:0.003964, train_acc:0.950781, test_loss:0.010644, test_acc:0.885417\n",
      "->epoch:973, train_loss:0.003936, train_acc:0.950637, test_loss:0.010973, test_acc:0.889453\n",
      "->epoch:974, train_loss:0.003959, train_acc:0.950376, test_loss:0.010817, test_acc:0.886719\n",
      "->epoch:975, train_loss:0.003980, train_acc:0.950260, test_loss:0.010998, test_acc:0.882552\n",
      "->epoch:976, train_loss:0.003929, train_acc:0.950796, test_loss:0.010454, test_acc:0.889193\n",
      "->epoch:977, train_loss:0.003985, train_acc:0.950463, test_loss:0.011317, test_acc:0.886198\n",
      "->epoch:978, train_loss:0.003992, train_acc:0.949436, test_loss:0.011450, test_acc:0.877214\n",
      "->epoch:979, train_loss:0.004057, train_acc:0.949306, test_loss:0.010302, test_acc:0.885807\n",
      "->epoch:980, train_loss:0.003995, train_acc:0.949609, test_loss:0.011079, test_acc:0.887500\n",
      "->epoch:981, train_loss:0.003936, train_acc:0.949609, test_loss:0.011192, test_acc:0.888151\n",
      "->epoch:982, train_loss:0.004052, train_acc:0.950174, test_loss:0.010255, test_acc:0.885417\n",
      "->epoch:983, train_loss:0.004037, train_acc:0.949407, test_loss:0.010842, test_acc:0.884896\n",
      "->epoch:984, train_loss:0.004023, train_acc:0.949653, test_loss:0.010622, test_acc:0.889453\n",
      "->epoch:985, train_loss:0.004016, train_acc:0.949899, test_loss:0.012212, test_acc:0.882682\n",
      "->epoch:986, train_loss:0.003984, train_acc:0.950680, test_loss:0.011208, test_acc:0.886328\n",
      "->epoch:987, train_loss:0.003970, train_acc:0.950463, test_loss:0.011242, test_acc:0.883333\n",
      "->epoch:988, train_loss:0.003997, train_acc:0.949392, test_loss:0.010171, test_acc:0.887760\n",
      "->epoch:989, train_loss:0.004008, train_acc:0.949754, test_loss:0.011892, test_acc:0.880729\n",
      "->epoch:990, train_loss:0.004004, train_acc:0.950593, test_loss:0.011131, test_acc:0.888281\n",
      "->epoch:991, train_loss:0.004132, train_acc:0.948929, test_loss:0.010560, test_acc:0.883854\n",
      "->epoch:992, train_loss:0.003970, train_acc:0.950043, test_loss:0.011182, test_acc:0.885417\n",
      "->epoch:993, train_loss:0.003930, train_acc:0.951288, test_loss:0.012589, test_acc:0.885547\n",
      "->epoch:994, train_loss:0.004014, train_acc:0.949190, test_loss:0.013203, test_acc:0.882552\n",
      "->epoch:995, train_loss:0.004046, train_acc:0.949913, test_loss:0.011198, test_acc:0.883594\n",
      "->epoch:996, train_loss:0.004029, train_acc:0.950203, test_loss:0.010805, test_acc:0.886589\n",
      "->epoch:997, train_loss:0.004025, train_acc:0.950275, test_loss:0.011087, test_acc:0.882422\n",
      "->epoch:998, train_loss:0.004022, train_acc:0.950564, test_loss:0.010416, test_acc:0.889453\n",
      "->epoch:999, train_loss:0.003997, train_acc:0.950680, test_loss:0.010358, test_acc:0.887109\n",
      "->epoch:1000, train_loss:0.004051, train_acc:0.948756, test_loss:0.010614, test_acc:0.888021\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f'->epoch:{epoch + 1}', end = ', ')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim)\n",
    "#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac519be-9424-45e4-99b3-c4f7469fb026",
   "metadata": {},
   "source": [
    "两层MKP组都加BN，->epoch:145, train_loss:0.000663, train_acc:0.992737, test_loss:0.010613, test_acc:0.934896"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa21ed2-da43-4dc3-ad70-081231e98ef4",
   "metadata": {},
   "source": [
    "# 增加模型容量\n",
    "- ->epoch:86, train_loss:0.002378, train_acc:0.980787, test_loss:0.017223, test_acc:0.908203\n",
    "- 改为heads=3， ->epoch:167, train_loss:0.001960, train_acc:0.985171, test_loss:0.022945, test_acc:0.910417"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964d82-e4ca-40db-ad83-0722b8a6b880",
   "metadata": {},
   "source": [
    "## 比较实验\n",
    "### GCN\n",
    "+ 仅包括自环时，->epoch:25, train_loss:0.000922, train_acc:0.990784, test_loss:0.010875, test_acc:0.919401\n",
    "+ 加上3x3卷积核的邻接边时，->epoch:32, train_loss:0.000819, train_acc:0.992173, test_loss:0.020655, test_acc:0.895313，邻接边设计的不好，限制了模型的发挥\n",
    "+ 别人的方法的准确率：89/90、93/94\n",
    "### GAT\n",
    "+ 仅包括自环时，->epoch:30, train_loss:0.001252, train_acc:0.986531, test_loss:0.015308, test_acc:0.912630\n",
    "+ 使用自己设计的边，->epoch:123, train_loss:0.002104, train_acc:0.982161, test_loss:0.029411, test_acc:0.904688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f4ff-9fbb-4323-ad0d-4dfc7848e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
