{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6df28-9919-4bff-8e9a-ea82c35eba11",
   "metadata": {},
   "source": [
    "### 脑电图注意力网络（GAT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def data_split(train_ratio=0.7):\n",
    "#     load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "#     trials = np.load(load_dir + 'trials.npy')\n",
    "#     bases = np.load(load_dir + 'bases.npy')\n",
    "#     labels = np.load(load_dir + 'labels.npy')\n",
    "#     # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "#     # 去基线\n",
    "#     for i, base in enumerate(bases):\n",
    "#         trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "#     # 离散化标签\n",
    "#     labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "#     # 复制标签以对齐样本\n",
    "#     labels = np.repeat(labels, 60, axis = 0)\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     shuffle_list = np.arange(trials.shape[0])\n",
    "#     np.random.shuffle(shuffle_list)\n",
    "#     trials = trials[shuffle_list]\n",
    "#     labels = labels[shuffle_list]\n",
    "    \n",
    "#     cut_point = int(trials.shape[0] * train_ratio)\n",
    "#     train_features, train_labels = trials[:cut_point], labels[:cut_point]\n",
    "#     test_features, test_labels = trials[cut_point:], labels[cut_point:]\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32 * 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32 * 128))\n",
    "    \n",
    "#     mean = train_features.mean(axis = 0)\n",
    "#     std = train_features.std(axis = 0)\n",
    "    \n",
    "#     train_features = (train_features - mean) / std\n",
    "#     test_features = (test_features - mean) / std\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32, 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32, 128))\n",
    "    \n",
    "#     save_dir = 'data/data_split/'\n",
    "#     np.save(save_dir + 'train_features.npy', train_features)\n",
    "#     np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "#     np.save(save_dir + 'test_features.npy', test_features)\n",
    "#     np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "# data_split(train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b5e808-1250-4784-b6b7-a0e58d86017a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if create_complete_graph:\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        return edge_index\n",
    "    \n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd42b5bf-ca24-4f71-95ad-b0b494aaa18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcb8368-b508-41fb-baa4-5e35d16ff337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  1  0 17  0  0  0]\n",
      " [ 0  0  0  2  0 18  0  0  0]\n",
      " [ 4  0  3  0 19  0 20  0 21]\n",
      " [ 0  5  0  6  0 23  0 22  0]\n",
      " [ 8  0  7  0 24  0 25  0 26]\n",
      " [ 0  9  0 10  0 28  0 27  0]\n",
      " [12  0 11  0 16  0 29  0 30]\n",
      " [ 0  0  0 13  0 31  0  0  0]\n",
      " [ 0  0  0 14 15 32  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x152dd3b2148>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_index = [[],[]]\n",
    "weight = []\n",
    "\n",
    "#用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "              9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "              17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "                25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "for k, v in chan_to_1020.items():\n",
    "    maps[v[0]][v[1]] = k + 1\n",
    "print(maps)\n",
    "plt.matshow(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    edge_index = None\n",
    "    def __init__(self, root, is_train_data, edge_index):\n",
    "        self.is_train_data = is_train_data\n",
    "        self.edge_index = edge_index\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['train.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor(labels[i].reshape(1, -1), dtype=torch.long)\n",
    "            data = Data(x = x, edge_index=self.edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(in_channels=embed_dim, out_channels=256, heads=3, concat=True, dropout=0.0, add_self_loops=False)\n",
    "        self.conv2 = GATConv(in_channels=256 * 3, out_channels=256, heads=3, concat=True, dropout=0.0, add_self_loops=False)\n",
    "        self.conv3 = GATConv(in_channels=256, out_channels=256, heads=1, concat=True, dropout=0.0, add_self_loops=False)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(8192 * 3, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1280)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(640)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # x = self.item_embedding(x)\n",
    "        # x = x.squeeze(1) # n*128\n",
    "        x, attention_weights = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        # x, edeg_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch) # pool 之后得到 n*128个点\n",
    "        # x1 = gap(x, batch)\n",
    "        x, attention_weights = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "#         x, attention_weights = self.conv3(x, edge_index, return_attention_weights=True) 使用三层GATConv，测试集准确率只能达到86左右。\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        # x, edeg_index, _, batch, _, _ = self.pool2(x, edeg_index, None, batch)\n",
    "        # x2 = gap(x, batch)\n",
    "        # x = F.relu(self.conv3(x, edge_index))\n",
    "        # x, edeg_index, _, batch, _, _ = self.pool3(x, edeg_index, None, batch)\n",
    "        # x3 = gap(x, batch)\n",
    "        # x = x1 + x2 + x3 # 获取不同尺度的全局特征\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emo_dim):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        train_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_train_sample = len(trainDataLoader.dataset)\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    train_acc = train_acc / num_train_sample\n",
    "    \n",
    "    # check测试集的性能\n",
    "    vali_loss = 0\n",
    "    vali_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in testDataLoader:\n",
    "        batch.to(device)\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        vali_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        vali_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_test_sample = len(testDataLoader.dataset)\n",
    "    vali_loss = vali_loss / num_test_sample\n",
    "    vali_acc = vali_acc / num_test_sample\n",
    "    \n",
    "    print(f'train_loss:{train_loss:.6f}, train_acc:{train_acc:.6f}, test_loss:{vali_loss:.6f}, test_acc:{vali_acc:.6f}')\n",
    "    \n",
    "    return train_loss, train_acc, vali_loss, vali_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9bf77-c20d-484b-932b-8b98917f374b",
   "metadata": {},
   "source": [
    "# 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1e1611-fac8-4210-b800-bf02914ed6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete_graph = False\n",
    "self_loop_only = False\n",
    "emo_dim = 0\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "edge_index = get_edge_index(create_complete_graph=create_complete_graph, self_loop_only=self_loop_only)\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, edge_index=edge_index)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, edge_index=edge_index)\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3f3d55-fc83-475e-a697-9b8a12789830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "crit = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:1, train_loss:0.020056, train_acc:0.626317, test_loss:0.017321, test_acc:0.713151\n",
      "->epoch:2, train_loss:0.014672, train_acc:0.772613, test_loss:0.013013, test_acc:0.795964\n",
      "->epoch:3, train_loss:0.010619, train_acc:0.846976, test_loss:0.010897, test_acc:0.843620\n",
      "->epoch:4, train_loss:0.008253, train_acc:0.885069, test_loss:0.010686, test_acc:0.849089\n",
      "->epoch:5, train_loss:0.007113, train_acc:0.900854, test_loss:0.009784, test_acc:0.860156\n",
      "->epoch:6, train_loss:0.006125, train_acc:0.916059, test_loss:0.010032, test_acc:0.863672\n",
      "->epoch:7, train_loss:0.005526, train_acc:0.923958, test_loss:0.009487, test_acc:0.874349\n",
      "->epoch:8, train_loss:0.004576, train_acc:0.938281, test_loss:0.012409, test_acc:0.877214\n",
      "->epoch:9, train_loss:0.004217, train_acc:0.944647, test_loss:0.009635, test_acc:0.883203\n",
      "->epoch:10, train_loss:0.003857, train_acc:0.948944, test_loss:0.010504, test_acc:0.868359\n",
      "->epoch:11, train_loss:0.003648, train_acc:0.952633, test_loss:0.009541, test_acc:0.883854\n",
      "->epoch:12, train_loss:0.003362, train_acc:0.956597, test_loss:0.010338, test_acc:0.887109\n",
      "->epoch:13, train_loss:0.003322, train_acc:0.957899, test_loss:0.010631, test_acc:0.884766\n",
      "->epoch:14, train_loss:0.002892, train_acc:0.964106, test_loss:0.010213, test_acc:0.888932\n",
      "->epoch:15, train_loss:0.002688, train_acc:0.967072, test_loss:0.011464, test_acc:0.890625\n",
      "->epoch:16, train_loss:0.002589, train_acc:0.969155, test_loss:0.013379, test_acc:0.888932\n",
      "->epoch:17, train_loss:0.002527, train_acc:0.970182, test_loss:0.011013, test_acc:0.891146\n",
      "->epoch:18, train_loss:0.002304, train_acc:0.972121, test_loss:0.011869, test_acc:0.893359\n",
      "->epoch:19, train_loss:0.002244, train_acc:0.974653, test_loss:0.012083, test_acc:0.888542\n",
      "->epoch:20, train_loss:0.002217, train_acc:0.974479, test_loss:0.010745, test_acc:0.888932\n",
      "->epoch:21, train_loss:0.002265, train_acc:0.973597, test_loss:0.012020, test_acc:0.892318\n",
      "->epoch:22, train_loss:0.002275, train_acc:0.972844, test_loss:0.012521, test_acc:0.892578\n",
      "->epoch:23, train_loss:0.001887, train_acc:0.978400, test_loss:0.011410, test_acc:0.897526\n",
      "->epoch:24, train_loss:0.002024, train_acc:0.977691, test_loss:0.013336, test_acc:0.893490\n",
      "->epoch:25, train_loss:0.001840, train_acc:0.978733, test_loss:0.012764, test_acc:0.889453\n",
      "->epoch:26, train_loss:0.001751, train_acc:0.980469, test_loss:0.014110, test_acc:0.896224\n",
      "->epoch:27, train_loss:0.002075, train_acc:0.976548, test_loss:0.008401, test_acc:0.889974\n",
      "->epoch:28, train_loss:0.002095, train_acc:0.976273, test_loss:0.013589, test_acc:0.901042\n",
      "->epoch:29, train_loss:0.001981, train_acc:0.976317, test_loss:0.013859, test_acc:0.896224\n",
      "->epoch:30, train_loss:0.002025, train_acc:0.977474, test_loss:0.013456, test_acc:0.897917\n",
      "->epoch:31, train_loss:0.002276, train_acc:0.973886, test_loss:0.014003, test_acc:0.892708\n",
      "->epoch:32, train_loss:0.001816, train_acc:0.978660, test_loss:0.015415, test_acc:0.899609\n",
      "->epoch:33, train_loss:0.001796, train_acc:0.978530, test_loss:0.010247, test_acc:0.897135\n",
      "->epoch:34, train_loss:0.001715, train_acc:0.979890, test_loss:0.019684, test_acc:0.897396\n",
      "->epoch:35, train_loss:0.001993, train_acc:0.978516, test_loss:0.012396, test_acc:0.893620\n",
      "->epoch:36, train_loss:0.001928, train_acc:0.978646, test_loss:0.012603, test_acc:0.898307\n",
      "->epoch:37, train_loss:0.001985, train_acc:0.977604, test_loss:0.014031, test_acc:0.896615\n",
      "->epoch:38, train_loss:0.001837, train_acc:0.978299, test_loss:0.013596, test_acc:0.892188\n",
      "->epoch:39, train_loss:0.001784, train_acc:0.981207, test_loss:0.013147, test_acc:0.894661\n",
      "->epoch:40, train_loss:0.001772, train_acc:0.981018, test_loss:0.011456, test_acc:0.903906\n",
      "->epoch:41, train_loss:0.001442, train_acc:0.983348, test_loss:0.016408, test_acc:0.905599\n",
      "->epoch:42, train_loss:0.001527, train_acc:0.982986, test_loss:0.015378, test_acc:0.900000\n",
      "->epoch:43, train_loss:0.001827, train_acc:0.979688, test_loss:0.014386, test_acc:0.898828\n",
      "->epoch:44, train_loss:0.002059, train_acc:0.976505, test_loss:0.015753, test_acc:0.899740\n",
      "->epoch:45, train_loss:0.001561, train_acc:0.981438, test_loss:0.018939, test_acc:0.897786\n",
      "->epoch:46, train_loss:0.001821, train_acc:0.978762, test_loss:0.014286, test_acc:0.895052\n",
      "->epoch:47, train_loss:0.001809, train_acc:0.980498, test_loss:0.016693, test_acc:0.894401\n",
      "->epoch:48, train_loss:0.002031, train_acc:0.976215, test_loss:0.015924, test_acc:0.896354\n",
      "->epoch:49, train_loss:0.002047, train_acc:0.976244, test_loss:0.012984, test_acc:0.893880\n",
      "->epoch:50, train_loss:0.001643, train_acc:0.979803, test_loss:0.014573, test_acc:0.896875\n",
      "->epoch:51, train_loss:0.002263, train_acc:0.976042, test_loss:0.016024, test_acc:0.895703\n",
      "->epoch:52, train_loss:0.001670, train_acc:0.977937, test_loss:0.016170, test_acc:0.895964\n",
      "->epoch:53, train_loss:0.001767, train_acc:0.980628, test_loss:0.015274, test_acc:0.899349\n",
      "->epoch:54, train_loss:0.001566, train_acc:0.981626, test_loss:0.015938, test_acc:0.899089\n",
      "->epoch:55, train_loss:0.001775, train_acc:0.979369, test_loss:0.019047, test_acc:0.901172\n",
      "->epoch:56, train_loss:0.001803, train_acc:0.978993, test_loss:0.020547, test_acc:0.891667\n",
      "->epoch:57, train_loss:0.002028, train_acc:0.978443, test_loss:0.013548, test_acc:0.895182\n",
      "->epoch:58, train_loss:0.001346, train_acc:0.985026, test_loss:0.020501, test_acc:0.898698\n",
      "->epoch:59, train_loss:0.001550, train_acc:0.982089, test_loss:0.020132, test_acc:0.889844\n",
      "->epoch:60, train_loss:0.001896, train_acc:0.978110, test_loss:0.014567, test_acc:0.903646\n",
      "->epoch:61, train_loss:0.001542, train_acc:0.982986, test_loss:0.018347, test_acc:0.892318\n",
      "->epoch:62, train_loss:0.002013, train_acc:0.979094, test_loss:0.015690, test_acc:0.900000\n",
      "->epoch:63, train_loss:0.001534, train_acc:0.982812, test_loss:0.018924, test_acc:0.902083\n",
      "->epoch:64, train_loss:0.001977, train_acc:0.977387, test_loss:0.024117, test_acc:0.888021\n",
      "->epoch:65, train_loss:0.001994, train_acc:0.976201, test_loss:0.023700, test_acc:0.893620\n",
      "->epoch:66, train_loss:0.001688, train_acc:0.980396, test_loss:0.010988, test_acc:0.902474\n",
      "->epoch:67, train_loss:0.001400, train_acc:0.982350, test_loss:0.015438, test_acc:0.898568\n",
      "->epoch:68, train_loss:0.001898, train_acc:0.977170, test_loss:0.016061, test_acc:0.888151\n",
      "->epoch:69, train_loss:0.001719, train_acc:0.980845, test_loss:0.026925, test_acc:0.886198\n",
      "->epoch:70, train_loss:0.003657, train_acc:0.969676, test_loss:0.018086, test_acc:0.889323\n",
      "->epoch:71, train_loss:0.001622, train_acc:0.980932, test_loss:0.034110, test_acc:0.897917\n",
      "->epoch:72, train_loss:0.002564, train_acc:0.968663, test_loss:0.024718, test_acc:0.892969\n",
      "->epoch:73, train_loss:0.002191, train_acc:0.976273, test_loss:0.011680, test_acc:0.897786\n",
      "->epoch:74, train_loss:0.002521, train_acc:0.976172, test_loss:0.021789, test_acc:0.903776\n",
      "->epoch:75, train_loss:0.002276, train_acc:0.974624, test_loss:0.015075, test_acc:0.899089\n",
      "->epoch:76, train_loss:0.001931, train_acc:0.981959, test_loss:0.018057, test_acc:0.894141\n",
      "->epoch:77, train_loss:0.001304, train_acc:0.985344, test_loss:0.023043, test_acc:0.893620\n",
      "->epoch:78, train_loss:0.002222, train_acc:0.978588, test_loss:0.020428, test_acc:0.899740\n",
      "->epoch:79, train_loss:0.001627, train_acc:0.983435, test_loss:0.022791, test_acc:0.904297\n",
      "->epoch:80, train_loss:0.001883, train_acc:0.983362, test_loss:0.020113, test_acc:0.898698\n",
      "->epoch:81, train_loss:0.001660, train_acc:0.982914, test_loss:0.018842, test_acc:0.905729\n",
      "->epoch:82, train_loss:0.002347, train_acc:0.977445, test_loss:0.017676, test_acc:0.902734\n",
      "->epoch:83, train_loss:0.002287, train_acc:0.983550, test_loss:0.049234, test_acc:0.901042\n",
      "->epoch:84, train_loss:0.002343, train_acc:0.983001, test_loss:0.026056, test_acc:0.897526\n",
      "->epoch:85, train_loss:0.002473, train_acc:0.974103, test_loss:0.015307, test_acc:0.893229\n",
      "->epoch:86, train_loss:0.002378, train_acc:0.980787, test_loss:0.017223, test_acc:0.908203\n",
      "->epoch:87, train_loss:0.002380, train_acc:0.981236, test_loss:0.019064, test_acc:0.896094\n",
      "->epoch:88, train_loss:0.002916, train_acc:0.974508, test_loss:0.022124, test_acc:0.901302\n",
      "->epoch:89, train_loss:0.002241, train_acc:0.978921, test_loss:0.023230, test_acc:0.896094\n",
      "->epoch:90, train_loss:0.002310, train_acc:0.980310, test_loss:0.023618, test_acc:0.893099\n",
      "->epoch:91, train_loss:0.001814, train_acc:0.982422, test_loss:0.031325, test_acc:0.900521\n",
      "->epoch:92, train_loss:0.002551, train_acc:0.980121, test_loss:0.020736, test_acc:0.897786\n",
      "->epoch:93, train_loss:0.003218, train_acc:0.971007, test_loss:0.027972, test_acc:0.904427\n",
      "->epoch:94, train_loss:0.002147, train_acc:0.983738, test_loss:0.022106, test_acc:0.902734\n",
      "->epoch:95, train_loss:0.002370, train_acc:0.982856, test_loss:0.021447, test_acc:0.903776\n",
      "->epoch:96, train_loss:0.002445, train_acc:0.981438, test_loss:0.013838, test_acc:0.890495\n",
      "->epoch:97, train_loss:0.001826, train_acc:0.983970, test_loss:0.048088, test_acc:0.907552\n",
      "->epoch:98, train_loss:0.003843, train_acc:0.971832, test_loss:0.027033, test_acc:0.892578\n",
      "->epoch:99, train_loss:0.002588, train_acc:0.980339, test_loss:0.016471, test_acc:0.898047\n",
      "->epoch:100, train_loss:0.002204, train_acc:0.982089, test_loss:0.017434, test_acc:0.899609\n",
      "->epoch:101, train_loss:0.002377, train_acc:0.982306, test_loss:0.023524, test_acc:0.897786\n",
      "->epoch:102, train_loss:0.001870, train_acc:0.985561, test_loss:0.015409, test_acc:0.896224\n",
      "->epoch:103, train_loss:0.002355, train_acc:0.982610, test_loss:0.028882, test_acc:0.904427\n",
      "->epoch:104, train_loss:0.002472, train_acc:0.983362, test_loss:0.023883, test_acc:0.902734\n",
      "->epoch:105, train_loss:0.002197, train_acc:0.983521, test_loss:0.019556, test_acc:0.905208\n",
      "->epoch:106, train_loss:0.002108, train_acc:0.984621, test_loss:0.024504, test_acc:0.901953\n",
      "->epoch:107, train_loss:0.002143, train_acc:0.984375, test_loss:0.015482, test_acc:0.898438\n",
      "->epoch:108, train_loss:0.002124, train_acc:0.986690, test_loss:0.031956, test_acc:0.902865\n",
      "->epoch:109, train_loss:0.002070, train_acc:0.986241, test_loss:0.028417, test_acc:0.903776\n",
      "->epoch:110, train_loss:0.002047, train_acc:0.987746, test_loss:0.016043, test_acc:0.902083\n",
      "->epoch:111, train_loss:0.001797, train_acc:0.986126, test_loss:0.047900, test_acc:0.901302\n",
      "->epoch:112, train_loss:0.003114, train_acc:0.977185, test_loss:0.019231, test_acc:0.895573\n",
      "->epoch:113, train_loss:0.003194, train_acc:0.978139, test_loss:0.018494, test_acc:0.900912\n",
      "->epoch:114, train_loss:0.002056, train_acc:0.983738, test_loss:0.029630, test_acc:0.903516\n",
      "->epoch:115, train_loss:0.002007, train_acc:0.985706, test_loss:0.018171, test_acc:0.898438\n",
      "->epoch:116, train_loss:0.002702, train_acc:0.981308, test_loss:0.017148, test_acc:0.900521\n",
      "->epoch:117, train_loss:0.001965, train_acc:0.984983, test_loss:0.069135, test_acc:0.883333\n",
      "->epoch:118, train_loss:0.003431, train_acc:0.975145, test_loss:0.032524, test_acc:0.901172\n",
      "->epoch:119, train_loss:0.002361, train_acc:0.981496, test_loss:0.023232, test_acc:0.897786\n",
      "->epoch:120, train_loss:0.002256, train_acc:0.984230, test_loss:0.030996, test_acc:0.899219\n",
      "->epoch:121, train_loss:0.003086, train_acc:0.982161, test_loss:0.033094, test_acc:0.887109\n",
      "->epoch:122, train_loss:0.002688, train_acc:0.983044, test_loss:0.014258, test_acc:0.869922\n",
      "->epoch:123, train_loss:0.002184, train_acc:0.983507, test_loss:0.028165, test_acc:0.901953\n",
      "->epoch:124, train_loss:0.002134, train_acc:0.983840, test_loss:0.017524, test_acc:0.891537\n",
      "->epoch:125, train_loss:0.002537, train_acc:0.980773, test_loss:0.031579, test_acc:0.898828\n",
      "->epoch:126, train_loss:0.002297, train_acc:0.981727, test_loss:0.034034, test_acc:0.903776\n",
      "->epoch:127, train_loss:0.001977, train_acc:0.985764, test_loss:0.020403, test_acc:0.907162\n",
      "->epoch:128, train_loss:0.002208, train_acc:0.983782, test_loss:0.023911, test_acc:0.906771\n",
      "->epoch:129, train_loss:0.002156, train_acc:0.984100, test_loss:0.018072, test_acc:0.901432\n",
      "->epoch:130, train_loss:0.002473, train_acc:0.983970, test_loss:0.048021, test_acc:0.901563\n",
      "->epoch:131, train_loss:0.002390, train_acc:0.983304, test_loss:0.024983, test_acc:0.894010\n",
      "->epoch:132, train_loss:0.002347, train_acc:0.982407, test_loss:0.020986, test_acc:0.897526\n",
      "->epoch:133, train_loss:0.002024, train_acc:0.986227, test_loss:0.024992, test_acc:0.886719\n",
      "->epoch:134, train_loss:0.002541, train_acc:0.983782, test_loss:0.013591, test_acc:0.892448\n",
      "->epoch:135, train_loss:0.008969, train_acc:0.981424, test_loss:0.020663, test_acc:0.901042\n",
      "->epoch:136, train_loss:0.001573, train_acc:0.987847, test_loss:0.031474, test_acc:0.905729\n",
      "->epoch:137, train_loss:0.005990, train_acc:0.967752, test_loss:0.026812, test_acc:0.906120\n",
      "->epoch:138, train_loss:0.004052, train_acc:0.977402, test_loss:0.027102, test_acc:0.905729\n",
      "->epoch:139, train_loss:0.001724, train_acc:0.986849, test_loss:0.033300, test_acc:0.905078\n",
      "->epoch:140, train_loss:0.003348, train_acc:0.982943, test_loss:0.015220, test_acc:0.895182\n",
      "->epoch:141, train_loss:0.002766, train_acc:0.980237, test_loss:0.025294, test_acc:0.899609\n",
      "->epoch:142, train_loss:0.002438, train_acc:0.983247, test_loss:0.015000, test_acc:0.817448\n",
      "->epoch:143, train_loss:0.002793, train_acc:0.981322, test_loss:0.016069, test_acc:0.898958\n",
      "->epoch:144, train_loss:0.001985, train_acc:0.986256, test_loss:0.043830, test_acc:0.905469\n",
      "->epoch:145, train_loss:0.004775, train_acc:0.973177, test_loss:0.015147, test_acc:0.900000\n",
      "->epoch:146, train_loss:0.002041, train_acc:0.984520, test_loss:0.018721, test_acc:0.893750\n",
      "->epoch:147, train_loss:0.002543, train_acc:0.981322, test_loss:0.019199, test_acc:0.899089\n",
      "->epoch:148, train_loss:0.003673, train_acc:0.975260, test_loss:0.022089, test_acc:0.897266\n",
      "->epoch:149, train_loss:0.002498, train_acc:0.980729, test_loss:0.022733, test_acc:0.902344\n",
      "->epoch:150, train_loss:0.003327, train_acc:0.980975, test_loss:0.035128, test_acc:0.895573\n",
      "->epoch:151, train_loss:0.001654, train_acc:0.987413, test_loss:0.021452, test_acc:0.895573\n",
      "->epoch:152, train_loss:0.002061, train_acc:0.985532, test_loss:0.029612, test_acc:0.901042\n",
      "->epoch:153, train_loss:0.002582, train_acc:0.983275, test_loss:0.025788, test_acc:0.895443\n",
      "->epoch:154, train_loss:0.005137, train_acc:0.977908, test_loss:0.021503, test_acc:0.904948\n",
      "->epoch:155, train_loss:0.001983, train_acc:0.985590, test_loss:0.029267, test_acc:0.897917\n",
      "->epoch:156, train_loss:0.002328, train_acc:0.983637, test_loss:0.020226, test_acc:0.899219\n",
      "->epoch:157, train_loss:0.002323, train_acc:0.985315, test_loss:0.013667, test_acc:0.896875\n",
      "->epoch:158, train_loss:0.002221, train_acc:0.984100, test_loss:0.036579, test_acc:0.899870\n",
      "->epoch:159, train_loss:0.003341, train_acc:0.981047, test_loss:0.018985, test_acc:0.903516\n",
      "->epoch:160, train_loss:0.002694, train_acc:0.982812, test_loss:0.019971, test_acc:0.897266\n",
      "->epoch:161, train_loss:0.004094, train_acc:0.973119, test_loss:0.016647, test_acc:0.902604\n",
      "->epoch:162, train_loss:0.004208, train_acc:0.973655, test_loss:0.019354, test_acc:0.904297\n",
      "->epoch:163, train_loss:0.002459, train_acc:0.983174, test_loss:0.058386, test_acc:0.892448\n",
      "->epoch:164, train_loss:0.003536, train_acc:0.978472, test_loss:0.016295, test_acc:0.907031\n",
      "->epoch:165, train_loss:0.001962, train_acc:0.984418, test_loss:0.024074, test_acc:0.906250\n",
      "->epoch:166, train_loss:0.002636, train_acc:0.983478, test_loss:0.048379, test_acc:0.898177\n",
      "->epoch:167, train_loss:0.001960, train_acc:0.985171, test_loss:0.022945, test_acc:0.910417\n",
      "->epoch:168, train_loss:0.007859, train_acc:0.960214, test_loss:0.022415, test_acc:0.896745\n",
      "->epoch:169, train_loss:0.003141, train_acc:0.978877, test_loss:0.027873, test_acc:0.899609\n",
      "->epoch:170, train_loss:0.002146, train_acc:0.985229, test_loss:0.034005, test_acc:0.901302\n",
      "->epoch:171, train_loss:0.003575, train_acc:0.976620, test_loss:0.014975, test_acc:0.889583\n",
      "->epoch:172, train_loss:0.002369, train_acc:0.982422, test_loss:0.036467, test_acc:0.901432\n",
      "->epoch:173, train_loss:0.003674, train_acc:0.975174, test_loss:0.020897, test_acc:0.896094\n",
      "->epoch:174, train_loss:0.002476, train_acc:0.982494, test_loss:0.019997, test_acc:0.902344\n",
      "->epoch:175, train_loss:0.005897, train_acc:0.963006, test_loss:0.017637, test_acc:0.900260\n",
      "->epoch:176, train_loss:0.003657, train_acc:0.973973, test_loss:0.018150, test_acc:0.893490\n",
      "->epoch:177, train_loss:0.003111, train_acc:0.979239, test_loss:0.126631, test_acc:0.892578\n",
      "->epoch:178, train_loss:0.002683, train_acc:0.981091, test_loss:0.017756, test_acc:0.901172\n",
      "->epoch:179, train_loss:0.002033, train_acc:0.986314, test_loss:0.023223, test_acc:0.902865\n",
      "->epoch:180, train_loss:0.002554, train_acc:0.982538, test_loss:0.030317, test_acc:0.895052\n",
      "->epoch:181, train_loss:0.003526, train_acc:0.977242, test_loss:0.015366, test_acc:0.881641\n",
      "->epoch:182, train_loss:0.002348, train_acc:0.983796, test_loss:0.015759, test_acc:0.881901\n",
      "->epoch:183, train_loss:0.003239, train_acc:0.978791, test_loss:0.030962, test_acc:0.900000\n",
      "->epoch:184, train_loss:0.002444, train_acc:0.981901, test_loss:0.013752, test_acc:0.880990\n",
      "->epoch:185, train_loss:0.006758, train_acc:0.970964, test_loss:0.014314, test_acc:0.887760\n",
      "->epoch:186, train_loss:0.002457, train_acc:0.982060, test_loss:0.013812, test_acc:0.898047\n",
      "->epoch:187, train_loss:0.001859, train_acc:0.986589, test_loss:0.021076, test_acc:0.902083\n",
      "->epoch:188, train_loss:0.003249, train_acc:0.979919, test_loss:0.030775, test_acc:0.899609\n",
      "->epoch:189, train_loss:0.002225, train_acc:0.985532, test_loss:0.021110, test_acc:0.902995\n",
      "->epoch:190, train_loss:0.003133, train_acc:0.979268, test_loss:0.035685, test_acc:0.901563\n",
      "->epoch:191, train_loss:0.004373, train_acc:0.977416, test_loss:0.013436, test_acc:0.902734\n",
      "->epoch:192, train_loss:0.001482, train_acc:0.989381, test_loss:0.028021, test_acc:0.903125\n",
      "->epoch:193, train_loss:0.002512, train_acc:0.984635, test_loss:0.139927, test_acc:0.880990\n",
      "->epoch:194, train_loss:0.003519, train_acc:0.984187, test_loss:0.016809, test_acc:0.893620\n",
      "->epoch:195, train_loss:0.001875, train_acc:0.987167, test_loss:0.024027, test_acc:0.894010\n",
      "->epoch:196, train_loss:0.004521, train_acc:0.972786, test_loss:0.029010, test_acc:0.903776\n",
      "->epoch:197, train_loss:0.003332, train_acc:0.983087, test_loss:0.018071, test_acc:0.903385\n",
      "->epoch:198, train_loss:0.002463, train_acc:0.983449, test_loss:0.020348, test_acc:0.896875\n",
      "->epoch:199, train_loss:0.001734, train_acc:0.988035, test_loss:0.026534, test_acc:0.898958\n",
      "->epoch:200, train_loss:0.002634, train_acc:0.984143, test_loss:0.014076, test_acc:0.902474\n",
      "->epoch:201, train_loss:0.003211, train_acc:0.979007, test_loss:0.015252, test_acc:0.901302\n",
      "->epoch:202, train_loss:0.001826, train_acc:0.987167, test_loss:0.023179, test_acc:0.904167\n",
      "->epoch:203, train_loss:0.002490, train_acc:0.982972, test_loss:0.029811, test_acc:0.885287\n",
      "->epoch:204, train_loss:0.008868, train_acc:0.972483, test_loss:0.040409, test_acc:0.870052\n",
      "->epoch:205, train_loss:0.004087, train_acc:0.973843, test_loss:0.022328, test_acc:0.903385\n",
      "->epoch:206, train_loss:0.002102, train_acc:0.984795, test_loss:0.023239, test_acc:0.901693\n",
      "->epoch:207, train_loss:0.005669, train_acc:0.960344, test_loss:0.115852, test_acc:0.869531\n",
      "->epoch:208, train_loss:0.003984, train_acc:0.972208, test_loss:0.071712, test_acc:0.903125\n",
      "->epoch:209, train_loss:0.003075, train_acc:0.979731, test_loss:0.016068, test_acc:0.906250\n",
      "->epoch:210, train_loss:0.002576, train_acc:0.982899, test_loss:0.065136, test_acc:0.899740\n",
      "->epoch:211, train_loss:0.003721, train_acc:0.975347, test_loss:0.013429, test_acc:0.890625\n",
      "->epoch:212, train_loss:0.002008, train_acc:0.985446, test_loss:0.015532, test_acc:0.898307\n",
      "->epoch:213, train_loss:0.003192, train_acc:0.978168, test_loss:0.026876, test_acc:0.880729\n",
      "->epoch:214, train_loss:0.002425, train_acc:0.982581, test_loss:0.016713, test_acc:0.904037\n",
      "->epoch:215, train_loss:0.004671, train_acc:0.976591, test_loss:0.024999, test_acc:0.896745\n",
      "->epoch:216, train_loss:0.002568, train_acc:0.984361, test_loss:0.086511, test_acc:0.868880\n",
      "->epoch:217, train_loss:0.003135, train_acc:0.980295, test_loss:0.044024, test_acc:0.847656\n",
      "->epoch:218, train_loss:0.003958, train_acc:0.970356, test_loss:0.026326, test_acc:0.888802\n",
      "->epoch:219, train_loss:0.002500, train_acc:0.981698, test_loss:0.020485, test_acc:0.905208\n",
      "->epoch:220, train_loss:0.004631, train_acc:0.968273, test_loss:0.022892, test_acc:0.901823\n",
      "->epoch:221, train_loss:0.002384, train_acc:0.983304, test_loss:0.016898, test_acc:0.901042\n",
      "->epoch:222, train_loss:0.005564, train_acc:0.964352, test_loss:0.053222, test_acc:0.873307\n",
      "->epoch:223, train_loss:0.006866, train_acc:0.970095, test_loss:0.018834, test_acc:0.894922\n",
      "->epoch:224, train_loss:0.002443, train_acc:0.981713, test_loss:0.075399, test_acc:0.898177\n",
      "->epoch:225, train_loss:0.003150, train_acc:0.979818, test_loss:0.017805, test_acc:0.893359\n",
      "->epoch:226, train_loss:0.002023, train_acc:0.985272, test_loss:0.020580, test_acc:0.903516\n",
      "->epoch:227, train_loss:0.002595, train_acc:0.983420, test_loss:0.081299, test_acc:0.899870\n",
      "->epoch:228, train_loss:0.006974, train_acc:0.949508, test_loss:0.022377, test_acc:0.899479\n",
      "->epoch:229, train_loss:0.007323, train_acc:0.942593, test_loss:0.056409, test_acc:0.870443\n",
      "->epoch:230, train_loss:0.004407, train_acc:0.977170, test_loss:0.012309, test_acc:0.890495\n",
      "->epoch:231, train_loss:0.002260, train_acc:0.983550, test_loss:0.026386, test_acc:0.901823\n",
      "->epoch:232, train_loss:0.002559, train_acc:0.983174, test_loss:0.022365, test_acc:0.893099\n",
      "->epoch:233, train_loss:0.008848, train_acc:0.974580, test_loss:0.024918, test_acc:0.883594\n",
      "->epoch:234, train_loss:0.002325, train_acc:0.983058, test_loss:0.029956, test_acc:0.900391\n",
      "->epoch:235, train_loss:0.003486, train_acc:0.978443, test_loss:0.038222, test_acc:0.894792\n",
      "->epoch:236, train_loss:0.006944, train_acc:0.963918, test_loss:0.018934, test_acc:0.891146\n",
      "->epoch:237, train_loss:0.002549, train_acc:0.981453, test_loss:0.024950, test_acc:0.897135\n",
      "->epoch:238, train_loss:0.002670, train_acc:0.982841, test_loss:0.023906, test_acc:0.897005\n",
      "->epoch:239, train_loss:0.002333, train_acc:0.983550, test_loss:0.015656, test_acc:0.900391\n",
      "->epoch:240, train_loss:0.005297, train_acc:0.963845, test_loss:0.037986, test_acc:0.895573\n",
      "->epoch:241, train_loss:0.017716, train_acc:0.972700, test_loss:0.011742, test_acc:0.881380\n",
      "->epoch:242, train_loss:0.002383, train_acc:0.981120, test_loss:0.012461, test_acc:0.889323\n",
      "->epoch:243, train_loss:0.003771, train_acc:0.976562, test_loss:0.026819, test_acc:0.889583\n",
      "->epoch:244, train_loss:0.001925, train_acc:0.986849, test_loss:0.021028, test_acc:0.902734\n",
      "->epoch:245, train_loss:0.006128, train_acc:0.960315, test_loss:0.015175, test_acc:0.893490\n",
      "->epoch:246, train_loss:0.002424, train_acc:0.983550, test_loss:0.024782, test_acc:0.898307\n",
      "->epoch:247, train_loss:0.002190, train_acc:0.984187, test_loss:0.042399, test_acc:0.897917\n",
      "->epoch:248, train_loss:0.003377, train_acc:0.978024, test_loss:0.044533, test_acc:0.894922\n",
      "->epoch:249, train_loss:0.002464, train_acc:0.983261, test_loss:0.018281, test_acc:0.903255\n",
      "->epoch:250, train_loss:0.005099, train_acc:0.983941, test_loss:0.040191, test_acc:0.906510\n",
      "->epoch:251, train_loss:0.007003, train_acc:0.981496, test_loss:0.027005, test_acc:0.898828\n",
      "->epoch:252, train_loss:0.001849, train_acc:0.987804, test_loss:0.022710, test_acc:0.896745\n",
      "->epoch:253, train_loss:0.012278, train_acc:0.971788, test_loss:0.020268, test_acc:0.899870\n",
      "->epoch:254, train_loss:0.002245, train_acc:0.985590, test_loss:0.050030, test_acc:0.902865\n",
      "->epoch:255, train_loss:0.004351, train_acc:0.978646, test_loss:0.037595, test_acc:0.895443\n",
      "->epoch:256, train_loss:0.004242, train_acc:0.978704, test_loss:0.017256, test_acc:0.893229\n",
      "->epoch:257, train_loss:0.004299, train_acc:0.973770, test_loss:0.032195, test_acc:0.881641\n",
      "->epoch:258, train_loss:0.002388, train_acc:0.986140, test_loss:0.029150, test_acc:0.896875\n",
      "->epoch:259, train_loss:0.009204, train_acc:0.981467, test_loss:0.117807, test_acc:0.887240\n",
      "->epoch:260, train_loss:0.007427, train_acc:0.963223, test_loss:0.047712, test_acc:0.891146\n",
      "->epoch:261, train_loss:0.002508, train_acc:0.982798, test_loss:0.013364, test_acc:0.870052\n",
      "->epoch:262, train_loss:0.004540, train_acc:0.972729, test_loss:0.028111, test_acc:0.902083\n",
      "->epoch:263, train_loss:0.004515, train_acc:0.970964, test_loss:0.019903, test_acc:0.900000\n",
      "->epoch:264, train_loss:0.004410, train_acc:0.975998, test_loss:0.040546, test_acc:0.819271\n",
      "->epoch:265, train_loss:0.003720, train_acc:0.973944, test_loss:0.078926, test_acc:0.889844\n",
      "->epoch:266, train_loss:0.004603, train_acc:0.970269, test_loss:0.063615, test_acc:0.864323\n",
      "->epoch:267, train_loss:0.003926, train_acc:0.977850, test_loss:0.013567, test_acc:0.861589\n",
      "->epoch:268, train_loss:0.002306, train_acc:0.984578, test_loss:0.023592, test_acc:0.899740\n",
      "->epoch:269, train_loss:0.002490, train_acc:0.983579, test_loss:0.032737, test_acc:0.892708\n",
      "->epoch:270, train_loss:0.008014, train_acc:0.967694, test_loss:0.034192, test_acc:0.895573\n",
      "->epoch:271, train_loss:0.003383, train_acc:0.975434, test_loss:0.031234, test_acc:0.890234\n",
      "->epoch:272, train_loss:0.002618, train_acc:0.981612, test_loss:0.191815, test_acc:0.890625\n",
      "->epoch:273, train_loss:0.007761, train_acc:0.940205, test_loss:0.021600, test_acc:0.893880\n",
      "->epoch:274, train_loss:0.004317, train_acc:0.971803, test_loss:0.061859, test_acc:0.878516\n",
      "->epoch:275, train_loss:0.002502, train_acc:0.982943, test_loss:0.051555, test_acc:0.894661\n",
      "->epoch:276, train_loss:0.004623, train_acc:0.969864, test_loss:0.015513, test_acc:0.886198\n",
      "->epoch:277, train_loss:0.004866, train_acc:0.975477, test_loss:0.019473, test_acc:0.892448\n",
      "->epoch:278, train_loss:0.002282, train_acc:0.984549, test_loss:0.025717, test_acc:0.901693\n",
      "->epoch:279, train_loss:0.004630, train_acc:0.969054, test_loss:0.021602, test_acc:0.893750\n",
      "->epoch:280, train_loss:0.003101, train_acc:0.978487, test_loss:0.027829, test_acc:0.891276\n",
      "->epoch:281, train_loss:0.003850, train_acc:0.976259, test_loss:0.036797, test_acc:0.889583\n",
      "->epoch:282, train_loss:0.002603, train_acc:0.981395, test_loss:0.036323, test_acc:0.897005\n",
      "->epoch:283, train_loss:0.011420, train_acc:0.931800, test_loss:0.070283, test_acc:0.879948\n",
      "->epoch:284, train_loss:0.004470, train_acc:0.973452, test_loss:0.019588, test_acc:0.894271\n",
      "->epoch:285, train_loss:0.003916, train_acc:0.974551, test_loss:0.117545, test_acc:0.873828\n",
      "->epoch:286, train_loss:0.003853, train_acc:0.977271, test_loss:0.020787, test_acc:0.891146\n",
      "->epoch:287, train_loss:0.004170, train_acc:0.977416, test_loss:0.017308, test_acc:0.896875\n",
      "->epoch:288, train_loss:0.004854, train_acc:0.972902, test_loss:0.019370, test_acc:0.897917\n",
      "->epoch:289, train_loss:0.003085, train_acc:0.981684, test_loss:0.095591, test_acc:0.875651\n",
      "->epoch:290, train_loss:0.002357, train_acc:0.984737, test_loss:0.031194, test_acc:0.903385\n",
      "->epoch:291, train_loss:0.017764, train_acc:0.942925, test_loss:0.041942, test_acc:0.881120\n",
      "->epoch:292, train_loss:0.002671, train_acc:0.983304, test_loss:0.099078, test_acc:0.892318\n",
      "->epoch:293, train_loss:0.004843, train_acc:0.965683, test_loss:0.036212, test_acc:0.897656\n",
      "->epoch:294, train_loss:0.005338, train_acc:0.961415, test_loss:0.032861, test_acc:0.894922\n",
      "->epoch:295, train_loss:0.002675, train_acc:0.983507, test_loss:0.016030, test_acc:0.901432\n",
      "->epoch:296, train_loss:0.002471, train_acc:0.983420, test_loss:0.018119, test_acc:0.893099\n",
      "->epoch:297, train_loss:0.005579, train_acc:0.966450, test_loss:0.038831, test_acc:0.839193\n",
      "->epoch:298, train_loss:0.006436, train_acc:0.962008, test_loss:0.029253, test_acc:0.837109\n",
      "->epoch:299, train_loss:0.002704, train_acc:0.980411, test_loss:0.065473, test_acc:0.896745\n",
      "->epoch:300, train_loss:0.002708, train_acc:0.981655, test_loss:0.091209, test_acc:0.900130\n",
      "->epoch:301, train_loss:0.007382, train_acc:0.958174, test_loss:0.060423, test_acc:0.899219\n",
      "->epoch:302, train_loss:0.002992, train_acc:0.981395, test_loss:0.070565, test_acc:0.894792\n",
      "->epoch:303, train_loss:0.003427, train_acc:0.978385, test_loss:0.075235, test_acc:0.890495\n",
      "->epoch:304, train_loss:0.002992, train_acc:0.980020, test_loss:0.038406, test_acc:0.892057\n",
      "->epoch:305, train_loss:0.008078, train_acc:0.951722, test_loss:0.014902, test_acc:0.880469\n",
      "->epoch:306, train_loss:0.002934, train_acc:0.978617, test_loss:0.031234, test_acc:0.895443\n",
      "->epoch:307, train_loss:0.003042, train_acc:0.979557, test_loss:0.052908, test_acc:0.894271\n",
      "->epoch:308, train_loss:0.008362, train_acc:0.968953, test_loss:0.021176, test_acc:0.894922\n",
      "->epoch:309, train_loss:0.005835, train_acc:0.970703, test_loss:0.017333, test_acc:0.894531\n",
      "->epoch:310, train_loss:0.002084, train_acc:0.986777, test_loss:0.045594, test_acc:0.899740\n",
      "->epoch:311, train_loss:0.013275, train_acc:0.952865, test_loss:0.019521, test_acc:0.874089\n",
      "->epoch:312, train_loss:0.003985, train_acc:0.969546, test_loss:0.013552, test_acc:0.882813\n",
      "->epoch:313, train_loss:0.003160, train_acc:0.978356, test_loss:0.026129, test_acc:0.892708\n",
      "->epoch:314, train_loss:0.002854, train_acc:0.980946, test_loss:0.022218, test_acc:0.898828\n",
      "->epoch:315, train_loss:0.005293, train_acc:0.966594, test_loss:0.011315, test_acc:0.878255\n",
      "->epoch:316, train_loss:0.002472, train_acc:0.982595, test_loss:0.035608, test_acc:0.900260\n",
      "->epoch:317, train_loss:0.004874, train_acc:0.967144, test_loss:0.123783, test_acc:0.898828\n",
      "->epoch:318, train_loss:0.003441, train_acc:0.977156, test_loss:0.054349, test_acc:0.896484\n",
      "->epoch:319, train_loss:0.008697, train_acc:0.929774, test_loss:0.015674, test_acc:0.887109\n",
      "->epoch:320, train_loss:0.006102, train_acc:0.955570, test_loss:0.015900, test_acc:0.892448\n",
      "->epoch:321, train_loss:0.003784, train_acc:0.975058, test_loss:0.017053, test_acc:0.894661\n",
      "->epoch:322, train_loss:0.004248, train_acc:0.979311, test_loss:0.032840, test_acc:0.885417\n",
      "->epoch:323, train_loss:0.002697, train_acc:0.981684, test_loss:0.031825, test_acc:0.824740\n",
      "->epoch:324, train_loss:0.055309, train_acc:0.890061, test_loss:0.095019, test_acc:0.800911\n",
      "->epoch:325, train_loss:0.023664, train_acc:0.839178, test_loss:0.061345, test_acc:0.829557\n",
      "->epoch:326, train_loss:0.006468, train_acc:0.936791, test_loss:0.017630, test_acc:0.888542\n",
      "->epoch:327, train_loss:0.005578, train_acc:0.957378, test_loss:0.045386, test_acc:0.878776\n",
      "->epoch:328, train_loss:0.006471, train_acc:0.948973, test_loss:0.036799, test_acc:0.865625\n",
      "->epoch:329, train_loss:0.004161, train_acc:0.969358, test_loss:0.019429, test_acc:0.854037\n",
      "->epoch:330, train_loss:0.003884, train_acc:0.971195, test_loss:0.024302, test_acc:0.893099\n",
      "->epoch:331, train_loss:0.003300, train_acc:0.975723, test_loss:0.074391, test_acc:0.890234\n",
      "->epoch:332, train_loss:0.004244, train_acc:0.973611, test_loss:0.062022, test_acc:0.890625\n",
      "->epoch:333, train_loss:0.003932, train_acc:0.973958, test_loss:0.019528, test_acc:0.875130\n",
      "->epoch:334, train_loss:0.004785, train_acc:0.964873, test_loss:0.037622, test_acc:0.891276\n",
      "->epoch:335, train_loss:0.008490, train_acc:0.942448, test_loss:0.071300, test_acc:0.836198\n",
      "->epoch:336, train_loss:0.004248, train_acc:0.967361, test_loss:0.071227, test_acc:0.890495\n",
      "->epoch:337, train_loss:0.004187, train_acc:0.970341, test_loss:0.036405, test_acc:0.801563\n",
      "->epoch:338, train_loss:0.003717, train_acc:0.973799, test_loss:0.031261, test_acc:0.878516\n",
      "->epoch:339, train_loss:0.004465, train_acc:0.977300, test_loss:0.051736, test_acc:0.894271\n",
      "->epoch:340, train_loss:0.010616, train_acc:0.958681, test_loss:0.015905, test_acc:0.889193\n",
      "->epoch:341, train_loss:0.002966, train_acc:0.979311, test_loss:0.013103, test_acc:0.880208\n",
      "->epoch:342, train_loss:0.004045, train_acc:0.972526, test_loss:0.028122, test_acc:0.899089\n",
      "->epoch:343, train_loss:0.013380, train_acc:0.960952, test_loss:0.013919, test_acc:0.882682\n",
      "->epoch:344, train_loss:0.005973, train_acc:0.973307, test_loss:0.019785, test_acc:0.888802\n",
      "->epoch:345, train_loss:0.002572, train_acc:0.982046, test_loss:0.025400, test_acc:0.891537\n",
      "->epoch:346, train_loss:0.005261, train_acc:0.962008, test_loss:0.027274, test_acc:0.886589\n",
      "->epoch:347, train_loss:0.006543, train_acc:0.960474, test_loss:0.018435, test_acc:0.887760\n",
      "->epoch:348, train_loss:0.013122, train_acc:0.931337, test_loss:0.037620, test_acc:0.883464\n",
      "->epoch:349, train_loss:0.024230, train_acc:0.964540, test_loss:0.031863, test_acc:0.886458\n",
      "->epoch:350, train_loss:0.002462, train_acc:0.981858, test_loss:0.030588, test_acc:0.892839\n",
      "->epoch:351, train_loss:0.003753, train_acc:0.975391, test_loss:0.014458, test_acc:0.891146\n",
      "->epoch:352, train_loss:0.002974, train_acc:0.981554, test_loss:0.035499, test_acc:0.893359\n",
      "->epoch:353, train_loss:0.004777, train_acc:0.970891, test_loss:0.084802, test_acc:0.862500\n",
      "->epoch:354, train_loss:0.002794, train_acc:0.980570, test_loss:0.037903, test_acc:0.892318\n",
      "->epoch:355, train_loss:0.002776, train_acc:0.981915, test_loss:0.060333, test_acc:0.886198\n",
      "->epoch:356, train_loss:0.007674, train_acc:0.945009, test_loss:0.046904, test_acc:0.888672\n",
      "->epoch:357, train_loss:0.004023, train_acc:0.981756, test_loss:0.137984, test_acc:0.889323\n",
      "->epoch:358, train_loss:0.003892, train_acc:0.975637, test_loss:0.037453, test_acc:0.895313\n",
      "->epoch:359, train_loss:0.010068, train_acc:0.964627, test_loss:0.029186, test_acc:0.886589\n",
      "->epoch:360, train_loss:0.003580, train_acc:0.974219, test_loss:0.030814, test_acc:0.892188\n",
      "->epoch:361, train_loss:0.002422, train_acc:0.982841, test_loss:0.029693, test_acc:0.896484\n",
      "->epoch:362, train_loss:0.007313, train_acc:0.966739, test_loss:0.059327, test_acc:0.876302\n",
      "->epoch:363, train_loss:0.003634, train_acc:0.973568, test_loss:0.022340, test_acc:0.894531\n",
      "->epoch:364, train_loss:0.007329, train_acc:0.952995, test_loss:0.034898, test_acc:0.885807\n",
      "->epoch:365, train_loss:0.005934, train_acc:0.953704, test_loss:0.061568, test_acc:0.862760\n",
      "->epoch:366, train_loss:0.007564, train_acc:0.963860, test_loss:0.024554, test_acc:0.863932\n",
      "->epoch:367, train_loss:0.004974, train_acc:0.962384, test_loss:0.035861, test_acc:0.851302\n",
      "->epoch:368, train_loss:0.003671, train_acc:0.975058, test_loss:0.032004, test_acc:0.883464\n",
      "->epoch:369, train_loss:0.019605, train_acc:0.965075, test_loss:0.020044, test_acc:0.879297\n",
      "->epoch:370, train_loss:0.002962, train_acc:0.979413, test_loss:0.021415, test_acc:0.884896\n",
      "->epoch:371, train_loss:0.004064, train_acc:0.975159, test_loss:0.024937, test_acc:0.891016\n",
      "->epoch:372, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13544\\1410925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'->epoch:{epoch + 1}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memo_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13544\\974053572.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(emo_dim)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memo_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f'->epoch:{epoch + 1}', end = ', ')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim)\n",
    "#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa21ed2-da43-4dc3-ad70-081231e98ef4",
   "metadata": {},
   "source": [
    "# 增加模型容量\n",
    "- ->epoch:86, train_loss:0.002378, train_acc:0.980787, test_loss:0.017223, test_acc:0.908203\n",
    "- 改为heads=3， ->epoch:167, train_loss:0.001960, train_acc:0.985171, test_loss:0.022945, test_acc:0.910417"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964d82-e4ca-40db-ad83-0722b8a6b880",
   "metadata": {},
   "source": [
    "## 比较实验\n",
    "### GCN\n",
    "+ 仅包括自环时，->epoch:25, train_loss:0.000922, train_acc:0.990784, test_loss:0.010875, test_acc:0.919401\n",
    "+ 加上3x3卷积核的邻接边时，->epoch:32, train_loss:0.000819, train_acc:0.992173, test_loss:0.020655, test_acc:0.895313，邻接边设计的不好，限制了模型的发挥\n",
    "+ 别人的方法的准确率：89/90、93/94\n",
    "### GAT\n",
    "+ 仅包括自环时，->epoch:30, train_loss:0.001252, train_acc:0.986531, test_loss:0.015308, test_acc:0.912630\n",
    "+ 使用自己设计的边，->epoch:123, train_loss:0.002104, train_acc:0.982161, test_loss:0.029411, test_acc:0.904688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f4ff-9fbb-4323-ad0d-4dfc7848e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
