{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6df28-9919-4bff-8e9a-ea82c35eba11",
   "metadata": {},
   "source": [
    "### 脑电图注意力网络（GAT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def data_split(train_ratio=0.7):\n",
    "#     load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "#     trials = np.load(load_dir + 'trials.npy')\n",
    "#     bases = np.load(load_dir + 'bases.npy')\n",
    "#     labels = np.load(load_dir + 'labels.npy')\n",
    "#     # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "#     # 去基线\n",
    "#     for i, base in enumerate(bases):\n",
    "#         trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "#     # 离散化标签\n",
    "#     labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "#     # 复制标签以对齐样本\n",
    "#     labels = np.repeat(labels, 60, axis = 0)\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     shuffle_list = np.arange(trials.shape[0])\n",
    "#     np.random.shuffle(shuffle_list)\n",
    "#     trials = trials[shuffle_list]\n",
    "#     labels = labels[shuffle_list]\n",
    "    \n",
    "#     cut_point = int(trials.shape[0] * train_ratio)\n",
    "#     train_features, train_labels = trials[:cut_point], labels[:cut_point]\n",
    "#     test_features, test_labels = trials[cut_point:], labels[cut_point:]\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32 * 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32 * 128))\n",
    "    \n",
    "#     mean = train_features.mean(axis = 0)\n",
    "#     std = train_features.std(axis = 0)\n",
    "    \n",
    "#     train_features = (train_features - mean) / std\n",
    "#     test_features = (test_features - mean) / std\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32, 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32, 128))\n",
    "    \n",
    "#     save_dir = 'data/data_split/'\n",
    "#     np.save(save_dir + 'train_features.npy', train_features)\n",
    "#     np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "#     np.save(save_dir + 'test_features.npy', test_features)\n",
    "#     np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "# data_split(train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b5e808-1250-4784-b6b7-a0e58d86017a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if create_complete_graph:\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        return edge_index\n",
    "    \n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd42b5bf-ca24-4f71-95ad-b0b494aaa18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcb8368-b508-41fb-baa4-5e35d16ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index = [[],[]]\n",
    "# weight = []\n",
    "\n",
    "# #用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "# chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "#               9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "#               17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "#                 25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "# maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "# for k, v in chan_to_1020.items():\n",
    "#     maps[v[0]][v[1]] = k + 1\n",
    "# print(maps)\n",
    "# plt.matshow(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    edge_index = None\n",
    "    def __init__(self, root, is_train_data, edge_index):\n",
    "        self.is_train_data = is_train_data\n",
    "        self.edge_index = edge_index\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['train.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor(labels[i].reshape(1, -1), dtype=torch.long)\n",
    "            data = Data(x = x, edge_index=self.edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(in_channels=embed_dim, out_channels=256)\n",
    "        self.conv2 = GCNConv(in_channels=256, out_channels=256)\n",
    "        \n",
    "#         self.conv1 = GATConv(in_channels=embed_dim, out_channels=256, heads=1, concat=True, dropout=0.2, add_self_loops=False)\n",
    "#         self.conv2 = GATConv(in_channels=256, out_channels=256, heads=1, concat=True, dropout=0.2, add_self_loops=False)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(8192, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1280)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(640)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # x = self.item_embedding(x)\n",
    "        # x = x.squeeze(1) # n*128\n",
    "#         x, attention_weights = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "#         x = F.relu(x)\n",
    "#         # x, edeg_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch) # pool 之后得到 n*128个点\n",
    "#         # x1 = gap(x, batch)\n",
    "#         x, attention_weights = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # x, edeg_index, _, batch, _, _ = self.pool2(x, edeg_index, None, batch)\n",
    "        # x2 = gap(x, batch)\n",
    "        # x = F.relu(self.conv3(x, edge_index))\n",
    "        # x, edeg_index, _, batch, _, _ = self.pool3(x, edeg_index, None, batch)\n",
    "        # x3 = gap(x, batch)\n",
    "        # x = x1 + x2 + x3 # 获取不同尺度的全局特征\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emo_dim):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        train_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_train_sample = len(trainDataLoader.dataset)\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    train_acc = train_acc / num_train_sample\n",
    "    \n",
    "    # check测试集的性能\n",
    "    vali_loss = 0\n",
    "    vali_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in testDataLoader:\n",
    "        batch.to(device)\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        vali_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        vali_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_test_sample = len(testDataLoader.dataset)\n",
    "    vali_loss = vali_loss / num_test_sample\n",
    "    vali_acc = vali_acc / num_test_sample\n",
    "    \n",
    "    print(f'train_loss:{train_loss:.6f}, train_acc:{train_acc:.6f}, test_loss:{vali_loss:.6f}, test_acc:{vali_acc:.6f}')\n",
    "    \n",
    "    return train_loss, train_acc, vali_loss, vali_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9bf77-c20d-484b-932b-8b98917f374b",
   "metadata": {},
   "source": [
    "# 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1e1611-fac8-4210-b800-bf02914ed6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete_graph = False\n",
    "self_loop_only = True\n",
    "emo_dim = 0\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "edge_index = get_edge_index(create_complete_graph=create_complete_graph, self_loop_only=self_loop_only)\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, edge_index=edge_index)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, edge_index=edge_index)\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3f3d55-fc83-475e-a697-9b8a12789830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "crit = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:1, train_loss:0.016937, train_acc:0.717390, test_loss:0.012284, test_acc:0.825912\n",
      "->epoch:2, train_loss:0.009116, train_acc:0.874060, test_loss:0.008901, test_acc:0.877734\n",
      "->epoch:3, train_loss:0.005824, train_acc:0.921947, test_loss:0.008651, test_acc:0.893880\n",
      "->epoch:4, train_loss:0.004306, train_acc:0.944633, test_loss:0.008639, test_acc:0.896745\n",
      "->epoch:5, train_loss:0.003469, train_acc:0.955744, test_loss:0.009453, test_acc:0.901432\n",
      "->epoch:6, train_loss:0.002793, train_acc:0.965639, test_loss:0.009722, test_acc:0.905208\n",
      "->epoch:7, train_loss:0.002409, train_acc:0.971687, test_loss:0.009780, test_acc:0.904167\n",
      "->epoch:8, train_loss:0.002058, train_acc:0.975723, test_loss:0.010463, test_acc:0.912500\n",
      "->epoch:9, train_loss:0.001790, train_acc:0.978791, test_loss:0.010959, test_acc:0.909115\n",
      "->epoch:10, train_loss:0.001646, train_acc:0.981076, test_loss:0.009760, test_acc:0.914583\n",
      "->epoch:11, train_loss:0.001486, train_acc:0.983029, test_loss:0.009733, test_acc:0.913411\n",
      "->epoch:12, train_loss:0.001319, train_acc:0.985156, test_loss:0.008993, test_acc:0.913672\n",
      "->epoch:13, train_loss:0.001299, train_acc:0.986256, test_loss:0.012408, test_acc:0.913802\n",
      "->epoch:14, train_loss:0.001213, train_acc:0.986415, test_loss:0.009862, test_acc:0.915495\n",
      "->epoch:15, train_loss:0.001156, train_acc:0.988397, test_loss:0.011888, test_acc:0.915885\n",
      "->epoch:16, train_loss:0.001174, train_acc:0.987760, test_loss:0.012126, test_acc:0.920182\n",
      "->epoch:17, train_loss:0.000924, train_acc:0.990741, test_loss:0.009511, test_acc:0.922135\n",
      "->epoch:18, train_loss:0.001005, train_acc:0.990176, test_loss:0.012776, test_acc:0.918620\n",
      "->epoch:19, train_loss:0.001016, train_acc:0.990263, test_loss:0.009436, test_acc:0.923047\n",
      "->epoch:20, train_loss:0.000925, train_acc:0.990755, test_loss:0.009306, test_acc:0.916276\n",
      "->epoch:21, train_loss:0.000877, train_acc:0.991319, test_loss:0.013974, test_acc:0.923568\n",
      "->epoch:22, train_loss:0.001130, train_acc:0.989511, test_loss:0.011283, test_acc:0.920573\n",
      "->epoch:23, train_loss:0.000775, train_acc:0.992419, test_loss:0.010751, test_acc:0.920313\n",
      "->epoch:24, train_loss:0.001030, train_acc:0.991334, test_loss:0.011540, test_acc:0.918490\n",
      "->epoch:25, train_loss:0.000772, train_acc:0.992650, test_loss:0.012677, test_acc:0.925911\n",
      "->epoch:26, train_loss:0.000804, train_acc:0.992216, test_loss:0.015194, test_acc:0.919662\n",
      "->epoch:27, train_loss:0.000695, train_acc:0.992896, test_loss:0.014492, test_acc:0.924870\n",
      "->epoch:28, train_loss:0.000912, train_acc:0.991421, test_loss:0.016859, test_acc:0.925391\n",
      "->epoch:29, train_loss:0.000777, train_acc:0.992896, test_loss:0.013822, test_acc:0.923307\n",
      "->epoch:30, train_loss:0.000822, train_acc:0.992882, test_loss:0.011136, test_acc:0.927604\n",
      "->epoch:31, train_loss:0.000634, train_acc:0.993287, test_loss:0.012230, test_acc:0.924219\n",
      "->epoch:32, train_loss:0.000942, train_acc:0.992144, test_loss:0.012284, test_acc:0.924609\n",
      "->epoch:33, train_loss:0.000656, train_acc:0.994126, test_loss:0.009946, test_acc:0.925391\n",
      "->epoch:34, train_loss:0.000771, train_acc:0.992130, test_loss:0.026535, test_acc:0.924349\n",
      "->epoch:35, train_loss:0.000963, train_acc:0.989858, test_loss:0.011712, test_acc:0.925651\n",
      "->epoch:36, train_loss:0.000598, train_acc:0.994444, test_loss:0.009848, test_acc:0.913802\n",
      "->epoch:37, train_loss:0.000679, train_acc:0.993504, test_loss:0.020060, test_acc:0.921094\n",
      "->epoch:38, train_loss:0.000739, train_acc:0.993244, test_loss:0.012455, test_acc:0.926432\n",
      "->epoch:39, train_loss:0.000730, train_acc:0.993012, test_loss:0.015414, test_acc:0.928255\n",
      "->epoch:40, train_loss:0.000702, train_acc:0.993432, test_loss:0.012475, test_acc:0.928776\n",
      "->epoch:41, train_loss:0.000845, train_acc:0.991840, test_loss:0.016240, test_acc:0.929036\n",
      "->epoch:42, train_loss:0.000792, train_acc:0.992636, test_loss:0.016359, test_acc:0.929688\n",
      "->epoch:43, train_loss:0.000594, train_acc:0.994401, test_loss:0.014349, test_acc:0.929948\n",
      "->epoch:44, train_loss:0.000641, train_acc:0.994575, test_loss:0.020611, test_acc:0.927083\n",
      "->epoch:45, train_loss:0.000982, train_acc:0.990668, test_loss:0.020267, test_acc:0.923698\n",
      "->epoch:46, train_loss:0.000591, train_acc:0.994546, test_loss:0.012081, test_acc:0.929688\n",
      "->epoch:47, train_loss:0.000833, train_acc:0.992839, test_loss:0.015310, test_acc:0.929427\n",
      "->epoch:48, train_loss:0.000709, train_acc:0.993461, test_loss:0.015723, test_acc:0.927865\n",
      "->epoch:49, train_loss:0.000547, train_acc:0.994792, test_loss:0.012465, test_acc:0.930469\n",
      "->epoch:50, train_loss:0.001276, train_acc:0.988860, test_loss:0.009902, test_acc:0.930599\n",
      "->epoch:51, train_loss:0.000642, train_acc:0.994430, test_loss:0.023913, test_acc:0.928906\n",
      "->epoch:52, train_loss:0.001191, train_acc:0.990234, test_loss:0.021306, test_acc:0.921615\n",
      "->epoch:53, train_loss:0.000798, train_acc:0.993258, test_loss:0.025577, test_acc:0.931510\n",
      "->epoch:54, train_loss:0.000603, train_acc:0.994792, test_loss:0.015902, test_acc:0.933984\n",
      "->epoch:55, train_loss:0.000780, train_acc:0.993576, test_loss:0.011324, test_acc:0.927214\n",
      "->epoch:56, train_loss:0.000527, train_acc:0.995052, test_loss:0.017838, test_acc:0.933724\n",
      "->epoch:57, train_loss:0.000978, train_acc:0.991334, test_loss:0.018257, test_acc:0.929557\n",
      "->epoch:58, train_loss:0.000596, train_acc:0.994488, test_loss:0.022080, test_acc:0.934635\n",
      "->epoch:59, train_loss:0.000648, train_acc:0.994575, test_loss:0.011772, test_acc:0.931771\n",
      "->epoch:60, train_loss:0.000779, train_acc:0.993186, test_loss:0.017136, test_acc:0.926823\n",
      "->epoch:61, train_loss:0.000834, train_acc:0.993041, test_loss:0.017167, test_acc:0.923438\n",
      "->epoch:62, train_loss:0.001162, train_acc:0.990914, test_loss:0.015195, test_acc:0.936198\n",
      "->epoch:63, train_loss:0.000684, train_acc:0.993967, test_loss:0.013822, test_acc:0.935156\n",
      "->epoch:64, train_loss:0.001087, train_acc:0.990958, test_loss:0.021255, test_acc:0.935807\n",
      "->epoch:65, train_loss:0.000945, train_acc:0.991855, test_loss:0.016707, test_acc:0.927604\n",
      "->epoch:66, train_loss:0.001309, train_acc:0.990828, test_loss:0.013893, test_acc:0.920313\n",
      "->epoch:67, train_loss:0.000727, train_acc:0.994444, test_loss:0.014010, test_acc:0.934375\n",
      "->epoch:68, train_loss:0.000675, train_acc:0.994560, test_loss:0.017706, test_acc:0.923568\n",
      "->epoch:69, train_loss:0.001042, train_acc:0.992882, test_loss:0.017843, test_acc:0.926042\n",
      "->epoch:70, train_loss:0.000921, train_acc:0.993547, test_loss:0.014174, test_acc:0.931641\n",
      "->epoch:71, train_loss:0.000599, train_acc:0.995009, test_loss:0.015051, test_acc:0.933203\n",
      "->epoch:72, train_loss:0.001074, train_acc:0.992506, test_loss:0.015590, test_acc:0.934375\n",
      "->epoch:73, train_loss:0.000683, train_acc:0.994719, test_loss:0.009135, test_acc:0.930599\n",
      "->epoch:74, train_loss:0.000853, train_acc:0.994213, test_loss:0.023489, test_acc:0.922266\n",
      "->epoch:75, train_loss:0.001097, train_acc:0.991768, test_loss:0.023546, test_acc:0.925781\n",
      "->epoch:76, train_loss:0.001023, train_acc:0.993056, test_loss:0.023111, test_acc:0.935156\n",
      "->epoch:77, train_loss:0.001690, train_acc:0.990813, test_loss:0.019489, test_acc:0.925911\n",
      "->epoch:78, train_loss:0.002203, train_acc:0.991117, test_loss:0.029613, test_acc:0.929297\n",
      "->epoch:79, train_loss:0.000804, train_acc:0.994777, test_loss:0.014427, test_acc:0.912891\n",
      "->epoch:80, train_loss:0.000897, train_acc:0.994488, test_loss:0.017823, test_acc:0.933594\n",
      "->epoch:81, train_loss:0.000845, train_acc:0.993938, test_loss:0.019807, test_acc:0.934375\n",
      "->epoch:82, train_loss:0.000782, train_acc:0.994025, test_loss:0.022977, test_acc:0.925781\n",
      "->epoch:83, train_loss:0.001189, train_acc:0.992347, test_loss:0.026251, test_acc:0.932813\n",
      "->epoch:84, train_loss:0.000835, train_acc:0.994488, test_loss:0.019400, test_acc:0.932292\n",
      "->epoch:85, train_loss:0.000961, train_acc:0.993446, test_loss:0.017649, test_acc:0.931771\n",
      "->epoch:86, train_loss:0.001558, train_acc:0.990003, test_loss:0.017389, test_acc:0.928125\n",
      "->epoch:87, train_loss:0.001020, train_acc:0.994126, test_loss:0.025152, test_acc:0.929297\n",
      "->epoch:88, train_loss:0.000744, train_acc:0.995790, test_loss:0.018072, test_acc:0.932162\n",
      "->epoch:89, train_loss:0.001184, train_acc:0.993027, test_loss:0.023562, test_acc:0.932552\n",
      "->epoch:90, train_loss:0.001144, train_acc:0.992911, test_loss:0.021672, test_acc:0.934635\n",
      "->epoch:91, train_loss:0.000883, train_acc:0.994618, test_loss:0.019908, test_acc:0.932682\n",
      "->epoch:92, train_loss:0.001398, train_acc:0.992723, test_loss:0.014953, test_acc:0.928125\n",
      "->epoch:93, train_loss:0.000881, train_acc:0.993981, test_loss:0.018286, test_acc:0.924870\n",
      "->epoch:94, train_loss:0.001079, train_acc:0.993446, test_loss:0.033522, test_acc:0.929036\n",
      "->epoch:95, train_loss:0.001019, train_acc:0.993938, test_loss:0.019633, test_acc:0.924349\n",
      "->epoch:96, train_loss:0.001201, train_acc:0.993490, test_loss:0.021240, test_acc:0.928385\n",
      "->epoch:97, train_loss:0.000870, train_acc:0.994647, test_loss:0.015127, test_acc:0.929427\n",
      "->epoch:98, train_loss:0.000890, train_acc:0.994242, test_loss:0.019385, test_acc:0.931641\n",
      "->epoch:99, train_loss:0.000810, train_acc:0.995067, test_loss:0.012796, test_acc:0.911979\n",
      "->epoch:100, train_loss:0.001074, train_acc:0.992925, test_loss:0.015176, test_acc:0.925000\n",
      "->epoch:101, train_loss:0.001085, train_acc:0.993186, test_loss:0.013005, test_acc:0.922656\n",
      "->epoch:102, train_loss:0.000850, train_acc:0.994632, test_loss:0.027621, test_acc:0.934505\n",
      "->epoch:103, train_loss:0.000785, train_acc:0.994835, test_loss:0.022087, test_acc:0.930078\n",
      "->epoch:104, train_loss:0.001238, train_acc:0.992607, test_loss:0.025649, test_acc:0.925911\n",
      "->epoch:105, train_loss:0.000819, train_acc:0.994719, test_loss:0.018872, test_acc:0.927604\n",
      "->epoch:106, train_loss:0.001554, train_acc:0.991508, test_loss:0.018799, test_acc:0.928516\n",
      "->epoch:107, train_loss:0.000670, train_acc:0.995544, test_loss:0.021990, test_acc:0.928125\n",
      "->epoch:108, train_loss:0.002179, train_acc:0.993186, test_loss:0.016309, test_acc:0.919141\n",
      "->epoch:109, train_loss:0.001493, train_acc:0.991884, test_loss:0.015538, test_acc:0.927604\n",
      "->epoch:110, train_loss:0.000900, train_acc:0.994676, test_loss:0.025130, test_acc:0.929818\n",
      "->epoch:111, train_loss:0.000665, train_acc:0.995833, test_loss:0.020368, test_acc:0.933594\n",
      "->epoch:112, train_loss:0.001346, train_acc:0.993273, test_loss:0.029820, test_acc:0.928385\n",
      "->epoch:113, train_loss:0.001491, train_acc:0.991942, test_loss:0.014309, test_acc:0.933333\n",
      "->epoch:114, train_loss:0.001514, train_acc:0.993764, test_loss:0.019059, test_acc:0.909766\n",
      "->epoch:115, train_loss:0.000945, train_acc:0.993822, test_loss:0.016163, test_acc:0.933203\n",
      "->epoch:116, train_loss:0.001046, train_acc:0.994025, test_loss:0.021739, test_acc:0.931510\n",
      "->epoch:117, train_loss:0.000912, train_acc:0.994444, test_loss:0.029011, test_acc:0.932813\n",
      "->epoch:118, train_loss:0.000856, train_acc:0.995009, test_loss:0.025855, test_acc:0.926432\n",
      "->epoch:119, train_loss:0.001548, train_acc:0.991291, test_loss:0.014542, test_acc:0.895182\n",
      "->epoch:120, train_loss:0.001123, train_acc:0.993345, test_loss:0.025880, test_acc:0.926432\n",
      "->epoch:121, train_loss:0.001361, train_acc:0.992694, test_loss:0.036208, test_acc:0.919010\n",
      "->epoch:122, train_loss:0.001190, train_acc:0.993330, test_loss:0.018660, test_acc:0.923438\n",
      "->epoch:123, train_loss:0.000855, train_acc:0.995009, test_loss:0.014833, test_acc:0.926953\n",
      "->epoch:124, train_loss:0.002658, train_acc:0.987601, test_loss:0.034790, test_acc:0.928385\n",
      "->epoch:125, train_loss:0.000830, train_acc:0.995284, test_loss:0.022712, test_acc:0.931120\n",
      "->epoch:126, train_loss:0.001298, train_acc:0.993113, test_loss:0.025855, test_acc:0.926953\n",
      "->epoch:127, train_loss:0.001197, train_acc:0.993576, test_loss:0.027116, test_acc:0.923568\n",
      "->epoch:128, train_loss:0.001154, train_acc:0.993750, test_loss:0.029651, test_acc:0.926563\n",
      "->epoch:129, train_loss:0.001208, train_acc:0.992896, test_loss:0.019131, test_acc:0.930208\n",
      "->epoch:130, train_loss:0.001461, train_acc:0.993793, test_loss:0.016544, test_acc:0.929297\n",
      "->epoch:131, train_loss:0.000848, train_acc:0.994806, test_loss:0.017731, test_acc:0.911068\n",
      "->epoch:132, train_loss:0.001186, train_acc:0.993707, test_loss:0.050859, test_acc:0.931771\n",
      "->epoch:133, train_loss:0.003252, train_acc:0.989786, test_loss:0.027122, test_acc:0.929427\n",
      "->epoch:134, train_loss:0.000746, train_acc:0.995862, test_loss:0.034401, test_acc:0.935156\n",
      "->epoch:135, train_loss:0.001243, train_acc:0.993822, test_loss:0.034859, test_acc:0.933464\n",
      "->epoch:136, train_loss:0.002035, train_acc:0.989829, test_loss:0.021215, test_acc:0.928125\n",
      "->epoch:137, train_loss:0.000994, train_acc:0.994661, test_loss:0.015153, test_acc:0.921484\n",
      "->epoch:138, train_loss:0.001011, train_acc:0.993591, test_loss:0.024693, test_acc:0.927083\n",
      "->epoch:139, train_loss:0.000888, train_acc:0.995038, test_loss:0.015552, test_acc:0.930469\n",
      "->epoch:140, train_loss:0.002560, train_acc:0.990495, test_loss:0.028280, test_acc:0.917057\n",
      "->epoch:141, train_loss:0.000939, train_acc:0.994850, test_loss:0.026056, test_acc:0.932031\n",
      "->epoch:142, train_loss:0.000692, train_acc:0.996513, test_loss:0.025849, test_acc:0.931250\n",
      "->epoch:143, train_loss:0.001974, train_acc:0.990394, test_loss:0.029396, test_acc:0.932682\n",
      "->epoch:144, train_loss:0.000719, train_acc:0.995732, test_loss:0.027386, test_acc:0.930078\n",
      "->epoch:145, train_loss:0.000832, train_acc:0.995081, test_loss:0.023789, test_acc:0.932943\n",
      "->epoch:146, train_loss:0.002899, train_acc:0.984881, test_loss:0.026988, test_acc:0.933984\n",
      "->epoch:147, train_loss:0.000813, train_acc:0.995341, test_loss:0.055651, test_acc:0.926693\n",
      "->epoch:148, train_loss:0.002744, train_acc:0.986458, test_loss:0.067621, test_acc:0.921875\n",
      "->epoch:149, train_loss:0.001804, train_acc:0.991826, test_loss:0.027701, test_acc:0.928516\n",
      "->epoch:150, train_loss:0.000879, train_acc:0.994965, test_loss:0.021260, test_acc:0.929167\n",
      "->epoch:151, train_loss:0.001358, train_acc:0.992839, test_loss:0.132631, test_acc:0.910417\n",
      "->epoch:152, train_loss:0.001672, train_acc:0.989844, test_loss:0.026537, test_acc:0.925651\n",
      "->epoch:153, train_loss:0.002364, train_acc:0.986429, test_loss:0.035116, test_acc:0.929167\n",
      "->epoch:154, train_loss:0.001487, train_acc:0.991160, test_loss:0.024889, test_acc:0.929557\n",
      "->epoch:155, train_loss:0.001106, train_acc:0.993808, test_loss:0.081657, test_acc:0.928255\n",
      "->epoch:156, train_loss:0.002582, train_acc:0.987023, test_loss:0.013857, test_acc:0.910677\n",
      "->epoch:157, train_loss:0.000723, train_acc:0.995978, test_loss:0.015989, test_acc:0.925781\n",
      "->epoch:158, train_loss:0.001411, train_acc:0.993244, test_loss:0.015304, test_acc:0.933984\n",
      "->epoch:159, train_loss:0.000995, train_acc:0.994792, test_loss:0.024878, test_acc:0.930990\n",
      "->epoch:160, train_loss:0.002263, train_acc:0.988368, test_loss:0.080372, test_acc:0.929036\n",
      "->epoch:161, train_loss:0.001860, train_acc:0.991667, test_loss:0.012306, test_acc:0.908594\n",
      "->epoch:162, train_loss:0.000941, train_acc:0.994502, test_loss:0.048930, test_acc:0.931641\n",
      "->epoch:163, train_loss:0.001674, train_acc:0.990162, test_loss:0.018730, test_acc:0.929948\n",
      "->epoch:164, train_loss:0.002407, train_acc:0.988759, test_loss:0.087742, test_acc:0.902865\n",
      "->epoch:165, train_loss:0.001864, train_acc:0.990162, test_loss:0.017146, test_acc:0.932813\n",
      "->epoch:166, train_loss:0.001179, train_acc:0.993634, test_loss:0.014204, test_acc:0.934375\n",
      "->epoch:167, train_loss:0.001465, train_acc:0.992086, test_loss:0.045887, test_acc:0.929557\n",
      "->epoch:168, train_loss:0.001153, train_acc:0.994010, test_loss:0.042376, test_acc:0.927865\n",
      "->epoch:169, train_loss:0.001224, train_acc:0.993562, test_loss:0.019416, test_acc:0.823698\n",
      "->epoch:170, train_loss:0.001811, train_acc:0.992824, test_loss:0.025380, test_acc:0.932422\n",
      "->epoch:171, train_loss:0.001018, train_acc:0.994777, test_loss:0.028114, test_acc:0.925521\n",
      "->epoch:172, train_loss:0.002044, train_acc:0.990379, test_loss:0.031337, test_acc:0.932682\n",
      "->epoch:173, train_loss:0.001172, train_acc:0.993200, test_loss:0.047957, test_acc:0.922396\n",
      "->epoch:174, train_loss:0.002661, train_acc:0.987153, test_loss:0.036317, test_acc:0.924479\n",
      "->epoch:175, train_loss:0.001282, train_acc:0.993605, test_loss:0.017753, test_acc:0.930990\n",
      "->epoch:176, train_loss:0.001385, train_acc:0.993316, test_loss:0.024571, test_acc:0.928646\n",
      "->epoch:177, train_loss:0.001836, train_acc:0.990509, test_loss:0.021296, test_acc:0.930339\n",
      "->epoch:178, train_loss:0.001143, train_acc:0.994227, test_loss:0.086084, test_acc:0.926563\n",
      "->epoch:179, train_loss:0.001771, train_acc:0.990784, test_loss:0.018609, test_acc:0.928255\n",
      "->epoch:180, train_loss:0.001506, train_acc:0.992361, test_loss:0.030604, test_acc:0.928516\n",
      "->epoch:181, train_loss:0.001121, train_acc:0.994126, test_loss:0.030358, test_acc:0.932552\n",
      "->epoch:182, train_loss:0.001708, train_acc:0.991565, test_loss:0.080953, test_acc:0.910938\n",
      "->epoch:183, train_loss:0.002071, train_acc:0.990466, test_loss:0.014670, test_acc:0.925781\n",
      "->epoch:184, train_loss:0.002195, train_acc:0.988802, test_loss:0.066548, test_acc:0.925000\n",
      "->epoch:185, train_loss:0.001774, train_acc:0.992925, test_loss:0.029734, test_acc:0.928646\n",
      "->epoch:186, train_loss:0.003925, train_acc:0.985706, test_loss:0.015367, test_acc:0.884375\n",
      "->epoch:187, train_loss:0.001410, train_acc:0.989887, test_loss:0.023132, test_acc:0.928255\n",
      "->epoch:188, train_loss:0.001839, train_acc:0.991435, test_loss:0.071688, test_acc:0.916406\n",
      "->epoch:189, train_loss:0.001669, train_acc:0.990466, test_loss:0.028290, test_acc:0.923177\n",
      "->epoch:190, train_loss:0.001190, train_acc:0.994517, test_loss:0.020944, test_acc:0.931510\n",
      "->epoch:191, train_loss:0.000659, train_acc:0.996470, test_loss:0.033818, test_acc:0.933464\n",
      "->epoch:192, train_loss:0.002783, train_acc:0.986646, test_loss:0.014812, test_acc:0.923698\n",
      "->epoch:193, train_loss:0.002198, train_acc:0.988223, test_loss:0.076137, test_acc:0.926563\n",
      "->epoch:194, train_loss:0.001084, train_acc:0.994213, test_loss:0.053638, test_acc:0.927344\n",
      "->epoch:195, train_loss:0.002332, train_acc:0.989149, test_loss:0.022424, test_acc:0.928516\n",
      "->epoch:196, train_loss:0.000920, train_acc:0.994922, test_loss:0.026837, test_acc:0.926823\n",
      "->epoch:197, train_loss:0.003769, train_acc:0.983247, test_loss:0.079196, test_acc:0.916276\n",
      "->epoch:198, train_loss:0.001374, train_acc:0.991927, test_loss:0.021372, test_acc:0.933464\n",
      "->epoch:199, train_loss:0.000935, train_acc:0.995197, test_loss:0.035595, test_acc:0.929036\n",
      "->epoch:200, train_loss:0.001981, train_acc:0.989091, test_loss:0.059015, test_acc:0.924870\n",
      "->epoch:201, train_loss:0.001586, train_acc:0.991146, test_loss:0.080527, test_acc:0.910026\n",
      "->epoch:202, train_loss:0.003807, train_acc:0.983970, test_loss:0.019338, test_acc:0.930339\n",
      "->epoch:203, train_loss:0.001176, train_acc:0.993302, test_loss:0.027690, test_acc:0.929818\n",
      "->epoch:204, train_loss:0.000946, train_acc:0.994473, test_loss:0.024857, test_acc:0.932031\n",
      "->epoch:205, train_loss:0.001127, train_acc:0.994300, test_loss:0.031933, test_acc:0.931510\n",
      "->epoch:206, train_loss:0.018554, train_acc:0.956467, test_loss:0.137820, test_acc:0.897266\n",
      "->epoch:207, train_loss:0.003579, train_acc:0.975203, test_loss:0.115487, test_acc:0.917969\n",
      "->epoch:208, train_loss:0.001851, train_acc:0.992578, test_loss:0.062671, test_acc:0.924089\n",
      "->epoch:209, train_loss:0.010865, train_acc:0.979543, test_loss:0.027505, test_acc:0.925260\n",
      "->epoch:210, train_loss:0.001852, train_acc:0.992231, test_loss:0.032610, test_acc:0.925260\n",
      "->epoch:211, train_loss:0.001695, train_acc:0.991782, test_loss:0.033519, test_acc:0.927474\n",
      "->epoch:212, train_loss:0.001087, train_acc:0.994415, test_loss:0.023448, test_acc:0.916537\n",
      "->epoch:213, train_loss:0.004301, train_acc:0.980628, test_loss:0.034315, test_acc:0.917708\n",
      "->epoch:214, train_loss:0.001163, train_acc:0.993099, test_loss:0.011308, test_acc:0.927734\n",
      "->epoch:215, train_loss:0.001817, train_acc:0.991479, test_loss:0.117411, test_acc:0.906250\n",
      "->epoch:216, train_loss:0.001838, train_acc:0.990495, test_loss:0.117083, test_acc:0.925651\n",
      "->epoch:217, train_loss:0.002120, train_acc:0.990379, test_loss:0.034307, test_acc:0.930339\n",
      "->epoch:218, train_loss:0.002843, train_acc:0.990394, test_loss:0.012616, test_acc:0.929948\n",
      "->epoch:219, train_loss:0.000920, train_acc:0.994907, test_loss:0.014762, test_acc:0.931510\n",
      "->epoch:220, train_loss:0.002511, train_acc:0.987037, test_loss:0.084744, test_acc:0.923307\n",
      "->epoch:221, train_loss:0.001744, train_acc:0.990567, test_loss:0.021207, test_acc:0.926172\n",
      "->epoch:222, train_loss:0.002593, train_acc:0.987211, test_loss:0.045424, test_acc:0.912760\n",
      "->epoch:223, train_loss:0.001470, train_acc:0.991942, test_loss:0.111112, test_acc:0.893490\n",
      "->epoch:224, train_loss:0.003872, train_acc:0.980483, test_loss:0.046408, test_acc:0.928906\n",
      "->epoch:225, train_loss:0.001041, train_acc:0.994285, test_loss:0.060408, test_acc:0.931120\n",
      "->epoch:226, train_loss:0.004580, train_acc:0.981944, test_loss:0.070224, test_acc:0.898177\n",
      "->epoch:227, train_loss:0.002824, train_acc:0.981236, test_loss:0.014461, test_acc:0.927865\n",
      "->epoch:228, train_loss:0.000757, train_acc:0.995819, test_loss:0.046966, test_acc:0.930859\n",
      "->epoch:229, train_loss:0.001595, train_acc:0.991652, test_loss:0.082333, test_acc:0.924609\n",
      "->epoch:230, train_loss:0.001192, train_acc:0.993605, test_loss:0.067339, test_acc:0.930339\n",
      "->epoch:231, train_loss:0.004555, train_acc:0.973264, test_loss:0.063840, test_acc:0.903776\n",
      "->epoch:232, train_loss:0.001738, train_acc:0.990162, test_loss:0.014007, test_acc:0.930599\n",
      "->epoch:233, train_loss:0.001156, train_acc:0.994083, test_loss:0.027672, test_acc:0.930469\n",
      "->epoch:234, train_loss:0.003726, train_acc:0.979239, test_loss:0.081861, test_acc:0.921094\n",
      "->epoch:235, train_loss:0.003299, train_acc:0.983174, test_loss:0.018801, test_acc:0.926172\n",
      "->epoch:236, train_loss:0.001784, train_acc:0.990683, test_loss:0.108496, test_acc:0.917969\n",
      "->epoch:237, train_loss:0.004774, train_acc:0.972179, test_loss:0.038801, test_acc:0.895703\n",
      "->epoch:238, train_loss:0.002597, train_acc:0.985663, test_loss:0.032577, test_acc:0.907552\n",
      "->epoch:239, train_loss:0.001316, train_acc:0.993345, test_loss:0.092721, test_acc:0.879948\n",
      "->epoch:240, train_loss:0.002688, train_acc:0.984954, test_loss:0.075658, test_acc:0.925781\n",
      "->epoch:241, train_loss:0.001213, train_acc:0.993461, test_loss:0.107413, test_acc:0.922266\n",
      "->epoch:242, train_loss:0.002541, train_acc:0.984549, test_loss:0.049540, test_acc:0.923698\n",
      "->epoch:243, train_loss:0.002137, train_acc:0.986400, test_loss:0.067566, test_acc:0.924349\n",
      "->epoch:244, train_loss:0.008133, train_acc:0.981438, test_loss:0.027901, test_acc:0.912240\n",
      "->epoch:245, train_loss:0.001668, train_acc:0.990741, test_loss:0.030046, test_acc:0.927344\n",
      "->epoch:246, train_loss:0.002374, train_acc:0.988050, test_loss:0.055745, test_acc:0.920573\n",
      "->epoch:247, train_loss:0.001597, train_acc:0.991377, test_loss:0.079877, test_acc:0.875521\n",
      "->epoch:248, train_loss:0.003405, train_acc:0.982205, test_loss:0.027173, test_acc:0.923828\n",
      "->epoch:249, train_loss:0.002534, train_acc:0.987196, test_loss:0.025625, test_acc:0.925260\n",
      "->epoch:250, train_loss:0.004512, train_acc:0.978964, test_loss:0.021946, test_acc:0.917057\n",
      "->epoch:251, train_loss:0.006435, train_acc:0.986675, test_loss:0.063559, test_acc:0.891016\n",
      "->epoch:252, train_loss:0.002697, train_acc:0.985012, test_loss:0.022438, test_acc:0.921354\n",
      "->epoch:253, train_loss:0.000925, train_acc:0.994922, test_loss:0.035041, test_acc:0.929297\n",
      "->epoch:254, train_loss:0.005173, train_acc:0.980150, test_loss:0.110116, test_acc:0.913542\n",
      "->epoch:255, train_loss:0.001878, train_acc:0.987659, test_loss:0.035677, test_acc:0.928906\n",
      "->epoch:256, train_loss:0.003018, train_acc:0.984418, test_loss:0.100613, test_acc:0.915104\n",
      "->epoch:257, train_loss:0.002254, train_acc:0.990046, test_loss:0.137047, test_acc:0.910417\n",
      "->epoch:258, train_loss:0.002548, train_acc:0.985937, test_loss:0.046694, test_acc:0.922917\n",
      "->epoch:259, train_loss:0.001131, train_acc:0.993851, test_loss:0.036511, test_acc:0.926823\n",
      "->epoch:260, train_loss:0.001733, train_acc:0.992405, test_loss:0.038296, test_acc:0.922266\n",
      "->epoch:261, train_loss:0.004377, train_acc:0.975072, test_loss:0.017335, test_acc:0.920443\n",
      "->epoch:262, train_loss:0.003050, train_acc:0.983883, test_loss:0.105019, test_acc:0.922005\n",
      "->epoch:263, train_loss:0.002473, train_acc:0.983579, test_loss:0.160108, test_acc:0.910026\n",
      "->epoch:264, train_loss:0.009651, train_acc:0.975680, test_loss:0.011787, test_acc:0.913672\n",
      "->epoch:265, train_loss:0.001119, train_acc:0.993287, test_loss:0.025257, test_acc:0.926172\n",
      "->epoch:266, train_loss:0.000658, train_acc:0.996369, test_loss:0.016111, test_acc:0.926302\n",
      "->epoch:267, train_loss:0.001414, train_acc:0.992896, test_loss:0.046222, test_acc:0.923958\n",
      "->epoch:268, train_loss:0.003105, train_acc:0.985619, test_loss:0.096335, test_acc:0.917969\n",
      "->epoch:269, train_loss:0.001808, train_acc:0.991638, test_loss:0.069401, test_acc:0.907682\n",
      "->epoch:270, train_loss:0.002117, train_acc:0.988252, test_loss:0.201218, test_acc:0.923568\n",
      "->epoch:271, train_loss:0.001972, train_acc:0.989916, test_loss:0.024162, test_acc:0.923698\n",
      "->epoch:272, train_loss:0.004020, train_acc:0.979688, test_loss:0.060604, test_acc:0.901432\n",
      "->epoch:273, train_loss:0.001940, train_acc:0.989120, test_loss:0.106958, test_acc:0.919662\n",
      "->epoch:274, train_loss:0.008215, train_acc:0.982465, test_loss:0.053121, test_acc:0.911979\n",
      "->epoch:275, train_loss:0.001395, train_acc:0.991580, test_loss:0.025508, test_acc:0.926823\n",
      "->epoch:276, train_loss:0.000909, train_acc:0.994777, test_loss:0.045887, test_acc:0.926953\n",
      "->epoch:277, train_loss:0.002928, train_acc:0.988874, test_loss:0.024317, test_acc:0.929557\n",
      "->epoch:278, train_loss:0.001844, train_acc:0.990958, test_loss:0.023598, test_acc:0.931510\n",
      "->epoch:279, train_loss:0.001802, train_acc:0.991811, test_loss:0.167230, test_acc:0.928255\n",
      "->epoch:280, train_loss:0.001792, train_acc:0.991117, test_loss:0.104935, test_acc:0.901302\n",
      "->epoch:281, train_loss:0.004477, train_acc:0.977156, test_loss:0.016100, test_acc:0.927995\n",
      "->epoch:282, train_loss:0.000826, train_acc:0.995862, test_loss:0.060978, test_acc:0.921354\n",
      "->epoch:283, train_loss:0.010705, train_acc:0.980382, test_loss:0.136110, test_acc:0.909375\n",
      "->epoch:284, train_loss:0.002160, train_acc:0.987355, test_loss:0.024563, test_acc:0.926563\n",
      "->epoch:285, train_loss:0.002265, train_acc:0.988932, test_loss:0.077729, test_acc:0.911328\n",
      "->epoch:286, train_loss:0.006003, train_acc:0.963267, test_loss:0.106410, test_acc:0.912109\n",
      "->epoch:287, train_loss:0.001810, train_acc:0.988339, test_loss:0.055868, test_acc:0.925391\n",
      "->epoch:288, train_loss:0.001302, train_acc:0.992448, test_loss:0.015198, test_acc:0.925130\n",
      "->epoch:289, train_loss:0.001678, train_acc:0.991204, test_loss:0.020248, test_acc:0.927344\n",
      "->epoch:290, train_loss:0.006235, train_acc:0.965524, test_loss:0.079228, test_acc:0.892188\n",
      "->epoch:291, train_loss:0.001825, train_acc:0.988845, test_loss:0.049920, test_acc:0.926432\n",
      "->epoch:292, train_loss:0.007234, train_acc:0.979499, test_loss:0.019970, test_acc:0.926823\n",
      "->epoch:293, train_loss:0.002434, train_acc:0.987182, test_loss:0.038607, test_acc:0.919010\n",
      "->epoch:294, train_loss:0.001732, train_acc:0.990929, test_loss:0.041284, test_acc:0.920052\n",
      "->epoch:295, train_loss:0.003715, train_acc:0.989207, test_loss:0.036851, test_acc:0.917708\n",
      "->epoch:296, train_loss:0.001770, train_acc:0.989858, test_loss:0.029813, test_acc:0.927865\n",
      "->epoch:297, train_loss:0.004429, train_acc:0.967723, test_loss:0.061126, test_acc:0.877474\n",
      "->epoch:298, train_loss:0.002564, train_acc:0.986126, test_loss:0.033071, test_acc:0.924740\n",
      "->epoch:299, train_loss:0.003161, train_acc:0.978284, test_loss:0.078990, test_acc:0.924349\n",
      "->epoch:300, train_loss:0.003755, train_acc:0.982726, test_loss:0.051183, test_acc:0.920182\n",
      "->epoch:301, train_loss:0.001771, train_acc:0.989468, test_loss:0.025296, test_acc:0.927344\n",
      "->epoch:302, train_loss:0.002612, train_acc:0.986444, test_loss:0.168749, test_acc:0.920182\n",
      "->epoch:303, train_loss:0.002303, train_acc:0.987775, test_loss:0.073778, test_acc:0.877344\n",
      "->epoch:304, train_loss:0.004450, train_acc:0.971846, test_loss:0.078764, test_acc:0.895052\n",
      "->epoch:305, train_loss:0.003393, train_acc:0.975188, test_loss:0.049925, test_acc:0.928646\n",
      "->epoch:306, train_loss:0.003667, train_acc:0.974031, test_loss:0.038340, test_acc:0.926302\n",
      "->epoch:307, train_loss:0.006392, train_acc:0.988918, test_loss:0.070586, test_acc:0.917318\n",
      "->epoch:308, train_loss:0.002992, train_acc:0.982870, test_loss:0.024111, test_acc:0.924089\n",
      "->epoch:309, train_loss:0.001650, train_acc:0.990162, test_loss:0.173957, test_acc:0.921484\n",
      "->epoch:310, train_loss:0.002896, train_acc:0.982567, test_loss:0.040963, test_acc:0.894661\n",
      "->epoch:311, train_loss:0.001012, train_acc:0.994039, test_loss:0.024980, test_acc:0.927604\n",
      "->epoch:312, train_loss:0.005546, train_acc:0.985446, test_loss:0.014554, test_acc:0.928646\n",
      "->epoch:313, train_loss:0.006406, train_acc:0.987891, test_loss:0.030354, test_acc:0.929557\n",
      "->epoch:314, train_loss:0.003440, train_acc:0.980035, test_loss:0.116266, test_acc:0.922266\n",
      "->epoch:315, train_loss:0.001649, train_acc:0.991160, test_loss:0.030980, test_acc:0.892969\n",
      "->epoch:316, train_loss:0.003267, train_acc:0.981366, test_loss:0.014832, test_acc:0.924609\n",
      "->epoch:317, train_loss:0.002983, train_acc:0.985532, test_loss:0.155210, test_acc:0.903776\n",
      "->epoch:318, train_loss:0.002739, train_acc:0.982306, test_loss:0.030483, test_acc:0.929557\n",
      "->epoch:319, train_loss:0.002175, train_acc:0.991059, test_loss:0.072943, test_acc:0.900000\n",
      "->epoch:320, train_loss:0.003765, train_acc:0.981221, test_loss:0.026050, test_acc:0.922135\n",
      "->epoch:321, train_loss:0.000989, train_acc:0.994821, test_loss:0.032416, test_acc:0.926823\n",
      "->epoch:322, train_loss:0.001229, train_acc:0.993403, test_loss:0.044554, test_acc:0.925391\n",
      "->epoch:323, train_loss:0.004010, train_acc:0.978110, test_loss:0.053080, test_acc:0.895443\n",
      "->epoch:324, train_loss:0.001099, train_acc:0.993128, test_loss:0.155448, test_acc:0.930859\n",
      "->epoch:325, train_loss:0.009094, train_acc:0.969083, test_loss:0.047879, test_acc:0.914583\n",
      "->epoch:326, train_loss:0.101406, train_acc:0.915683, test_loss:0.045268, test_acc:0.869662\n",
      "->epoch:327, train_loss:0.012288, train_acc:0.902098, test_loss:0.055283, test_acc:0.896745\n",
      "->epoch:328, train_loss:0.006057, train_acc:0.956684, test_loss:0.048584, test_acc:0.887370\n",
      "->epoch:329, train_loss:0.003356, train_acc:0.979094, test_loss:0.020290, test_acc:0.919010\n",
      "->epoch:330, train_loss:0.001145, train_acc:0.993953, test_loss:0.027458, test_acc:0.924219\n",
      "->epoch:331, train_loss:0.002180, train_acc:0.988889, test_loss:0.024181, test_acc:0.927604\n",
      "->epoch:332, train_loss:0.000976, train_acc:0.994850, test_loss:0.280865, test_acc:0.923568\n",
      "->epoch:333, train_loss:0.004412, train_acc:0.975420, test_loss:0.025168, test_acc:0.920964\n",
      "->epoch:334, train_loss:0.002784, train_acc:0.985749, test_loss:0.025340, test_acc:0.923307\n",
      "->epoch:335, train_loss:0.001495, train_acc:0.992274, test_loss:0.078247, test_acc:0.926172\n",
      "->epoch:336, train_loss:0.009429, train_acc:0.988310, test_loss:0.114330, test_acc:0.920573\n",
      "->epoch:337, train_loss:0.002494, train_acc:0.986979, test_loss:0.051214, test_acc:0.926042\n",
      "->epoch:338, train_loss:0.001136, train_acc:0.994285, test_loss:0.109311, test_acc:0.920833\n",
      "->epoch:339, train_loss:0.001757, train_acc:0.990234, test_loss:0.079232, test_acc:0.924219\n",
      "->epoch:340, train_loss:0.001267, train_acc:0.992737, test_loss:0.081144, test_acc:0.923958\n",
      "->epoch:341, train_loss:0.004367, train_acc:0.980845, test_loss:0.057716, test_acc:0.924870\n",
      "->epoch:342, train_loss:0.001817, train_acc:0.991160, test_loss:0.292055, test_acc:0.876823\n",
      "->epoch:343, train_loss:0.004200, train_acc:0.979789, test_loss:0.037648, test_acc:0.924479\n",
      "->epoch:344, train_loss:0.000784, train_acc:0.995501, test_loss:0.065235, test_acc:0.930078\n",
      "->epoch:345, train_loss:0.009740, train_acc:0.982740, test_loss:0.067388, test_acc:0.916146\n",
      "->epoch:346, train_loss:0.000806, train_acc:0.995182, test_loss:0.046624, test_acc:0.927214\n",
      "->epoch:347, train_loss:0.004826, train_acc:0.987442, test_loss:0.035367, test_acc:0.919922\n",
      "->epoch:348, train_loss:0.001349, train_acc:0.992969, test_loss:0.206911, test_acc:0.913542\n",
      "->epoch:349, train_loss:0.002891, train_acc:0.979123, test_loss:0.169690, test_acc:0.916537\n",
      "->epoch:350, train_loss:0.009843, train_acc:0.977459, test_loss:0.116974, test_acc:0.901693\n",
      "->epoch:351, train_loss:0.007435, train_acc:0.977792, test_loss:0.051228, test_acc:0.921484\n",
      "->epoch:352, train_loss:0.000972, train_acc:0.994387, test_loss:0.115031, test_acc:0.923438\n",
      "->epoch:353, train_loss:0.001920, train_acc:0.989439, test_loss:0.133470, test_acc:0.862109\n",
      "->epoch:354, train_loss:0.003974, train_acc:0.980628, test_loss:0.152846, test_acc:0.915234\n",
      "->epoch:355, train_loss:0.001335, train_acc:0.992347, test_loss:0.113804, test_acc:0.917057\n",
      "->epoch:356, train_loss:0.001693, train_acc:0.991319, test_loss:0.047584, test_acc:0.926042\n",
      "->epoch:357, train_loss:0.005459, train_acc:0.971152, test_loss:0.295186, test_acc:0.897917\n",
      "->epoch:358, train_loss:0.007769, train_acc:0.952517, test_loss:0.113456, test_acc:0.908594\n",
      "->epoch:359, train_loss:0.002099, train_acc:0.987891, test_loss:0.295201, test_acc:0.918620\n",
      "->epoch:360, train_loss:0.005680, train_acc:0.977546, test_loss:0.039089, test_acc:0.919531\n",
      "->epoch:361, train_loss:0.000968, train_acc:0.994517, test_loss:0.066306, test_acc:0.924740\n",
      "->epoch:362, train_loss:0.004065, train_acc:0.979225, test_loss:0.074284, test_acc:0.925000\n",
      "->epoch:363, train_loss:0.001084, train_acc:0.994126, test_loss:0.035606, test_acc:0.922005\n",
      "->epoch:364, train_loss:0.002975, train_acc:0.988860, test_loss:0.155913, test_acc:0.912370\n",
      "->epoch:365, train_loss:0.002886, train_acc:0.983044, test_loss:0.061363, test_acc:0.921224\n",
      "->epoch:366, train_loss:0.005070, train_acc:0.966898, test_loss:0.084976, test_acc:0.873958\n",
      "->epoch:367, train_loss:0.001901, train_acc:0.986632, test_loss:0.052591, test_acc:0.923958\n",
      "->epoch:368, train_loss:0.000967, train_acc:0.994415, test_loss:0.097722, test_acc:0.919141\n",
      "->epoch:369, train_loss:0.007629, train_acc:0.980830, test_loss:0.067754, test_acc:0.916406\n",
      "->epoch:370, train_loss:0.007432, train_acc:0.982147, test_loss:0.246956, test_acc:0.894922\n",
      "->epoch:371, train_loss:0.364129, train_acc:0.963455, test_loss:0.160012, test_acc:0.898438\n",
      "->epoch:372, train_loss:0.006099, train_acc:0.955411, test_loss:0.087979, test_acc:0.889583\n",
      "->epoch:373, train_loss:0.004620, train_acc:0.969633, test_loss:0.140230, test_acc:0.906510\n",
      "->epoch:374, train_loss:0.002690, train_acc:0.984549, test_loss:0.099289, test_acc:0.921615\n",
      "->epoch:375, train_loss:0.001102, train_acc:0.994083, test_loss:0.037886, test_acc:0.923958\n",
      "->epoch:376, train_loss:0.001014, train_acc:0.993851, test_loss:0.046471, test_acc:0.922396\n",
      "->epoch:377, train_loss:0.003537, train_acc:0.979572, test_loss:0.318465, test_acc:0.922787\n",
      "->epoch:378, train_loss:0.026800, train_acc:0.984115, test_loss:0.151483, test_acc:0.892057\n",
      "->epoch:379, train_loss:0.003729, train_acc:0.976056, test_loss:0.101969, test_acc:0.921745\n",
      "->epoch:380, train_loss:0.000811, train_acc:0.995385, test_loss:0.108424, test_acc:0.928906\n",
      "->epoch:381, train_loss:0.001131, train_acc:0.994300, test_loss:0.168054, test_acc:0.922656\n",
      "->epoch:382, train_loss:0.001142, train_acc:0.993981, test_loss:0.079301, test_acc:0.919141\n",
      "->epoch:383, train_loss:0.002755, train_acc:0.982870, test_loss:0.292383, test_acc:0.923047\n",
      "->epoch:384, train_loss:0.012468, train_acc:0.949624, test_loss:0.121760, test_acc:0.887760\n",
      "->epoch:385, train_loss:0.006036, train_acc:0.972439, test_loss:0.067235, test_acc:0.915755\n",
      "->epoch:386, train_loss:0.002095, train_acc:0.988643, test_loss:0.019700, test_acc:0.925781\n",
      "->epoch:387, train_loss:0.001253, train_acc:0.992477, test_loss:0.124254, test_acc:0.891146\n",
      "->epoch:388, train_loss:0.001776, train_acc:0.989583, test_loss:0.208627, test_acc:0.910026\n",
      "->epoch:389, train_loss:0.002844, train_acc:0.981264, test_loss:0.221044, test_acc:0.903516\n",
      "->epoch:390, train_loss:0.011811, train_acc:0.969994, test_loss:0.089977, test_acc:0.912370\n",
      "->epoch:391, train_loss:0.006292, train_acc:0.980801, test_loss:0.181695, test_acc:0.815625\n",
      "->epoch:392, train_loss:0.006906, train_acc:0.968186, test_loss:0.121804, test_acc:0.901693\n",
      "->epoch:393, train_loss:0.003061, train_acc:0.982885, test_loss:0.161178, test_acc:0.911849\n",
      "->epoch:394, train_loss:0.002499, train_acc:0.983681, test_loss:0.243053, test_acc:0.917188\n",
      "->epoch:395, train_loss:0.003626, train_acc:0.976649, test_loss:0.190382, test_acc:0.907682\n",
      "->epoch:396, train_loss:0.009626, train_acc:0.982943, test_loss:0.495026, test_acc:0.902995\n",
      "->epoch:397, train_loss:0.020679, train_acc:0.967448, test_loss:0.088427, test_acc:0.904948\n",
      "->epoch:398, train_loss:0.001859, train_acc:0.988238, test_loss:0.054343, test_acc:0.926693\n",
      "->epoch:399, train_loss:0.002109, train_acc:0.986878, test_loss:0.173679, test_acc:0.879688\n",
      "->epoch:400, train_loss:0.002207, train_acc:0.990437, test_loss:0.190247, test_acc:0.924349\n",
      "->epoch:401, train_loss:0.001229, train_acc:0.992781, test_loss:0.141242, test_acc:0.927344\n",
      "->epoch:402, train_loss:0.005105, train_acc:0.989222, test_loss:0.118122, test_acc:0.848047\n",
      "->epoch:403, train_loss:0.003271, train_acc:0.980237, test_loss:0.170484, test_acc:0.920182\n",
      "->epoch:404, train_loss:0.000859, train_acc:0.995168, test_loss:0.070645, test_acc:0.911068\n",
      "->epoch:405, train_loss:0.003253, train_acc:0.984158, test_loss:0.024504, test_acc:0.917448\n",
      "->epoch:406, train_loss:0.000805, train_acc:0.995761, test_loss:0.191689, test_acc:0.925391\n",
      "->epoch:407, train_loss:0.009740, train_acc:0.985417, test_loss:0.110892, test_acc:0.916797\n",
      "->epoch:408, train_loss:0.001456, train_acc:0.991826, test_loss:0.037075, test_acc:0.926563\n",
      "->epoch:409, train_loss:0.000879, train_acc:0.995067, test_loss:0.214902, test_acc:0.917318\n",
      "->epoch:410, train_loss:0.004950, train_acc:0.979470, test_loss:0.147626, test_acc:0.922656\n",
      "->epoch:411, train_loss:0.002932, train_acc:0.992520, test_loss:0.213553, test_acc:0.917318\n",
      "->epoch:412, train_loss:0.003655, train_acc:0.976013, test_loss:0.127043, test_acc:0.920964\n",
      "->epoch:413, train_loss:0.001726, train_acc:0.988730, test_loss:0.196576, test_acc:0.905990\n",
      "->epoch:414, train_loss:0.008119, train_acc:0.983666, test_loss:0.078498, test_acc:0.911198\n",
      "->epoch:415, train_loss:0.003922, train_acc:0.982885, test_loss:0.038540, test_acc:0.919271\n",
      "->epoch:416, train_loss:0.001002, train_acc:0.994546, test_loss:0.057572, test_acc:0.920964\n",
      "->epoch:417, train_loss:0.000836, train_acc:0.995515, test_loss:0.068793, test_acc:0.923177\n",
      "->epoch:418, train_loss:0.014204, train_acc:0.970457, test_loss:0.123317, test_acc:0.905078\n",
      "->epoch:419, train_loss:0.003743, train_acc:0.978472, test_loss:0.199138, test_acc:0.919922\n",
      "->epoch:420, train_loss:0.001607, train_acc:0.991580, test_loss:0.063145, test_acc:0.926432\n",
      "->epoch:421, train_loss:0.003625, train_acc:0.989439, test_loss:0.050971, test_acc:0.924479\n",
      "->epoch:422, train_loss:0.000757, train_acc:0.995790, test_loss:0.129044, test_acc:0.922135\n",
      "->epoch:423, train_loss:0.005627, train_acc:0.981525, test_loss:0.089886, test_acc:0.878255\n",
      "->epoch:424, train_loss:0.012081, train_acc:0.974089, test_loss:0.150987, test_acc:0.914323\n",
      "->epoch:425, train_loss:0.002115, train_acc:0.988165, test_loss:0.425078, test_acc:0.891797\n",
      "->epoch:426, train_loss:0.005509, train_acc:0.976635, test_loss:0.072227, test_acc:0.911328\n",
      "->epoch:427, train_loss:0.001689, train_acc:0.990712, test_loss:0.090830, test_acc:0.918490\n",
      "->epoch:428, train_loss:0.000819, train_acc:0.995616, test_loss:0.056598, test_acc:0.920443\n",
      "->epoch:429, train_loss:0.006329, train_acc:0.981713, test_loss:0.248484, test_acc:0.916927\n",
      "->epoch:430, train_loss:0.002770, train_acc:0.983608, test_loss:0.087050, test_acc:0.906250\n",
      "->epoch:431, train_loss:0.001005, train_acc:0.993981, test_loss:0.097921, test_acc:0.922396\n",
      "->epoch:432, train_loss:0.002901, train_acc:0.985214, test_loss:0.210329, test_acc:0.917318\n",
      "->epoch:433, train_loss:0.006046, train_acc:0.975723, test_loss:0.101903, test_acc:0.914714\n",
      "->epoch:434, train_loss:0.002989, train_acc:0.987746, test_loss:0.028896, test_acc:0.923047\n",
      "->epoch:435, train_loss:0.002293, train_acc:0.985286, test_loss:0.110495, test_acc:0.899740\n",
      "->epoch:436, train_loss:0.003791, train_acc:0.987572, test_loss:0.046272, test_acc:0.922396\n",
      "->epoch:437, train_loss:0.001077, train_acc:0.994401, test_loss:0.057003, test_acc:0.924219\n",
      "->epoch:438, train_loss:0.002600, train_acc:0.984433, test_loss:0.304599, test_acc:0.886328\n",
      "->epoch:439, train_loss:0.010302, train_acc:0.969010, test_loss:0.098246, test_acc:0.915755\n",
      "->epoch:440, train_loss:0.000923, train_acc:0.995038, test_loss:0.093369, test_acc:0.917188\n",
      "->epoch:441, train_loss:0.003170, train_acc:0.982436, test_loss:0.173680, test_acc:0.920313\n",
      "->epoch:442, train_loss:0.003018, train_acc:0.983232, test_loss:0.161844, test_acc:0.922266\n",
      "->epoch:443, train_loss:0.001198, train_acc:0.992954, test_loss:0.212070, test_acc:0.913542\n",
      "->epoch:444, train_loss:0.009042, train_acc:0.982422, test_loss:0.034425, test_acc:0.925521\n",
      "->epoch:445, train_loss:0.003772, train_acc:0.979470, test_loss:0.140590, test_acc:0.916667\n",
      "->epoch:446, train_loss:0.001697, train_acc:0.992867, test_loss:0.139889, test_acc:0.919010\n",
      "->epoch:447, train_loss:0.000835, train_acc:0.995284, test_loss:0.243718, test_acc:0.914974\n",
      "->epoch:448, train_loss:0.001821, train_acc:0.987920, test_loss:0.520991, test_acc:0.903906\n",
      "->epoch:449, train_loss:0.024516, train_acc:0.958131, test_loss:0.116667, test_acc:0.894010\n",
      "->epoch:450, train_loss:0.002977, train_acc:0.980671, test_loss:0.047667, test_acc:0.924479\n",
      "->epoch:451, train_loss:0.001876, train_acc:0.989062, test_loss:0.202693, test_acc:0.923177\n",
      "->epoch:452, train_loss:0.002574, train_acc:0.983608, test_loss:0.190601, test_acc:0.898177\n",
      "->epoch:453, train_loss:0.006436, train_acc:0.983275, test_loss:0.056857, test_acc:0.912240\n",
      "->epoch:454, train_loss:0.002817, train_acc:0.986285, test_loss:0.184611, test_acc:0.920703\n",
      "->epoch:455, train_loss:0.001576, train_acc:0.990119, test_loss:0.095920, test_acc:0.923958\n",
      "->epoch:456, train_loss:0.003099, train_acc:0.983492, test_loss:0.197120, test_acc:0.913021\n",
      "->epoch:457, train_loss:0.015284, train_acc:0.934230, test_loss:0.151718, test_acc:0.886068\n",
      "->epoch:458, train_loss:0.076963, train_acc:0.951418, test_loss:0.031005, test_acc:0.906380\n",
      "->epoch:459, train_loss:0.001481, train_acc:0.991262, test_loss:0.025833, test_acc:0.924609\n",
      "->epoch:460, train_loss:0.001526, train_acc:0.991840, test_loss:0.275944, test_acc:0.909245\n",
      "->epoch:461, train_loss:0.021848, train_acc:0.975246, test_loss:0.080409, test_acc:0.916276\n",
      "->epoch:462, train_loss:0.002225, train_acc:0.993533, test_loss:0.036158, test_acc:0.921615\n",
      "->epoch:463, train_loss:0.000541, train_acc:0.997049, test_loss:0.067930, test_acc:0.913802\n",
      "->epoch:464, train_loss:0.001910, train_acc:0.990408, test_loss:0.050855, test_acc:0.920964\n",
      "->epoch:465, train_loss:0.001153, train_acc:0.994531, test_loss:0.057021, test_acc:0.920703\n",
      "->epoch:466, train_loss:0.002101, train_acc:0.989207, test_loss:0.058036, test_acc:0.921745\n",
      "->epoch:467, train_loss:0.001297, train_acc:0.992911, test_loss:0.125148, test_acc:0.918359\n",
      "->epoch:468, train_loss:0.004095, train_acc:0.983565, test_loss:0.038259, test_acc:0.920573\n",
      "->epoch:469, train_loss:0.001406, train_acc:0.991276, test_loss:0.042138, test_acc:0.922526\n",
      "->epoch:470, train_loss:0.008216, train_acc:0.983377, test_loss:0.067472, test_acc:0.919401\n",
      "->epoch:471, train_loss:0.000894, train_acc:0.995341, test_loss:0.060485, test_acc:0.922787\n",
      "->epoch:472, train_loss:0.002743, train_acc:0.981742, test_loss:0.252697, test_acc:0.911979\n",
      "->epoch:473, train_loss:0.001591, train_acc:0.989381, test_loss:0.447209, test_acc:0.920443\n",
      "->epoch:474, train_loss:0.004889, train_acc:0.976751, test_loss:0.079225, test_acc:0.917969\n",
      "->epoch:475, train_loss:0.001801, train_acc:0.989497, test_loss:0.315557, test_acc:0.912370\n",
      "->epoch:476, train_loss:0.002952, train_acc:0.979991, test_loss:0.562271, test_acc:0.874870\n",
      "->epoch:477, train_loss:0.003240, train_acc:0.972859, test_loss:0.140979, test_acc:0.911198\n",
      "->epoch:478, train_loss:0.006085, train_acc:0.976664, test_loss:0.225766, test_acc:0.908724\n",
      "->epoch:479, train_loss:0.002672, train_acc:0.987862, test_loss:0.281753, test_acc:0.919662\n",
      "->epoch:480, train_loss:0.004086, train_acc:0.992506, test_loss:0.050320, test_acc:0.912370\n",
      "->epoch:481, train_loss:0.001337, train_acc:0.993316, test_loss:0.063319, test_acc:0.920182\n",
      "->epoch:482, train_loss:0.002078, train_acc:0.986907, test_loss:0.107178, test_acc:0.911849\n",
      "->epoch:483, train_loss:0.003685, train_acc:0.987008, test_loss:0.051933, test_acc:0.919662\n",
      "->epoch:484, train_loss:0.003837, train_acc:0.990249, test_loss:0.157485, test_acc:0.905078\n",
      "->epoch:485, train_loss:0.014144, train_acc:0.971398, test_loss:0.182506, test_acc:0.911719\n",
      "->epoch:486, train_loss:0.001924, train_acc:0.990451, test_loss:0.016758, test_acc:0.912240\n",
      "->epoch:487, train_loss:0.000728, train_acc:0.996557, test_loss:0.042851, test_acc:0.924219\n",
      "->epoch:488, train_loss:0.001257, train_acc:0.993475, test_loss:0.058027, test_acc:0.921875\n",
      "->epoch:489, train_loss:0.002967, train_acc:0.985894, test_loss:0.054123, test_acc:0.907292\n",
      "->epoch:490, train_loss:0.004299, train_acc:0.988325, test_loss:0.270520, test_acc:0.909505\n",
      "->epoch:491, train_loss:0.001949, train_acc:0.987963, test_loss:0.121593, test_acc:0.918620\n",
      "->epoch:492, train_loss:0.002374, train_acc:0.994850, test_loss:0.184738, test_acc:0.911719\n",
      "->epoch:493, train_loss:0.001233, train_acc:0.993084, test_loss:0.089264, test_acc:0.921094\n",
      "->epoch:494, train_loss:0.001018, train_acc:0.995631, test_loss:0.162729, test_acc:0.922787\n",
      "->epoch:495, train_loss:0.005018, train_acc:0.989757, test_loss:0.032560, test_acc:0.898828\n",
      "->epoch:496, train_loss:0.001525, train_acc:0.990741, test_loss:0.152733, test_acc:0.914974\n",
      "->epoch:497, train_loss:0.003410, train_acc:0.980541, test_loss:0.045122, test_acc:0.911068\n",
      "->epoch:498, train_loss:0.002028, train_acc:0.989670, test_loss:0.120138, test_acc:0.919010\n",
      "->epoch:499, train_loss:0.000896, train_acc:0.994285, test_loss:0.106098, test_acc:0.927083\n",
      "->epoch:500, train_loss:0.003898, train_acc:0.989019, test_loss:0.239646, test_acc:0.911328\n",
      "->epoch:501, train_loss:0.001594, train_acc:0.992057, test_loss:0.154786, test_acc:0.923568\n",
      "->epoch:502, train_loss:0.001803, train_acc:0.989699, test_loss:0.465696, test_acc:0.910156\n",
      "->epoch:503, train_loss:0.007667, train_acc:0.984795, test_loss:0.288582, test_acc:0.911198\n",
      "->epoch:504, train_loss:0.021745, train_acc:0.969213, test_loss:0.199157, test_acc:0.905990\n",
      "->epoch:505, train_loss:0.005238, train_acc:0.977488, test_loss:0.401341, test_acc:0.873958\n",
      "->epoch:506, train_loss:0.002534, train_acc:0.981959, test_loss:0.114463, test_acc:0.917318\n",
      "->epoch:507, train_loss:0.000998, train_acc:0.994444, test_loss:0.158186, test_acc:0.901563\n",
      "->epoch:508, train_loss:0.001673, train_acc:0.991262, test_loss:0.288420, test_acc:0.915885\n",
      "->epoch:509, train_loss:0.001012, train_acc:0.995038, test_loss:0.101595, test_acc:0.918880\n",
      "->epoch:510, train_loss:0.000968, train_acc:0.994271, test_loss:0.152514, test_acc:0.918880\n",
      "->epoch:511, train_loss:0.002614, train_acc:0.987240, test_loss:1.481111, test_acc:0.864844\n",
      "->epoch:512, train_loss:0.008694, train_acc:0.985489, test_loss:0.075678, test_acc:0.910156\n",
      "->epoch:513, train_loss:0.000989, train_acc:0.993981, test_loss:0.043942, test_acc:0.921875\n",
      "->epoch:514, train_loss:0.001218, train_acc:0.993866, test_loss:0.104197, test_acc:0.919401\n",
      "->epoch:515, train_loss:0.006346, train_acc:0.975145, test_loss:0.232010, test_acc:0.917057\n",
      "->epoch:516, train_loss:0.006426, train_acc:0.982986, test_loss:0.161599, test_acc:0.904688\n",
      "->epoch:517, train_loss:0.001036, train_acc:0.993909, test_loss:0.171104, test_acc:0.923047\n",
      "->epoch:518, train_loss:0.002114, train_acc:0.991334, test_loss:0.407006, test_acc:0.911849\n",
      "->epoch:519, train_loss:0.001743, train_acc:0.990090, test_loss:0.553453, test_acc:0.902214\n",
      "->epoch:520, train_loss:0.007334, train_acc:0.973394, test_loss:0.306321, test_acc:0.890625\n",
      "->epoch:521, train_loss:0.012044, train_acc:0.989482, test_loss:0.182984, test_acc:0.888151\n",
      "->epoch:522, train_loss:0.004583, train_acc:0.966623, test_loss:0.094272, test_acc:0.919531\n",
      "->epoch:523, train_loss:0.001021, train_acc:0.994415, test_loss:0.085579, test_acc:0.922526\n",
      "->epoch:524, train_loss:0.002228, train_acc:0.989034, test_loss:0.155853, test_acc:0.920443\n",
      "->epoch:525, train_loss:0.001567, train_acc:0.993533, test_loss:0.082733, test_acc:0.924349\n",
      "->epoch:526, train_loss:0.003688, train_acc:0.984375, test_loss:0.071736, test_acc:0.917448\n",
      "->epoch:527, train_loss:0.000991, train_acc:0.993996, test_loss:0.085842, test_acc:0.925130\n",
      "->epoch:528, train_loss:0.002677, train_acc:0.990205, test_loss:0.076715, test_acc:0.911068\n",
      "->epoch:529, train_loss:0.001223, train_acc:0.993142, test_loss:0.094448, test_acc:0.916797\n",
      "->epoch:530, train_loss:0.000999, train_acc:0.995414, test_loss:0.238940, test_acc:0.913021\n",
      "->epoch:531, train_loss:0.002172, train_acc:0.987558, test_loss:0.159879, test_acc:0.919792\n",
      "->epoch:532, train_loss:0.002990, train_acc:0.989598, test_loss:0.198375, test_acc:0.918359\n",
      "->epoch:533, train_loss:0.000681, train_acc:0.996311, test_loss:0.128465, test_acc:0.923958\n",
      "->epoch:534, train_loss:0.007052, train_acc:0.987905, test_loss:0.059453, test_acc:0.901823\n",
      "->epoch:535, train_loss:0.001228, train_acc:0.992491, test_loss:0.064935, test_acc:0.920182\n",
      "->epoch:536, train_loss:0.001527, train_acc:0.992520, test_loss:0.195809, test_acc:0.910807\n",
      "->epoch:537, train_loss:0.000880, train_acc:0.994415, test_loss:0.397554, test_acc:0.916537\n",
      "->epoch:538, train_loss:0.008936, train_acc:0.982263, test_loss:0.029933, test_acc:0.909505\n",
      "->epoch:539, train_loss:0.002305, train_acc:0.992636, test_loss:0.392935, test_acc:0.910026\n",
      "->epoch:540, train_loss:0.002577, train_acc:0.985735, test_loss:0.066713, test_acc:0.919401\n",
      "->epoch:541, train_loss:0.001345, train_acc:0.993388, test_loss:0.088438, test_acc:0.870833\n",
      "->epoch:542, train_loss:0.002668, train_acc:0.989771, test_loss:0.161228, test_acc:0.916406\n",
      "->epoch:543, train_loss:0.001950, train_acc:0.987587, test_loss:0.152277, test_acc:0.921094\n",
      "->epoch:544, train_loss:0.001789, train_acc:0.989135, test_loss:0.258324, test_acc:0.918620\n",
      "->epoch:545, train_loss:0.006021, train_acc:0.981120, test_loss:0.116314, test_acc:0.899349\n",
      "->epoch:546, train_loss:0.001365, train_acc:0.991247, test_loss:0.112239, test_acc:0.917578\n",
      "->epoch:547, train_loss:0.001117, train_acc:0.993924, test_loss:0.188059, test_acc:0.917839\n",
      "->epoch:548, train_loss:0.001180, train_acc:0.993721, test_loss:0.180982, test_acc:0.919141\n",
      "->epoch:549, train_loss:0.001708, train_acc:0.993258, test_loss:0.160286, test_acc:0.917839\n",
      "->epoch:550, train_loss:0.005709, train_acc:0.984997, test_loss:0.172732, test_acc:0.905859\n",
      "->epoch:551, train_loss:0.001519, train_acc:0.990726, test_loss:0.110858, test_acc:0.921224\n",
      "->epoch:552, train_loss:0.000897, train_acc:0.995153, test_loss:0.162857, test_acc:0.916667\n",
      "->epoch:553, train_loss:0.000734, train_acc:0.995833, test_loss:0.206149, test_acc:0.917969\n",
      "->epoch:554, train_loss:0.011016, train_acc:0.982089, test_loss:0.311674, test_acc:0.874219\n",
      "->epoch:555, train_loss:0.003495, train_acc:0.977127, test_loss:0.202006, test_acc:0.912891\n",
      "->epoch:556, train_loss:0.001704, train_acc:0.990119, test_loss:0.165375, test_acc:0.918620\n",
      "->epoch:557, train_loss:0.001024, train_acc:0.995428, test_loss:0.157455, test_acc:0.920313\n",
      "->epoch:558, train_loss:0.001604, train_acc:0.990712, test_loss:0.566565, test_acc:0.879036\n",
      "->epoch:559, train_loss:0.003436, train_acc:0.977893, test_loss:0.167027, test_acc:0.877474\n",
      "->epoch:560, train_loss:0.009784, train_acc:0.976432, test_loss:0.067319, test_acc:0.917708\n",
      "->epoch:561, train_loss:0.005173, train_acc:0.977199, test_loss:0.070352, test_acc:0.912240\n",
      "->epoch:562, train_loss:0.001462, train_acc:0.992072, test_loss:0.043297, test_acc:0.916667\n",
      "->epoch:563, train_loss:0.001019, train_acc:0.994083, test_loss:0.043944, test_acc:0.921745\n",
      "->epoch:564, train_loss:0.001227, train_acc:0.994517, test_loss:0.085281, test_acc:0.927865\n",
      "->epoch:565, train_loss:0.001627, train_acc:0.992621, test_loss:0.145439, test_acc:0.918750\n",
      "->epoch:566, train_loss:0.003885, train_acc:0.985937, test_loss:0.146926, test_acc:0.915365\n",
      "->epoch:567, train_loss:0.000904, train_acc:0.993996, test_loss:0.216512, test_acc:0.924479\n",
      "->epoch:568, train_loss:0.003690, train_acc:0.987413, test_loss:0.052137, test_acc:0.922787\n",
      "->epoch:569, train_loss:0.001861, train_acc:0.993070, test_loss:0.384317, test_acc:0.908594\n",
      "->epoch:570, train_loss:0.002624, train_acc:0.987153, test_loss:0.643053, test_acc:0.912630\n",
      "->epoch:571, train_loss:0.007801, train_acc:0.981221, test_loss:0.074829, test_acc:0.919922\n",
      "->epoch:572, train_loss:0.001541, train_acc:0.992911, test_loss:0.231274, test_acc:0.901693\n",
      "->epoch:573, train_loss:0.012136, train_acc:0.964077, test_loss:0.146996, test_acc:0.904688\n",
      "->epoch:574, train_loss:0.002640, train_acc:0.984809, test_loss:0.090353, test_acc:0.916146\n",
      "->epoch:575, train_loss:0.001065, train_acc:0.994213, test_loss:0.169589, test_acc:0.919792\n",
      "->epoch:576, train_loss:0.002891, train_acc:0.984954, test_loss:0.047119, test_acc:0.911719\n",
      "->epoch:577, train_loss:0.002472, train_acc:0.987326, test_loss:0.098752, test_acc:0.919792\n",
      "->epoch:578, train_loss:0.000828, train_acc:0.995182, test_loss:0.095968, test_acc:0.925911\n",
      "->epoch:579, train_loss:0.004421, train_acc:0.977127, test_loss:0.115563, test_acc:0.915365\n",
      "->epoch:580, train_loss:0.000950, train_acc:0.994748, test_loss:0.396034, test_acc:0.920313\n",
      "->epoch:581, train_loss:0.001766, train_acc:0.992289, test_loss:0.145371, test_acc:0.923307\n",
      "->epoch:582, train_loss:0.002380, train_acc:0.990003, test_loss:0.056558, test_acc:0.919792\n",
      "->epoch:583, train_loss:0.001090, train_acc:0.994039, test_loss:0.090172, test_acc:0.920182\n",
      "->epoch:584, train_loss:0.002775, train_acc:0.986748, test_loss:0.127662, test_acc:0.917578\n",
      "->epoch:585, train_loss:0.002379, train_acc:0.984288, test_loss:0.144255, test_acc:0.917839\n",
      "->epoch:586, train_loss:0.005083, train_acc:0.973872, test_loss:0.069110, test_acc:0.886719\n",
      "->epoch:587, train_loss:0.001376, train_acc:0.992144, test_loss:0.040490, test_acc:0.922656\n",
      "->epoch:588, train_loss:0.001145, train_acc:0.994719, test_loss:0.191218, test_acc:0.920182\n",
      "->epoch:589, train_loss:0.001999, train_acc:0.987051, test_loss:0.074548, test_acc:0.919010\n",
      "->epoch:590, train_loss:0.003828, train_acc:0.986632, test_loss:0.090599, test_acc:0.919792\n",
      "->epoch:591, train_loss:0.001192, train_acc:0.994546, test_loss:0.242189, test_acc:0.900391\n",
      "->epoch:592, train_loss:0.001914, train_acc:0.991898, test_loss:0.062020, test_acc:0.926693\n",
      "->epoch:593, train_loss:0.031945, train_acc:0.970703, test_loss:0.141005, test_acc:0.917318\n",
      "->epoch:594, train_loss:0.002876, train_acc:0.985431, test_loss:0.215781, test_acc:0.912109\n",
      "->epoch:595, train_loss:0.002650, train_acc:0.983304, test_loss:0.131543, test_acc:0.917708\n",
      "->epoch:596, train_loss:0.007489, train_acc:0.988339, test_loss:0.116809, test_acc:0.913021\n",
      "->epoch:597, train_loss:0.000937, train_acc:0.994806, test_loss:0.096618, test_acc:0.920833\n",
      "->epoch:598, train_loss:0.000530, train_acc:0.997280, test_loss:0.120396, test_acc:0.915495\n",
      "->epoch:599, train_loss:0.001393, train_acc:0.993417, test_loss:0.095360, test_acc:0.922135\n",
      "->epoch:600, train_loss:0.004223, train_acc:0.981424, test_loss:0.068843, test_acc:0.907031\n",
      "->epoch:601, train_loss:0.002038, train_acc:0.988874, test_loss:0.064283, test_acc:0.917578\n",
      "->epoch:602, train_loss:0.000691, train_acc:0.996470, test_loss:0.144686, test_acc:0.917448\n",
      "->epoch:603, train_loss:0.039399, train_acc:0.956120, test_loss:0.047119, test_acc:0.892969\n",
      "->epoch:604, train_loss:0.002333, train_acc:0.984664, test_loss:0.021904, test_acc:0.915495\n",
      "->epoch:605, train_loss:0.001018, train_acc:0.994227, test_loss:0.022799, test_acc:0.914453\n",
      "->epoch:606, train_loss:0.001487, train_acc:0.991985, test_loss:0.283444, test_acc:0.904167\n",
      "->epoch:607, train_loss:0.001880, train_acc:0.989699, test_loss:0.059985, test_acc:0.921224\n",
      "->epoch:608, train_loss:0.002867, train_acc:0.990553, test_loss:0.090776, test_acc:0.909505\n",
      "->epoch:609, train_loss:0.001448, train_acc:0.992433, test_loss:0.180119, test_acc:0.923438\n",
      "->epoch:610, train_loss:0.001896, train_acc:0.988296, test_loss:0.119322, test_acc:0.915755\n",
      "->epoch:611, train_loss:0.008478, train_acc:0.973582, test_loss:0.040580, test_acc:0.904037\n",
      "->epoch:612, train_loss:0.002519, train_acc:0.985648, test_loss:0.297723, test_acc:0.914453\n",
      "->epoch:613, train_loss:0.002567, train_acc:0.982494, test_loss:0.093900, test_acc:0.900651\n",
      "->epoch:614, train_loss:0.003771, train_acc:0.985258, test_loss:0.035721, test_acc:0.904688\n",
      "->epoch:615, train_loss:0.001356, train_acc:0.992723, test_loss:0.071480, test_acc:0.918880\n",
      "->epoch:616, train_loss:0.000722, train_acc:0.995819, test_loss:0.063805, test_acc:0.918880\n",
      "->epoch:617, train_loss:0.004721, train_acc:0.979731, test_loss:0.104524, test_acc:0.921615\n",
      "->epoch:618, train_loss:0.002562, train_acc:0.987196, test_loss:0.153621, test_acc:0.864323\n",
      "->epoch:619, train_loss:0.002517, train_acc:0.986285, test_loss:0.086295, test_acc:0.914453\n",
      "->epoch:620, train_loss:0.007602, train_acc:0.983724, test_loss:0.075563, test_acc:0.918620\n",
      "->epoch:621, train_loss:0.001272, train_acc:0.992390, test_loss:0.116185, test_acc:0.922656\n",
      "->epoch:622, train_loss:0.000912, train_acc:0.995052, test_loss:0.226090, test_acc:0.914323\n",
      "->epoch:623, train_loss:0.001509, train_acc:0.993171, test_loss:0.595600, test_acc:0.823958\n",
      "->epoch:624, train_loss:0.004960, train_acc:0.981409, test_loss:0.169573, test_acc:0.917969\n",
      "->epoch:625, train_loss:0.000808, train_acc:0.994907, test_loss:0.153305, test_acc:0.924089\n",
      "->epoch:626, train_loss:0.004575, train_acc:0.981843, test_loss:0.100562, test_acc:0.870833\n",
      "->epoch:627, train_loss:0.001907, train_acc:0.987514, test_loss:0.080729, test_acc:0.921745\n",
      "->epoch:628, train_loss:0.003751, train_acc:0.990234, test_loss:0.252859, test_acc:0.911198\n",
      "->epoch:629, train_loss:0.020705, train_acc:0.974306, test_loss:0.013552, test_acc:0.918099\n",
      "->epoch:630, train_loss:0.001217, train_acc:0.992549, test_loss:0.035540, test_acc:0.921875\n",
      "->epoch:631, train_loss:0.003814, train_acc:0.988600, test_loss:0.425206, test_acc:0.901172\n",
      "->epoch:632, train_loss:0.064182, train_acc:0.935923, test_loss:0.013107, test_acc:0.901302\n",
      "->epoch:633, train_loss:0.002069, train_acc:0.988122, test_loss:0.021501, test_acc:0.916146\n",
      "->epoch:634, train_loss:0.001137, train_acc:0.993750, test_loss:0.021010, test_acc:0.919922\n",
      "->epoch:635, train_loss:0.001782, train_acc:0.993822, test_loss:0.048943, test_acc:0.920964\n",
      "->epoch:636, train_loss:0.000801, train_acc:0.995602, test_loss:0.030637, test_acc:0.923698\n",
      "->epoch:637, train_loss:0.001561, train_acc:0.991710, test_loss:0.062255, test_acc:0.920182\n",
      "->epoch:638, train_loss:0.001966, train_acc:0.992014, test_loss:0.084599, test_acc:0.919141\n",
      "->epoch:639, train_loss:0.001190, train_acc:0.993909, test_loss:0.111442, test_acc:0.913021\n",
      "->epoch:640, train_loss:0.005149, train_acc:0.992057, test_loss:0.285255, test_acc:0.905078\n",
      "->epoch:641, train_loss:0.002735, train_acc:0.986777, test_loss:0.063223, test_acc:0.918359\n",
      "->epoch:642, train_loss:0.001123, train_acc:0.994083, test_loss:0.258758, test_acc:0.898177\n",
      "->epoch:643, train_loss:0.001714, train_acc:0.992376, test_loss:0.081874, test_acc:0.919922\n",
      "->epoch:644, train_loss:0.005264, train_acc:0.984491, test_loss:0.058766, test_acc:0.918620\n",
      "->epoch:645, train_loss:0.001165, train_acc:0.993779, test_loss:0.093953, test_acc:0.924089\n",
      "->epoch:646, train_loss:0.001038, train_acc:0.994300, test_loss:0.295198, test_acc:0.902865\n",
      "->epoch:647, train_loss:0.009277, train_acc:0.978704, test_loss:0.190674, test_acc:0.900391\n",
      "->epoch:648, train_loss:0.009409, train_acc:0.988064, test_loss:0.028314, test_acc:0.917578\n",
      "->epoch:649, train_loss:0.002250, train_acc:0.987659, test_loss:0.029040, test_acc:0.921484\n",
      "->epoch:650, train_loss:0.000958, train_acc:0.994980, test_loss:0.067668, test_acc:0.921875\n",
      "->epoch:651, train_loss:0.000968, train_acc:0.994748, test_loss:0.038095, test_acc:0.922917\n",
      "->epoch:652, train_loss:0.001982, train_acc:0.993186, test_loss:0.087500, test_acc:0.900260\n",
      "->epoch:653, train_loss:0.001043, train_acc:0.993432, test_loss:0.065097, test_acc:0.922396\n",
      "->epoch:654, train_loss:0.004562, train_acc:0.984462, test_loss:0.090311, test_acc:0.918099\n",
      "->epoch:655, train_loss:0.000932, train_acc:0.994661, test_loss:0.076326, test_acc:0.922656\n",
      "->epoch:656, train_loss:0.004990, train_acc:0.984650, test_loss:0.033239, test_acc:0.918099\n",
      "->epoch:657, train_loss:0.001095, train_acc:0.993866, test_loss:0.202261, test_acc:0.921224\n",
      "->epoch:658, train_loss:0.001850, train_acc:0.989135, test_loss:0.040433, test_acc:0.920833\n",
      "->epoch:659, train_loss:0.011902, train_acc:0.990567, test_loss:0.170985, test_acc:0.898698\n",
      "->epoch:660, train_loss:0.001051, train_acc:0.993302, test_loss:0.054267, test_acc:0.920703\n",
      "->epoch:661, train_loss:0.009145, train_acc:0.983319, test_loss:0.188322, test_acc:0.884245\n",
      "->epoch:662, train_loss:0.003784, train_acc:0.977459, test_loss:0.056531, test_acc:0.913281\n",
      "->epoch:663, train_loss:0.001026, train_acc:0.994068, test_loss:0.127408, test_acc:0.903646\n",
      "->epoch:664, train_loss:0.002571, train_acc:0.989352, test_loss:0.185089, test_acc:0.913542\n",
      "->epoch:665, train_loss:0.003238, train_acc:0.991030, test_loss:0.901905, test_acc:0.895964\n",
      "->epoch:666, train_loss:0.005494, train_acc:0.986646, test_loss:0.061449, test_acc:0.916667\n",
      "->epoch:667, train_loss:0.002557, train_acc:0.989077, test_loss:0.050896, test_acc:0.916016\n",
      "->epoch:668, train_loss:0.000598, train_acc:0.996615, test_loss:0.062749, test_acc:0.920313\n",
      "->epoch:669, train_loss:0.003411, train_acc:0.988267, test_loss:0.343649, test_acc:0.877865\n",
      "->epoch:670, train_loss:0.014586, train_acc:0.959317, test_loss:0.222783, test_acc:0.910547\n",
      "->epoch:671, train_loss:0.002977, train_acc:0.979528, test_loss:0.124111, test_acc:0.914063\n",
      "->epoch:672, train_loss:0.004061, train_acc:0.985055, test_loss:0.430400, test_acc:0.855859\n",
      "->epoch:673, train_loss:0.003238, train_acc:0.973785, test_loss:0.191741, test_acc:0.912630\n",
      "->epoch:674, train_loss:0.006723, train_acc:0.975231, test_loss:0.042163, test_acc:0.902474\n",
      "->epoch:675, train_loss:0.001208, train_acc:0.993099, test_loss:0.166468, test_acc:0.912109\n",
      "->epoch:676, train_loss:0.006711, train_acc:0.970023, test_loss:0.059694, test_acc:0.912109\n",
      "->epoch:677, train_loss:0.002833, train_acc:0.984881, test_loss:0.057299, test_acc:0.916276\n",
      "->epoch:678, train_loss:0.000978, train_acc:0.994980, test_loss:0.060809, test_acc:0.921745\n",
      "->epoch:679, train_loss:0.003631, train_acc:0.978718, test_loss:0.100972, test_acc:0.920182\n",
      "->epoch:680, train_loss:0.025165, train_acc:0.990567, test_loss:0.115772, test_acc:0.861589\n",
      "->epoch:681, train_loss:0.004107, train_acc:0.975969, test_loss:0.030837, test_acc:0.918229\n",
      "->epoch:682, train_loss:0.001158, train_acc:0.993041, test_loss:0.032450, test_acc:0.920833\n",
      "->epoch:683, train_loss:0.003428, train_acc:0.978255, test_loss:0.216370, test_acc:0.916667\n",
      "->epoch:684, train_loss:0.011969, train_acc:0.941710, test_loss:0.114821, test_acc:0.876302\n",
      "->epoch:685, train_loss:0.004598, train_acc:0.967578, test_loss:0.138880, test_acc:0.905208\n",
      "->epoch:686, train_loss:0.002253, train_acc:0.986994, test_loss:0.144357, test_acc:0.901693\n",
      "->epoch:687, train_loss:0.004463, train_acc:0.984057, test_loss:0.161448, test_acc:0.925781\n",
      "->epoch:688, train_loss:0.001197, train_acc:0.993475, test_loss:0.454181, test_acc:0.912109\n",
      "->epoch:689, train_loss:0.002787, train_acc:0.984332, test_loss:0.146801, test_acc:0.916276\n",
      "->epoch:690, train_loss:0.003341, train_acc:0.989873, test_loss:0.041053, test_acc:0.915104\n",
      "->epoch:691, train_loss:0.004021, train_acc:0.975651, test_loss:0.181918, test_acc:0.914063\n",
      "->epoch:692, train_loss:0.009322, train_acc:0.967289, test_loss:0.246621, test_acc:0.871094\n",
      "->epoch:693, train_loss:0.008403, train_acc:0.965799, test_loss:0.196743, test_acc:0.902214\n",
      "->epoch:694, train_loss:0.005254, train_acc:0.964077, test_loss:0.041821, test_acc:0.919401\n",
      "->epoch:695, train_loss:0.002063, train_acc:0.988354, test_loss:0.092798, test_acc:0.913151\n",
      "->epoch:696, train_loss:0.002194, train_acc:0.989685, test_loss:0.175789, test_acc:0.891406\n",
      "->epoch:697, train_loss:0.005077, train_acc:0.980830, test_loss:0.131433, test_acc:0.919922\n",
      "->epoch:698, train_loss:0.001289, train_acc:0.992332, test_loss:0.199561, test_acc:0.919010\n",
      "->epoch:699, train_loss:0.003444, train_acc:0.984317, test_loss:0.153006, test_acc:0.887109\n",
      "->epoch:700, train_loss:0.007304, train_acc:0.979196, test_loss:0.082728, test_acc:0.920573\n",
      "->epoch:701, train_loss:0.001371, train_acc:0.992708, test_loss:0.145539, test_acc:0.922656\n",
      "->epoch:702, train_loss:0.003587, train_acc:0.981264, test_loss:0.133760, test_acc:0.869922\n",
      "->epoch:703, train_loss:0.002934, train_acc:0.988643, test_loss:0.120847, test_acc:0.920182\n",
      "->epoch:704, train_loss:0.002933, train_acc:0.990611, test_loss:0.470548, test_acc:0.912370\n",
      "->epoch:705, train_loss:0.004575, train_acc:0.973900, test_loss:0.231966, test_acc:0.920443\n",
      "->epoch:706, train_loss:0.002425, train_acc:0.984852, test_loss:0.152707, test_acc:0.910417\n",
      "->epoch:707, train_loss:0.004569, train_acc:0.972251, test_loss:0.304395, test_acc:0.887760\n",
      "->epoch:708, train_loss:0.002003, train_acc:0.987124, test_loss:0.162608, test_acc:0.925260\n",
      "->epoch:709, train_loss:0.002206, train_acc:0.989699, test_loss:0.155077, test_acc:0.923568\n",
      "->epoch:710, train_loss:0.002830, train_acc:0.983637, test_loss:0.265180, test_acc:0.898698\n",
      "->epoch:711, train_loss:0.002067, train_acc:0.985605, test_loss:0.310177, test_acc:0.910938\n",
      "->epoch:712, train_loss:0.003503, train_acc:0.977271, test_loss:0.208617, test_acc:0.917448\n",
      "->epoch:713, train_loss:0.007865, train_acc:0.980686, test_loss:0.299231, test_acc:0.895833\n",
      "->epoch:714, train_loss:0.008524, train_acc:0.985952, test_loss:0.138041, test_acc:0.889583\n",
      "->epoch:715, train_loss:0.003152, train_acc:0.981308, test_loss:0.090780, test_acc:0.921354\n",
      "->epoch:716, train_loss:0.001714, train_acc:0.991609, test_loss:0.243770, test_acc:0.914974\n",
      "->epoch:717, train_loss:0.009699, train_acc:0.975506, test_loss:0.046389, test_acc:0.919792\n",
      "->epoch:718, train_loss:0.001469, train_acc:0.991913, test_loss:0.093351, test_acc:0.903385\n",
      "->epoch:719, train_loss:0.002318, train_acc:0.983594, test_loss:0.094772, test_acc:0.915755\n",
      "->epoch:720, train_loss:0.001680, train_acc:0.989844, test_loss:0.237497, test_acc:0.892448\n",
      "->epoch:721, train_loss:0.004259, train_acc:0.971267, test_loss:0.261256, test_acc:0.916276\n",
      "->epoch:722, train_loss:0.002076, train_acc:0.985431, test_loss:0.161427, test_acc:0.923828\n",
      "->epoch:723, train_loss:0.007965, train_acc:0.943273, test_loss:0.412232, test_acc:0.888542\n",
      "->epoch:724, train_loss:0.003039, train_acc:0.976259, test_loss:0.215670, test_acc:0.916016\n",
      "->epoch:725, train_loss:0.004324, train_acc:0.978487, test_loss:0.315503, test_acc:0.909245\n",
      "->epoch:726, train_loss:0.007590, train_acc:0.956597, test_loss:0.296108, test_acc:0.913672\n",
      "->epoch:727, train_loss:0.003659, train_acc:0.983304, test_loss:0.065206, test_acc:0.923307\n",
      "->epoch:728, train_loss:0.001992, train_acc:0.987442, test_loss:0.192536, test_acc:0.912370\n",
      "->epoch:729, train_loss:0.003314, train_acc:0.982002, test_loss:0.102314, test_acc:0.922917\n",
      "->epoch:730, train_loss:0.001912, train_acc:0.990046, test_loss:0.269906, test_acc:0.923568\n",
      "->epoch:731, train_loss:0.008734, train_acc:0.973843, test_loss:0.142378, test_acc:0.914063\n",
      "->epoch:732, train_loss:0.002330, train_acc:0.984823, test_loss:0.061263, test_acc:0.924349\n",
      "->epoch:733, train_loss:0.000925, train_acc:0.994560, test_loss:0.231933, test_acc:0.909505\n",
      "->epoch:734, train_loss:0.003531, train_acc:0.986531, test_loss:0.177618, test_acc:0.920443\n",
      "->epoch:735, train_loss:0.001998, train_acc:0.991522, test_loss:0.152954, test_acc:0.919010\n",
      "->epoch:736, train_loss:0.021545, train_acc:0.968186, test_loss:0.111187, test_acc:0.904427\n",
      "->epoch:737, train_loss:0.002077, train_acc:0.988484, test_loss:0.195426, test_acc:0.916146\n",
      "->epoch:738, train_loss:0.001504, train_acc:0.992057, test_loss:0.148278, test_acc:0.916927\n",
      "->epoch:739, train_loss:0.004365, train_acc:0.985692, test_loss:0.401910, test_acc:0.911458\n",
      "->epoch:740, train_loss:0.002924, train_acc:0.982812, test_loss:0.105131, test_acc:0.923438\n",
      "->epoch:741, train_loss:0.001344, train_acc:0.992940, test_loss:0.136761, test_acc:0.923307\n",
      "->epoch:742, train_loss:0.004425, train_acc:0.975072, test_loss:0.167144, test_acc:0.904037\n",
      "->epoch:743, train_loss:0.002262, train_acc:0.987312, test_loss:0.184495, test_acc:0.912891\n",
      "->epoch:744, train_loss:0.001441, train_acc:0.992028, test_loss:0.136710, test_acc:0.918490\n",
      "->epoch:745, train_loss:0.001830, train_acc:0.990466, test_loss:0.252932, test_acc:0.889583\n",
      "->epoch:746, train_loss:0.009627, train_acc:0.944401, test_loss:0.319508, test_acc:0.887109\n",
      "->epoch:747, train_loss:0.011718, train_acc:0.956727, test_loss:0.139437, test_acc:0.904948\n",
      "->epoch:748, train_loss:0.002164, train_acc:0.987109, test_loss:0.067744, test_acc:0.918880\n",
      "->epoch:749, train_loss:0.002362, train_acc:0.983840, test_loss:0.214761, test_acc:0.914974\n",
      "->epoch:750, train_loss:0.002272, train_acc:0.988440, test_loss:0.399474, test_acc:0.910286\n",
      "->epoch:751, train_loss:0.004555, train_acc:0.976534, test_loss:0.107023, test_acc:0.919401\n",
      "->epoch:752, train_loss:0.006509, train_acc:0.952373, test_loss:0.178253, test_acc:0.870964\n",
      "->epoch:753, train_loss:0.004466, train_acc:0.974754, test_loss:0.208892, test_acc:0.917969\n",
      "->epoch:754, train_loss:0.001451, train_acc:0.990480, test_loss:0.251946, test_acc:0.920703\n",
      "->epoch:755, train_loss:0.005893, train_acc:0.962442, test_loss:0.340317, test_acc:0.904297\n",
      "->epoch:756, train_loss:0.003753, train_acc:0.977488, test_loss:0.070708, test_acc:0.863802\n",
      "->epoch:757, train_loss:0.011904, train_acc:0.982465, test_loss:0.074149, test_acc:0.922396\n",
      "->epoch:758, train_loss:0.001120, train_acc:0.993446, test_loss:0.128616, test_acc:0.922396\n",
      "->epoch:759, train_loss:0.001760, train_acc:0.989931, test_loss:0.460753, test_acc:0.901563\n",
      "->epoch:760, train_loss:0.005023, train_acc:0.977127, test_loss:0.130138, test_acc:0.922526\n",
      "->epoch:761, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12680\\1410925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'->epoch:{epoch + 1}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memo_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12680\\974053572.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(emo_dim)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memo_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f'->epoch:{epoch + 1}', end = ', ')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim)\n",
    "#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0debada-3b0f-4255-abe1-809ce1f7c30c",
   "metadata": {},
   "source": [
    "- 两层GATConv加每层0.3的Dropout， ->epoch:132, train_loss:0.002720, train_acc:0.967549, test_loss:0.016331, test_acc:0.906901\n",
    "- 两层GATConv加每层0.2的Dropout， ->epoch:142, train_loss:0.001992, train_acc:0.976143, test_loss:0.011121, test_acc:0.916927\n",
    "- GCN仅包括自环，->epoch:62, train_loss:0.001162, train_acc:0.990914, test_loss:0.015195, test_acc:0.936198"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964d82-e4ca-40db-ad83-0722b8a6b880",
   "metadata": {},
   "source": [
    "## 比较实验\n",
    "### GCN\n",
    "+ 仅包括自环时，->epoch:25, train_loss:0.000922, train_acc:0.990784, test_loss:0.010875, test_acc:0.919401\n",
    "+ 加上3x3卷积核的邻接边时，->epoch:32, train_loss:0.000819, train_acc:0.992173, test_loss:0.020655, test_acc:0.895313，邻接边设计的不好，限制了模型的发挥\n",
    "+ 别人的方法的准确率：89/90、93/94\n",
    "### GAT\n",
    "+ 仅包括自环时，->epoch:30, train_loss:0.001252, train_acc:0.986531, test_loss:0.015308, test_acc:0.912630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f4ff-9fbb-4323-ad0d-4dfc7848e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
