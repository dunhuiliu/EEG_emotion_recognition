{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6df28-9919-4bff-8e9a-ea82c35eba11",
   "metadata": {},
   "source": [
    "### 脑电图注意力网络（GAT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def data_split(train_ratio=0.7):\n",
    "#     load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "#     trials = np.load(load_dir + 'trials.npy')\n",
    "#     bases = np.load(load_dir + 'bases.npy')\n",
    "#     labels = np.load(load_dir + 'labels.npy')\n",
    "#     # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "#     # 去基线\n",
    "#     for i, base in enumerate(bases):\n",
    "#         trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "#     # 离散化标签\n",
    "#     labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "#     # 复制标签以对齐样本\n",
    "#     labels = np.repeat(labels, 60, axis = 0)\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     shuffle_list = np.arange(trials.shape[0])\n",
    "#     np.random.shuffle(shuffle_list)\n",
    "#     trials = trials[shuffle_list]\n",
    "#     labels = labels[shuffle_list]\n",
    "    \n",
    "#     cut_point = int(trials.shape[0] * train_ratio)\n",
    "#     train_features, train_labels = trials[:cut_point], labels[:cut_point]\n",
    "#     test_features, test_labels = trials[cut_point:], labels[cut_point:]\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32 * 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32 * 128))\n",
    "    \n",
    "#     mean = train_features.mean(axis = 0)\n",
    "#     std = train_features.std(axis = 0)\n",
    "    \n",
    "#     train_features = (train_features - mean) / std\n",
    "#     test_features = (test_features - mean) / std\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32, 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32, 128))\n",
    "    \n",
    "#     save_dir = 'data/data_split/'\n",
    "#     np.save(save_dir + 'train_features.npy', train_features)\n",
    "#     np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "#     np.save(save_dir + 'test_features.npy', test_features)\n",
    "#     np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "# data_split(train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b5e808-1250-4784-b6b7-a0e58d86017a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if create_complete_graph:\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                edge_index[0].append(i)\n",
    "                edge_index[1].append(j)\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        return edge_index\n",
    "    \n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd42b5bf-ca24-4f71-95ad-b0b494aaa18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcb8368-b508-41fb-baa4-5e35d16ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index = [[],[]]\n",
    "# weight = []\n",
    "\n",
    "# #用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "# chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "#               9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "#               17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "#                 25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "# maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "# for k, v in chan_to_1020.items():\n",
    "#     maps[v[0]][v[1]] = k + 1\n",
    "# print(maps)\n",
    "# plt.matshow(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    edge_index = None\n",
    "    def __init__(self, root, is_train_data, edge_index):\n",
    "        self.is_train_data = is_train_data\n",
    "        self.edge_index = edge_index\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['train.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor(labels[i].reshape(1, -1), dtype=torch.long)\n",
    "            data = Data(x = x, edge_index=self.edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.rnns = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.rnns.append(nn.GRU(1, 2, 2, device=device))\n",
    "            \n",
    "        \n",
    "        self.temporalMLPs1 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.temporalMLPs1.append(nn.Linear(256, 256, device=device))\n",
    "        \n",
    "        self.temporalMLPs2 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.temporalMLPs2.append(nn.Linear(256, 256, device=device))\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(8192, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.view(batch_size, 32, 128)\n",
    "        \n",
    "        temporalMLPs_out = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            x1 = x[:, i, :].transpose(0, 1).unsqueeze(-1)\n",
    "            x1, h_n = self.rnns[i](x1)\n",
    "            x1 = x1.transpose(0, 1)\n",
    "            x1 = x1.reshape(batch_size, 256)\n",
    "            x1 = self.temporalMLPs1[i](x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, 0.2)\n",
    "            x1 = self.temporalMLPs2[i](x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, 0.2)\n",
    "            temporalMLPs_out.append(x1)\n",
    "        \n",
    "        # concat\n",
    "        x = torch.concat(temporalMLPs_out, dim = 1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emo_dim):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        train_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_train_sample = len(trainDataLoader.dataset)\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    train_acc = train_acc / num_train_sample\n",
    "    \n",
    "    # check测试集的性能\n",
    "    vali_loss = 0\n",
    "    vali_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in testDataLoader:\n",
    "        batch.to(device)\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        vali_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        vali_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_test_sample = len(testDataLoader.dataset)\n",
    "    vali_loss = vali_loss / num_test_sample\n",
    "    vali_acc = vali_acc / num_test_sample\n",
    "    \n",
    "    print(f'train_loss:{train_loss:.6f}, train_acc:{train_acc:.6f}, test_loss:{vali_loss:.6f}, test_acc:{vali_acc:.6f}')\n",
    "    \n",
    "    return train_loss, train_acc, vali_loss, vali_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9bf77-c20d-484b-932b-8b98917f374b",
   "metadata": {},
   "source": [
    "# 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1e1611-fac8-4210-b800-bf02914ed6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete_graph = False\n",
    "self_loop_only = False\n",
    "emo_dim = 0\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "edge_index = get_edge_index(create_complete_graph=create_complete_graph, self_loop_only=self_loop_only)\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, edge_index=edge_index)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, edge_index=edge_index)\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3f3d55-fc83-475e-a697-9b8a12789830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "crit = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:1, train_loss:0.021369, train_acc:0.568547, test_loss:0.021158, test_acc:0.580208\n",
      "->epoch:2, train_loss:0.021132, train_acc:0.581800, test_loss:0.021015, test_acc:0.587109\n",
      "->epoch:3, train_loss:0.020984, train_acc:0.589829, test_loss:0.020838, test_acc:0.593229\n",
      "->epoch:4, train_loss:0.020846, train_acc:0.593490, test_loss:0.020698, test_acc:0.597266\n",
      "->epoch:5, train_loss:0.020849, train_acc:0.593518, test_loss:0.020726, test_acc:0.599219\n",
      "->epoch:6, train_loss:0.020800, train_acc:0.595269, test_loss:0.020685, test_acc:0.599479\n",
      "->epoch:7, train_loss:0.020788, train_acc:0.594039, test_loss:0.020680, test_acc:0.601302\n",
      "->epoch:8, train_loss:0.020764, train_acc:0.595486, test_loss:0.020782, test_acc:0.592708\n",
      "->epoch:9, train_loss:0.020709, train_acc:0.598105, test_loss:0.021088, test_acc:0.559245\n",
      "->epoch:10, train_loss:0.020674, train_acc:0.600506, test_loss:0.020740, test_acc:0.592448\n",
      "->epoch:11, train_loss:0.020725, train_acc:0.596311, test_loss:0.020708, test_acc:0.597135\n",
      "->epoch:12, train_loss:0.020673, train_acc:0.598698, test_loss:0.020599, test_acc:0.604948\n",
      "->epoch:13, train_loss:0.020686, train_acc:0.598510, test_loss:0.020508, test_acc:0.608333\n",
      "->epoch:14, train_loss:0.020673, train_acc:0.597483, test_loss:0.020659, test_acc:0.594792\n",
      "->epoch:15, train_loss:0.020615, train_acc:0.600246, test_loss:0.020727, test_acc:0.592839\n",
      "->epoch:16, train_loss:0.020639, train_acc:0.600058, test_loss:0.020629, test_acc:0.598828\n",
      "->epoch:17, train_loss:0.020619, train_acc:0.600101, test_loss:0.020834, test_acc:0.592188\n",
      "->epoch:18, train_loss:0.020616, train_acc:0.600535, test_loss:0.020490, test_acc:0.605078\n",
      "->epoch:19, train_loss:0.020625, train_acc:0.599696, test_loss:0.020487, test_acc:0.600911\n",
      "->epoch:20, train_loss:0.020577, train_acc:0.601273, test_loss:0.020906, test_acc:0.596875\n",
      "->epoch:21, train_loss:0.020573, train_acc:0.600217, test_loss:0.020543, test_acc:0.599089\n",
      "->epoch:22, train_loss:0.020575, train_acc:0.601953, test_loss:0.020489, test_acc:0.602083\n",
      "->epoch:23, train_loss:0.020579, train_acc:0.602561, test_loss:0.020485, test_acc:0.609505\n",
      "->epoch:24, train_loss:0.020557, train_acc:0.603979, test_loss:0.020544, test_acc:0.598828\n",
      "->epoch:25, train_loss:0.020588, train_acc:0.600680, test_loss:0.020649, test_acc:0.606250\n",
      "->epoch:26, train_loss:0.020556, train_acc:0.603125, test_loss:0.020860, test_acc:0.594401\n",
      "->epoch:27, train_loss:0.020538, train_acc:0.602083, test_loss:0.020569, test_acc:0.605990\n",
      "->epoch:28, train_loss:0.020535, train_acc:0.602807, test_loss:0.020683, test_acc:0.599740\n",
      "->epoch:29, train_loss:0.020515, train_acc:0.603675, test_loss:0.020516, test_acc:0.604557\n",
      "->epoch:30, train_loss:0.020528, train_acc:0.602127, test_loss:0.020432, test_acc:0.603646\n",
      "->epoch:31, train_loss:0.020530, train_acc:0.601736, test_loss:0.020521, test_acc:0.602734\n",
      "->epoch:32, train_loss:0.020500, train_acc:0.604861, test_loss:0.020710, test_acc:0.594922\n",
      "->epoch:33, train_loss:0.020510, train_acc:0.603892, test_loss:0.020687, test_acc:0.598698\n",
      "->epoch:34, train_loss:0.020514, train_acc:0.603602, test_loss:0.020576, test_acc:0.598307\n",
      "->epoch:35, train_loss:0.020510, train_acc:0.602662, test_loss:0.020601, test_acc:0.601302\n",
      "->epoch:36, train_loss:0.020519, train_acc:0.602691, test_loss:0.020515, test_acc:0.605859\n",
      "->epoch:37, train_loss:0.020495, train_acc:0.606178, test_loss:0.020472, test_acc:0.604037\n",
      "->epoch:38, train_loss:0.020520, train_acc:0.604311, test_loss:0.020769, test_acc:0.601042\n",
      "->epoch:39, train_loss:0.020479, train_acc:0.604543, test_loss:0.020552, test_acc:0.597786\n",
      "->epoch:40, train_loss:0.020492, train_acc:0.603675, test_loss:0.020433, test_acc:0.609375\n",
      "->epoch:41, train_loss:0.020504, train_acc:0.605382, test_loss:0.020462, test_acc:0.601563\n",
      "->epoch:42, train_loss:0.020483, train_acc:0.602763, test_loss:0.020410, test_acc:0.610547\n",
      "->epoch:43, train_loss:0.020479, train_acc:0.604586, test_loss:0.021000, test_acc:0.590755\n",
      "->epoch:44, train_loss:0.020477, train_acc:0.605223, test_loss:0.020502, test_acc:0.616667\n",
      "->epoch:45, train_loss:0.020467, train_acc:0.603660, test_loss:0.020593, test_acc:0.601042\n",
      "->epoch:46, train_loss:0.020478, train_acc:0.606062, test_loss:0.020590, test_acc:0.600781\n",
      "->epoch:47, train_loss:0.020461, train_acc:0.603487, test_loss:0.020542, test_acc:0.602734\n",
      "->epoch:48, train_loss:0.020462, train_acc:0.605599, test_loss:0.020526, test_acc:0.603255\n",
      "->epoch:49, train_loss:0.020481, train_acc:0.603212, test_loss:0.020580, test_acc:0.603385\n",
      "->epoch:50, train_loss:0.020448, train_acc:0.605729, test_loss:0.020539, test_acc:0.606771\n",
      "->epoch:51, train_loss:0.020455, train_acc:0.603472, test_loss:0.020356, test_acc:0.614063\n",
      "->epoch:52, train_loss:0.020479, train_acc:0.605628, test_loss:0.020468, test_acc:0.605859\n",
      "->epoch:53, train_loss:0.020472, train_acc:0.604398, test_loss:0.020485, test_acc:0.604297\n",
      "->epoch:54, train_loss:0.020451, train_acc:0.606105, test_loss:0.020473, test_acc:0.600000\n",
      "->epoch:55, train_loss:0.020474, train_acc:0.605179, test_loss:0.020419, test_acc:0.606250\n",
      "->epoch:56, train_loss:0.020444, train_acc:0.606915, test_loss:0.021084, test_acc:0.592057\n",
      "->epoch:57, train_loss:0.020467, train_acc:0.606756, test_loss:0.020484, test_acc:0.607813\n",
      "->epoch:58, train_loss:0.020447, train_acc:0.605527, test_loss:0.020435, test_acc:0.608333\n",
      "->epoch:59, train_loss:0.020461, train_acc:0.605093, test_loss:0.020451, test_acc:0.602995\n",
      "->epoch:60, train_loss:0.020454, train_acc:0.604832, test_loss:0.020492, test_acc:0.601693\n",
      "->epoch:61, train_loss:0.020438, train_acc:0.606438, test_loss:0.020449, test_acc:0.606771\n",
      "->epoch:62, train_loss:0.020456, train_acc:0.603863, test_loss:0.020552, test_acc:0.608333\n",
      "->epoch:63, train_loss:0.020454, train_acc:0.603979, test_loss:0.020408, test_acc:0.606250\n",
      "->epoch:64, train_loss:0.020407, train_acc:0.605729, test_loss:0.020638, test_acc:0.598568\n",
      "->epoch:65, train_loss:0.020477, train_acc:0.605744, test_loss:0.020511, test_acc:0.600651\n",
      "->epoch:66, train_loss:0.020454, train_acc:0.604876, test_loss:0.020485, test_acc:0.607161\n",
      "->epoch:67, train_loss:0.020439, train_acc:0.605469, test_loss:0.020604, test_acc:0.601432\n",
      "->epoch:68, train_loss:0.020433, train_acc:0.606308, test_loss:0.020586, test_acc:0.603516\n",
      "->epoch:69, train_loss:0.020461, train_acc:0.604008, test_loss:0.020459, test_acc:0.608854\n",
      "->epoch:70, train_loss:0.020447, train_acc:0.606033, test_loss:0.020551, test_acc:0.604557\n",
      "->epoch:71, train_loss:0.020471, train_acc:0.605628, test_loss:0.020469, test_acc:0.600521\n",
      "->epoch:72, train_loss:0.020473, train_acc:0.604630, test_loss:0.020771, test_acc:0.590625\n",
      "->epoch:73, train_loss:0.020457, train_acc:0.604861, test_loss:0.020455, test_acc:0.599089\n",
      "->epoch:74, train_loss:0.020509, train_acc:0.603675, test_loss:0.020486, test_acc:0.603646\n",
      "->epoch:75, train_loss:0.020433, train_acc:0.606076, test_loss:0.020341, test_acc:0.614583\n",
      "->epoch:76, train_loss:0.020400, train_acc:0.607769, test_loss:0.020480, test_acc:0.601432\n",
      "->epoch:77, train_loss:0.020411, train_acc:0.605194, test_loss:0.020623, test_acc:0.596224\n",
      "->epoch:78, train_loss:0.020460, train_acc:0.604861, test_loss:0.020403, test_acc:0.605208\n",
      "->epoch:79, train_loss:0.020408, train_acc:0.605888, test_loss:0.020497, test_acc:0.605729\n",
      "->epoch:80, train_loss:0.020392, train_acc:0.606771, test_loss:0.020410, test_acc:0.606901\n",
      "->epoch:81, train_loss:0.020430, train_acc:0.604630, test_loss:0.020448, test_acc:0.604948\n",
      "->epoch:82, train_loss:0.020437, train_acc:0.608174, test_loss:0.020541, test_acc:0.601693\n",
      "->epoch:83, train_loss:0.020414, train_acc:0.608406, test_loss:0.020504, test_acc:0.605599\n",
      "->epoch:84, train_loss:0.020433, train_acc:0.606380, test_loss:0.020409, test_acc:0.607943\n",
      "->epoch:85, train_loss:0.020406, train_acc:0.604065, test_loss:0.020410, test_acc:0.607031\n",
      "->epoch:86, train_loss:0.020413, train_acc:0.607046, test_loss:0.020515, test_acc:0.601042\n",
      "->epoch:87, train_loss:0.020407, train_acc:0.606380, test_loss:0.020427, test_acc:0.605339\n",
      "->epoch:88, train_loss:0.020393, train_acc:0.608926, test_loss:0.020461, test_acc:0.604167\n",
      "->epoch:89, train_loss:0.020405, train_acc:0.605975, test_loss:0.020462, test_acc:0.606380\n",
      "->epoch:90, train_loss:0.020400, train_acc:0.606453, test_loss:0.020402, test_acc:0.604037\n",
      "->epoch:91, train_loss:0.020438, train_acc:0.607118, test_loss:0.020419, test_acc:0.601042\n",
      "->epoch:92, train_loss:0.020416, train_acc:0.606959, test_loss:0.020614, test_acc:0.598177\n",
      "->epoch:93, train_loss:0.020418, train_acc:0.606699, test_loss:0.020531, test_acc:0.606641\n",
      "->epoch:94, train_loss:0.020425, train_acc:0.606134, test_loss:0.020496, test_acc:0.599740\n",
      "->epoch:95, train_loss:0.020434, train_acc:0.604181, test_loss:0.020830, test_acc:0.578125\n",
      "->epoch:96, train_loss:0.020405, train_acc:0.607335, test_loss:0.020356, test_acc:0.606250\n",
      "->epoch:97, train_loss:0.020411, train_acc:0.606539, test_loss:0.020422, test_acc:0.609505\n",
      "->epoch:98, train_loss:0.020387, train_acc:0.608362, test_loss:0.020439, test_acc:0.607161\n",
      "->epoch:99, train_loss:0.020407, train_acc:0.606105, test_loss:0.020398, test_acc:0.610417\n",
      "->epoch:100, train_loss:0.020388, train_acc:0.606163, test_loss:0.020408, test_acc:0.605859\n",
      "->epoch:101, train_loss:0.020383, train_acc:0.606395, test_loss:0.020425, test_acc:0.606380\n",
      "->epoch:102, train_loss:0.020378, train_acc:0.606684, test_loss:0.020587, test_acc:0.598958\n",
      "->epoch:103, train_loss:0.020417, train_acc:0.607509, test_loss:0.020775, test_acc:0.596094\n",
      "->epoch:104, train_loss:0.020386, train_acc:0.608854, test_loss:0.020437, test_acc:0.604688\n",
      "->epoch:105, train_loss:0.020425, train_acc:0.606163, test_loss:0.020585, test_acc:0.595573\n",
      "->epoch:106, train_loss:0.020375, train_acc:0.608218, test_loss:0.020539, test_acc:0.600781\n",
      "->epoch:107, train_loss:0.020406, train_acc:0.607306, test_loss:0.020429, test_acc:0.611589\n",
      "->epoch:108, train_loss:0.020390, train_acc:0.607205, test_loss:0.020405, test_acc:0.605469\n",
      "->epoch:109, train_loss:0.020337, train_acc:0.608044, test_loss:0.020464, test_acc:0.607422\n",
      "->epoch:110, train_loss:0.020401, train_acc:0.605773, test_loss:0.020485, test_acc:0.602214\n",
      "->epoch:111, train_loss:0.020406, train_acc:0.607972, test_loss:0.020449, test_acc:0.606641\n",
      "->epoch:112, train_loss:0.020381, train_acc:0.607798, test_loss:0.020414, test_acc:0.608984\n",
      "->epoch:113, train_loss:0.020412, train_acc:0.606959, test_loss:0.020479, test_acc:0.604818\n",
      "->epoch:114, train_loss:0.020402, train_acc:0.607046, test_loss:0.020360, test_acc:0.607161\n",
      "->epoch:115, train_loss:0.020426, train_acc:0.608275, test_loss:0.020477, test_acc:0.602865\n",
      "->epoch:116, train_loss:0.020422, train_acc:0.604745, test_loss:0.020522, test_acc:0.605990\n",
      "->epoch:117, train_loss:0.020400, train_acc:0.606583, test_loss:0.020334, test_acc:0.610807\n",
      "->epoch:118, train_loss:0.020391, train_acc:0.607538, test_loss:0.020592, test_acc:0.605469\n",
      "->epoch:119, train_loss:0.020419, train_acc:0.604876, test_loss:0.020531, test_acc:0.610807\n",
      "->epoch:120, train_loss:0.020430, train_acc:0.607726, test_loss:0.020494, test_acc:0.603776\n",
      "->epoch:121, train_loss:0.020379, train_acc:0.607812, test_loss:0.020407, test_acc:0.605208\n",
      "->epoch:122, train_loss:0.020414, train_acc:0.606930, test_loss:0.020448, test_acc:0.605208\n",
      "->epoch:123, train_loss:0.020378, train_acc:0.607104, test_loss:0.020767, test_acc:0.597396\n",
      "->epoch:124, train_loss:0.020393, train_acc:0.607480, test_loss:0.020588, test_acc:0.602214\n",
      "->epoch:125, train_loss:0.020404, train_acc:0.607494, test_loss:0.020329, test_acc:0.608073\n",
      "->epoch:126, train_loss:0.020386, train_acc:0.608391, test_loss:0.020469, test_acc:0.601042\n",
      "->epoch:127, train_loss:0.020384, train_acc:0.607465, test_loss:0.020414, test_acc:0.604688\n",
      "->epoch:128, train_loss:0.020387, train_acc:0.606380, test_loss:0.020433, test_acc:0.603906\n",
      "->epoch:129, train_loss:0.020383, train_acc:0.607682, test_loss:0.020483, test_acc:0.606641\n",
      "->epoch:130, train_loss:0.020428, train_acc:0.604644, test_loss:0.020530, test_acc:0.602344\n",
      "->epoch:131, train_loss:0.020343, train_acc:0.610171, test_loss:0.020558, test_acc:0.604818\n",
      "->epoch:132, train_loss:0.020375, train_acc:0.608073, test_loss:0.020360, test_acc:0.605599\n",
      "->epoch:133, train_loss:0.020399, train_acc:0.606380, test_loss:0.020493, test_acc:0.606120\n",
      "->epoch:134, train_loss:0.020409, train_acc:0.605440, test_loss:0.020306, test_acc:0.606771\n",
      "->epoch:135, train_loss:0.020382, train_acc:0.607321, test_loss:0.020431, test_acc:0.604688\n",
      "->epoch:136, train_loss:0.020409, train_acc:0.606959, test_loss:0.020440, test_acc:0.605729\n",
      "->epoch:137, train_loss:0.020397, train_acc:0.605816, test_loss:0.020373, test_acc:0.606901\n",
      "->epoch:138, train_loss:0.020399, train_acc:0.607176, test_loss:0.020231, test_acc:0.618750\n",
      "->epoch:139, train_loss:0.020371, train_acc:0.608189, test_loss:0.020457, test_acc:0.603255\n",
      "->epoch:140, train_loss:0.020408, train_acc:0.606120, test_loss:0.020351, test_acc:0.612109\n",
      "->epoch:141, train_loss:0.020361, train_acc:0.610200, test_loss:0.020376, test_acc:0.604297\n",
      "->epoch:142, train_loss:0.020389, train_acc:0.606279, test_loss:0.020452, test_acc:0.605599\n",
      "->epoch:143, train_loss:0.020394, train_acc:0.608116, test_loss:0.020527, test_acc:0.612370\n",
      "->epoch:144, train_loss:0.020345, train_acc:0.608145, test_loss:0.020434, test_acc:0.605469\n",
      "->epoch:145, train_loss:0.020328, train_acc:0.608391, test_loss:0.020314, test_acc:0.611849\n",
      "->epoch:146, train_loss:0.020399, train_acc:0.606192, test_loss:0.020499, test_acc:0.604948\n",
      "->epoch:147, train_loss:0.020362, train_acc:0.608724, test_loss:0.020598, test_acc:0.601042\n",
      "->epoch:148, train_loss:0.020398, train_acc:0.606207, test_loss:0.020566, test_acc:0.611198\n",
      "->epoch:149, train_loss:0.020410, train_acc:0.606539, test_loss:0.020415, test_acc:0.609375\n",
      "->epoch:150, train_loss:0.020385, train_acc:0.606887, test_loss:0.020306, test_acc:0.610938\n",
      "->epoch:151, train_loss:0.020379, train_acc:0.607581, test_loss:0.020390, test_acc:0.602865\n",
      "->epoch:152, train_loss:0.020339, train_acc:0.608333, test_loss:0.020409, test_acc:0.605990\n",
      "->epoch:153, train_loss:0.020387, train_acc:0.607046, test_loss:0.020396, test_acc:0.602734\n",
      "->epoch:154, train_loss:0.020373, train_acc:0.608507, test_loss:0.020212, test_acc:0.613672\n",
      "->epoch:155, train_loss:0.020366, train_acc:0.609592, test_loss:0.020469, test_acc:0.600781\n",
      "->epoch:156, train_loss:0.020373, train_acc:0.608116, test_loss:0.020407, test_acc:0.607161\n",
      "->epoch:157, train_loss:0.020380, train_acc:0.606322, test_loss:0.020345, test_acc:0.609245\n",
      "->epoch:158, train_loss:0.020340, train_acc:0.610547, test_loss:0.020383, test_acc:0.612760\n",
      "->epoch:159, train_loss:0.020370, train_acc:0.607465, test_loss:0.020535, test_acc:0.602995\n",
      "->epoch:160, train_loss:0.020407, train_acc:0.606568, test_loss:0.020395, test_acc:0.610807\n",
      "->epoch:161, train_loss:0.020372, train_acc:0.609332, test_loss:0.020481, test_acc:0.605729\n",
      "->epoch:162, train_loss:0.020384, train_acc:0.607451, test_loss:0.020370, test_acc:0.607031\n",
      "->epoch:163, train_loss:0.020383, train_acc:0.608695, test_loss:0.020778, test_acc:0.594531\n",
      "->epoch:164, train_loss:0.020313, train_acc:0.609983, test_loss:0.020375, test_acc:0.610286\n",
      "->epoch:165, train_loss:0.020358, train_acc:0.609230, test_loss:0.020438, test_acc:0.606250\n",
      "->epoch:166, train_loss:0.020384, train_acc:0.608044, test_loss:0.020420, test_acc:0.608984\n",
      "->epoch:167, train_loss:0.020378, train_acc:0.607480, test_loss:0.020496, test_acc:0.607292\n",
      "->epoch:168, train_loss:0.020320, train_acc:0.610344, test_loss:0.020287, test_acc:0.610938\n",
      "->epoch:169, train_loss:0.020378, train_acc:0.607986, test_loss:0.020413, test_acc:0.605469\n",
      "->epoch:170, train_loss:0.020387, train_acc:0.608999, test_loss:0.020504, test_acc:0.604948\n",
      "->epoch:171, train_loss:0.020405, train_acc:0.606163, test_loss:0.020390, test_acc:0.611328\n",
      "->epoch:172, train_loss:0.020371, train_acc:0.608189, test_loss:0.020405, test_acc:0.610677\n",
      "->epoch:173, train_loss:0.020388, train_acc:0.606062, test_loss:0.020423, test_acc:0.608854\n",
      "->epoch:174, train_loss:0.020406, train_acc:0.607407, test_loss:0.020430, test_acc:0.608724\n",
      "->epoch:175, train_loss:0.020397, train_acc:0.605946, test_loss:0.020543, test_acc:0.601432\n",
      "->epoch:176, train_loss:0.020371, train_acc:0.609100, test_loss:0.020443, test_acc:0.603516\n",
      "->epoch:177, train_loss:0.020357, train_acc:0.606583, test_loss:0.020361, test_acc:0.606120\n",
      "->epoch:178, train_loss:0.020362, train_acc:0.609216, test_loss:0.020355, test_acc:0.611589\n",
      "->epoch:179, train_loss:0.020367, train_acc:0.610619, test_loss:0.020462, test_acc:0.606120\n",
      "->epoch:180, train_loss:0.020340, train_acc:0.609737, test_loss:0.020464, test_acc:0.606120\n",
      "->epoch:181, train_loss:0.020361, train_acc:0.609664, test_loss:0.020334, test_acc:0.609896\n",
      "->epoch:182, train_loss:0.020362, train_acc:0.609954, test_loss:0.020273, test_acc:0.608984\n",
      "->epoch:183, train_loss:0.020342, train_acc:0.606887, test_loss:0.020491, test_acc:0.601042\n",
      "->epoch:184, train_loss:0.020357, train_acc:0.609201, test_loss:0.020332, test_acc:0.614844\n",
      "->epoch:185, train_loss:0.020383, train_acc:0.607321, test_loss:0.020403, test_acc:0.603906\n",
      "->epoch:186, train_loss:0.020383, train_acc:0.608999, test_loss:0.020469, test_acc:0.608073\n",
      "->epoch:187, train_loss:0.020373, train_acc:0.607046, test_loss:0.020325, test_acc:0.612109\n",
      "->epoch:188, train_loss:0.020333, train_acc:0.608811, test_loss:0.020405, test_acc:0.604818\n",
      "->epoch:189, train_loss:0.020401, train_acc:0.607002, test_loss:0.020325, test_acc:0.615234\n",
      "->epoch:190, train_loss:0.020347, train_acc:0.609664, test_loss:0.020352, test_acc:0.608594\n",
      "->epoch:191, train_loss:0.020369, train_acc:0.608333, test_loss:0.020333, test_acc:0.608854\n",
      "->epoch:192, train_loss:0.020375, train_acc:0.609143, test_loss:0.020389, test_acc:0.611068\n",
      "->epoch:193, train_loss:0.020364, train_acc:0.607552, test_loss:0.020306, test_acc:0.611328\n",
      "->epoch:194, train_loss:0.020342, train_acc:0.611748, test_loss:0.020637, test_acc:0.594922\n",
      "->epoch:195, train_loss:0.020372, train_acc:0.607509, test_loss:0.020653, test_acc:0.600130\n",
      "->epoch:196, train_loss:0.020392, train_acc:0.606713, test_loss:0.020415, test_acc:0.608724\n",
      "->epoch:197, train_loss:0.020362, train_acc:0.608102, test_loss:0.020398, test_acc:0.608203\n",
      "->epoch:198, train_loss:0.020367, train_acc:0.604528, test_loss:0.020296, test_acc:0.609375\n",
      "->epoch:199, train_loss:0.020342, train_acc:0.609737, test_loss:0.020347, test_acc:0.617708\n",
      "->epoch:200, train_loss:0.020350, train_acc:0.606814, test_loss:0.020598, test_acc:0.601172\n",
      "->epoch:201, train_loss:0.020360, train_acc:0.607104, test_loss:0.020430, test_acc:0.605469\n",
      "->epoch:202, train_loss:0.020383, train_acc:0.608594, test_loss:0.020315, test_acc:0.610417\n",
      "->epoch:203, train_loss:0.020337, train_acc:0.607957, test_loss:0.020522, test_acc:0.610286\n",
      "->epoch:204, train_loss:0.020366, train_acc:0.607407, test_loss:0.020328, test_acc:0.612630\n",
      "->epoch:205, train_loss:0.020347, train_acc:0.606684, test_loss:0.020550, test_acc:0.599479\n",
      "->epoch:206, train_loss:0.020368, train_acc:0.608348, test_loss:0.020509, test_acc:0.598828\n",
      "->epoch:207, train_loss:0.020368, train_acc:0.609303, test_loss:0.020406, test_acc:0.601042\n",
      "->epoch:208, train_loss:0.020371, train_acc:0.607161, test_loss:0.020421, test_acc:0.604818\n",
      "->epoch:209, train_loss:0.020335, train_acc:0.609910, test_loss:0.020438, test_acc:0.602995\n",
      "->epoch:210, train_loss:0.020334, train_acc:0.609795, test_loss:0.020418, test_acc:0.607292\n",
      "->epoch:211, train_loss:0.020318, train_acc:0.610692, test_loss:0.020293, test_acc:0.610677\n",
      "->epoch:212, train_loss:0.020314, train_acc:0.609968, test_loss:0.020313, test_acc:0.612760\n",
      "->epoch:213, train_loss:0.020372, train_acc:0.606655, test_loss:0.020347, test_acc:0.614583\n",
      "->epoch:214, train_loss:0.020363, train_acc:0.607711, test_loss:0.020350, test_acc:0.606510\n",
      "->epoch:215, train_loss:0.020341, train_acc:0.608883, test_loss:0.020428, test_acc:0.600130\n",
      "->epoch:216, train_loss:0.020351, train_acc:0.608145, test_loss:0.020378, test_acc:0.611458\n",
      "->epoch:217, train_loss:0.020342, train_acc:0.610460, test_loss:0.020346, test_acc:0.614974\n",
      "->epoch:218, train_loss:0.020332, train_acc:0.609115, test_loss:0.020351, test_acc:0.611589\n",
      "->epoch:219, train_loss:0.020354, train_acc:0.607567, test_loss:0.020419, test_acc:0.603646\n",
      "->epoch:220, train_loss:0.020317, train_acc:0.610663, test_loss:0.020302, test_acc:0.610417\n",
      "->epoch:221, train_loss:0.020365, train_acc:0.608970, test_loss:0.020404, test_acc:0.609505\n",
      "->epoch:222, train_loss:0.020368, train_acc:0.609722, test_loss:0.020593, test_acc:0.594922\n",
      "->epoch:223, train_loss:0.020361, train_acc:0.608362, test_loss:0.020455, test_acc:0.613411\n",
      "->epoch:224, train_loss:0.020357, train_acc:0.608738, test_loss:0.020464, test_acc:0.607682\n",
      "->epoch:225, train_loss:0.020341, train_acc:0.611068, test_loss:0.020375, test_acc:0.611719\n",
      "->epoch:226, train_loss:0.020323, train_acc:0.609592, test_loss:0.020423, test_acc:0.604557\n",
      "->epoch:227, train_loss:0.020344, train_acc:0.609549, test_loss:0.020522, test_acc:0.597526\n",
      "->epoch:228, train_loss:0.020357, train_acc:0.608565, test_loss:0.020411, test_acc:0.609766\n",
      "->epoch:229, train_loss:0.020324, train_acc:0.609664, test_loss:0.020429, test_acc:0.606380\n",
      "->epoch:230, train_loss:0.020374, train_acc:0.607161, test_loss:0.020340, test_acc:0.613932\n",
      "->epoch:231, train_loss:0.020347, train_acc:0.608825, test_loss:0.020372, test_acc:0.605859\n",
      "->epoch:232, train_loss:0.020376, train_acc:0.609823, test_loss:0.020282, test_acc:0.606250\n",
      "->epoch:233, train_loss:0.020345, train_acc:0.609086, test_loss:0.020475, test_acc:0.607552\n",
      "->epoch:234, train_loss:0.020314, train_acc:0.609766, test_loss:0.020367, test_acc:0.606510\n",
      "->epoch:235, train_loss:0.020324, train_acc:0.610749, test_loss:0.020422, test_acc:0.608724\n",
      "->epoch:236, train_loss:0.020338, train_acc:0.610098, test_loss:0.020628, test_acc:0.601563\n",
      "->epoch:237, train_loss:0.020338, train_acc:0.609158, test_loss:0.020447, test_acc:0.599349\n",
      "->epoch:238, train_loss:0.020381, train_acc:0.608174, test_loss:0.020713, test_acc:0.607943\n",
      "->epoch:239, train_loss:0.020326, train_acc:0.608825, test_loss:0.020267, test_acc:0.608984\n",
      "->epoch:240, train_loss:0.020317, train_acc:0.607986, test_loss:0.020321, test_acc:0.610677\n",
      "->epoch:241, train_loss:0.020350, train_acc:0.609143, test_loss:0.020499, test_acc:0.607161\n",
      "->epoch:242, train_loss:0.020363, train_acc:0.609100, test_loss:0.020426, test_acc:0.604948\n",
      "->epoch:243, train_loss:0.020335, train_acc:0.609143, test_loss:0.020677, test_acc:0.598568\n",
      "->epoch:244, train_loss:0.020348, train_acc:0.607697, test_loss:0.020396, test_acc:0.604167\n",
      "->epoch:245, train_loss:0.020351, train_acc:0.609418, test_loss:0.020340, test_acc:0.614453\n",
      "->epoch:246, train_loss:0.020332, train_acc:0.609635, test_loss:0.020582, test_acc:0.605208\n",
      "->epoch:247, train_loss:0.020315, train_acc:0.612095, test_loss:0.020385, test_acc:0.602344\n",
      "->epoch:248, train_loss:0.020333, train_acc:0.610503, test_loss:0.020320, test_acc:0.609896\n",
      "->epoch:249, train_loss:0.020384, train_acc:0.606973, test_loss:0.020435, test_acc:0.600781\n",
      "->epoch:250, train_loss:0.020335, train_acc:0.609187, test_loss:0.020445, test_acc:0.615755\n",
      "->epoch:251, train_loss:0.020351, train_acc:0.608492, test_loss:0.020479, test_acc:0.601693\n",
      "->epoch:252, train_loss:0.020346, train_acc:0.608579, test_loss:0.020366, test_acc:0.609635\n",
      "->epoch:253, train_loss:0.020337, train_acc:0.608811, test_loss:0.020301, test_acc:0.613672\n",
      "->epoch:254, train_loss:0.020336, train_acc:0.609259, test_loss:0.020308, test_acc:0.612630\n",
      "->epoch:255, train_loss:0.020310, train_acc:0.611270, test_loss:0.020513, test_acc:0.601172\n",
      "->epoch:256, train_loss:0.020360, train_acc:0.607827, test_loss:0.020417, test_acc:0.604037\n",
      "->epoch:257, train_loss:0.020332, train_acc:0.607263, test_loss:0.020406, test_acc:0.608854\n",
      "->epoch:258, train_loss:0.020331, train_acc:0.609491, test_loss:0.020365, test_acc:0.601172\n",
      "->epoch:259, train_loss:0.020351, train_acc:0.607726, test_loss:0.020454, test_acc:0.603385\n",
      "->epoch:260, train_loss:0.020333, train_acc:0.609375, test_loss:0.020298, test_acc:0.617057\n",
      "->epoch:261, train_loss:0.020341, train_acc:0.605671, test_loss:0.020208, test_acc:0.604948\n",
      "->epoch:262, train_loss:0.020293, train_acc:0.609983, test_loss:0.020413, test_acc:0.604557\n",
      "->epoch:263, train_loss:0.020339, train_acc:0.609375, test_loss:0.020487, test_acc:0.607552\n",
      "->epoch:264, train_loss:0.020374, train_acc:0.608608, test_loss:0.020466, test_acc:0.605859\n",
      "->epoch:265, train_loss:0.020356, train_acc:0.607378, test_loss:0.020482, test_acc:0.604557\n",
      "->epoch:266, train_loss:0.020342, train_acc:0.607147, test_loss:0.020385, test_acc:0.607422\n",
      "->epoch:267, train_loss:0.020331, train_acc:0.609201, test_loss:0.020437, test_acc:0.606901\n",
      "->epoch:268, train_loss:0.020342, train_acc:0.608521, test_loss:0.020543, test_acc:0.601172\n",
      "->epoch:269, train_loss:0.020374, train_acc:0.607885, test_loss:0.020356, test_acc:0.608333\n",
      "->epoch:270, train_loss:0.020336, train_acc:0.605367, test_loss:0.020310, test_acc:0.612891\n",
      "->epoch:271, train_loss:0.020321, train_acc:0.609346, test_loss:0.020253, test_acc:0.610026\n",
      "->epoch:272, train_loss:0.020354, train_acc:0.608565, test_loss:0.020367, test_acc:0.605339\n",
      "->epoch:273, train_loss:0.020297, train_acc:0.609563, test_loss:0.020451, test_acc:0.602214\n",
      "->epoch:274, train_loss:0.020337, train_acc:0.609896, test_loss:0.020308, test_acc:0.608854\n",
      "->epoch:275, train_loss:0.020352, train_acc:0.609606, test_loss:0.020264, test_acc:0.613932\n",
      "->epoch:276, train_loss:0.020355, train_acc:0.610402, test_loss:0.020402, test_acc:0.605208\n",
      "->epoch:277, train_loss:0.020357, train_acc:0.608073, test_loss:0.020398, test_acc:0.608594\n",
      "->epoch:278, train_loss:0.020290, train_acc:0.609635, test_loss:0.020337, test_acc:0.604167\n",
      "->epoch:279, train_loss:0.020314, train_acc:0.610417, test_loss:0.020311, test_acc:0.609115\n",
      "->epoch:280, train_loss:0.020323, train_acc:0.608955, test_loss:0.020449, test_acc:0.606771\n",
      "->epoch:281, train_loss:0.020316, train_acc:0.610142, test_loss:0.020272, test_acc:0.617448\n",
      "->epoch:282, train_loss:0.020334, train_acc:0.608869, test_loss:0.020343, test_acc:0.610286\n",
      "->epoch:283, train_loss:0.020341, train_acc:0.610417, test_loss:0.020614, test_acc:0.599740\n",
      "->epoch:284, train_loss:0.020312, train_acc:0.610156, test_loss:0.020277, test_acc:0.615625\n",
      "->epoch:285, train_loss:0.020350, train_acc:0.608999, test_loss:0.020343, test_acc:0.611589\n",
      "->epoch:286, train_loss:0.020350, train_acc:0.610836, test_loss:0.020430, test_acc:0.606771\n",
      "->epoch:287, train_loss:0.020311, train_acc:0.610663, test_loss:0.020416, test_acc:0.606380\n",
      "->epoch:288, train_loss:0.020316, train_acc:0.610200, test_loss:0.020299, test_acc:0.609245\n",
      "->epoch:289, train_loss:0.020346, train_acc:0.609635, test_loss:0.020402, test_acc:0.605339\n",
      "->epoch:290, train_loss:0.020351, train_acc:0.608666, test_loss:0.020367, test_acc:0.609766\n",
      "->epoch:291, train_loss:0.020341, train_acc:0.607943, test_loss:0.020450, test_acc:0.602083\n",
      "->epoch:292, train_loss:0.020359, train_acc:0.607668, test_loss:0.020198, test_acc:0.616667\n",
      "->epoch:293, train_loss:0.020373, train_acc:0.606438, test_loss:0.020321, test_acc:0.611328\n",
      "->epoch:294, train_loss:0.020360, train_acc:0.609129, test_loss:0.020287, test_acc:0.608464\n",
      "->epoch:295, train_loss:0.020330, train_acc:0.609042, test_loss:0.020395, test_acc:0.615625\n",
      "->epoch:296, train_loss:0.020348, train_acc:0.606684, test_loss:0.020492, test_acc:0.602083\n",
      "->epoch:297, train_loss:0.020315, train_acc:0.610373, test_loss:0.020382, test_acc:0.603385\n",
      "->epoch:298, train_loss:0.020350, train_acc:0.607523, test_loss:0.020521, test_acc:0.601302\n",
      "->epoch:299, train_loss:0.020323, train_acc:0.608854, test_loss:0.020343, test_acc:0.611849\n",
      "->epoch:300, train_loss:0.020276, train_acc:0.614106, test_loss:0.020336, test_acc:0.608854\n",
      "->epoch:301, train_loss:0.020307, train_acc:0.611097, test_loss:0.020408, test_acc:0.603125\n",
      "->epoch:302, train_loss:0.020339, train_acc:0.607494, test_loss:0.020426, test_acc:0.604818\n",
      "->epoch:303, train_loss:0.020333, train_acc:0.608796, test_loss:0.020365, test_acc:0.611589\n",
      "->epoch:304, train_loss:0.020313, train_acc:0.609505, test_loss:0.020249, test_acc:0.608464\n",
      "->epoch:305, train_loss:0.020344, train_acc:0.608782, test_loss:0.020849, test_acc:0.598438\n",
      "->epoch:306, train_loss:0.020345, train_acc:0.609722, test_loss:0.020355, test_acc:0.615495\n",
      "->epoch:307, train_loss:0.020366, train_acc:0.606684, test_loss:0.020338, test_acc:0.611589\n",
      "->epoch:308, train_loss:0.020321, train_acc:0.609852, test_loss:0.020243, test_acc:0.613151\n",
      "->epoch:309, train_loss:0.020343, train_acc:0.609737, test_loss:0.020463, test_acc:0.602734\n",
      "->epoch:310, train_loss:0.020330, train_acc:0.609476, test_loss:0.020300, test_acc:0.609115\n",
      "->epoch:311, train_loss:0.020320, train_acc:0.610532, test_loss:0.020299, test_acc:0.614193\n",
      "->epoch:312, train_loss:0.020311, train_acc:0.609983, test_loss:0.020415, test_acc:0.614063\n",
      "->epoch:313, train_loss:0.020315, train_acc:0.609997, test_loss:0.020403, test_acc:0.607552\n",
      "->epoch:314, train_loss:0.020353, train_acc:0.608898, test_loss:0.020360, test_acc:0.608594\n",
      "->epoch:315, train_loss:0.020344, train_acc:0.607711, test_loss:0.020287, test_acc:0.613021\n",
      "->epoch:316, train_loss:0.020345, train_acc:0.608608, test_loss:0.020403, test_acc:0.606250\n",
      "->epoch:317, train_loss:0.020306, train_acc:0.610098, test_loss:0.020388, test_acc:0.608984\n",
      "->epoch:318, train_loss:0.020324, train_acc:0.608290, test_loss:0.020326, test_acc:0.611589\n",
      "->epoch:319, train_loss:0.020349, train_acc:0.608261, test_loss:0.020407, test_acc:0.602734\n",
      "->epoch:320, train_loss:0.020357, train_acc:0.609042, test_loss:0.020342, test_acc:0.607813\n",
      "->epoch:321, train_loss:0.020327, train_acc:0.608275, test_loss:0.020335, test_acc:0.608724\n",
      "->epoch:322, train_loss:0.020335, train_acc:0.608608, test_loss:0.020251, test_acc:0.611849\n",
      "->epoch:323, train_loss:0.020325, train_acc:0.610286, test_loss:0.020334, test_acc:0.608073\n",
      "->epoch:324, train_loss:0.020321, train_acc:0.612355, test_loss:0.020302, test_acc:0.609245\n",
      "->epoch:325, train_loss:0.020341, train_acc:0.609795, test_loss:0.020384, test_acc:0.607422\n",
      "->epoch:326, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9296\\1410925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'->epoch:{epoch + 1}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memo_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9296\\974053572.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(emo_dim)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memo_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[0;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 488\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         )\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f'->epoch:{epoch + 1}', end = ', ')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim)\n",
    "#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaaa6f-c3fb-4b07-91b9-aa185cf2b0ba",
   "metadata": {},
   "source": [
    "- MLP_base ->epoch:195, train_loss:0.000070, train_acc:0.999494, test_loss:0.022968, test_acc:0.933854\n",
    "- MLP_2层dropout（p=0.2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa21ed2-da43-4dc3-ad70-081231e98ef4",
   "metadata": {},
   "source": [
    "# 增加模型容量\n",
    "- ->epoch:86, train_loss:0.002378, train_acc:0.980787, test_loss:0.017223, test_acc:0.908203\n",
    "- 改为heads=3， ->epoch:167, train_loss:0.001960, train_acc:0.985171, test_loss:0.022945, test_acc:0.910417"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964d82-e4ca-40db-ad83-0722b8a6b880",
   "metadata": {},
   "source": [
    "## 比较实验\n",
    "### GCN\n",
    "+ 仅包括自环时，->epoch:25, train_loss:0.000922, train_acc:0.990784, test_loss:0.010875, test_acc:0.919401\n",
    "+ 加上3x3卷积核的邻接边时，->epoch:32, train_loss:0.000819, train_acc:0.992173, test_loss:0.020655, test_acc:0.895313，邻接边设计的不好，限制了模型的发挥\n",
    "+ 别人的方法的准确率：89/90、93/94\n",
    "### GAT\n",
    "+ 仅包括自环时，->epoch:30, train_loss:0.001252, train_acc:0.986531, test_loss:0.015308, test_acc:0.912630\n",
    "+ 使用自己设计的边，->epoch:123, train_loss:0.002104, train_acc:0.982161, test_loss:0.029411, test_acc:0.904688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f4ff-9fbb-4323-ad0d-4dfc7848e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
