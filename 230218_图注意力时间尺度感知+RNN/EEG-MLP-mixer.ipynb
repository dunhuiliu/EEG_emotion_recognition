{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8a2f88-794f-41ec-8149-bc8a5e11166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d86c9a-a9b3-496d-a650-e4b4c054fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def data_split(train_ratio=0.7):\n",
    "#     load_dir = '../global_data/time_76800x32x128/'\n",
    "\n",
    "#     trials = np.load(load_dir + 'trials.npy')\n",
    "#     bases = np.load(load_dir + 'bases.npy')\n",
    "#     labels = np.load(load_dir + 'labels.npy')\n",
    "#     # print(trials.shape, bases.shape, labels.shape)\n",
    "    \n",
    "#     # 去基线\n",
    "#     for i, base in enumerate(bases):\n",
    "#         trials[i * 60 : (i + 1) * 60] -= base\n",
    "    \n",
    "#     # 离散化标签\n",
    "#     labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "#     # 复制标签以对齐样本\n",
    "#     labels = np.repeat(labels, 60, axis = 0)\n",
    "#     # print(labels.shape)\n",
    "    \n",
    "#     shuffle_list = np.arange(trials.shape[0])\n",
    "#     np.random.shuffle(shuffle_list)\n",
    "#     trials = trials[shuffle_list]\n",
    "#     labels = labels[shuffle_list]\n",
    "    \n",
    "#     cut_point = int(trials.shape[0] * train_ratio)\n",
    "#     train_features, train_labels = trials[:cut_point], labels[:cut_point]\n",
    "#     test_features, test_labels = trials[cut_point:], labels[cut_point:]\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32 * 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32 * 128))\n",
    "    \n",
    "#     mean = train_features.mean(axis = 0)\n",
    "#     std = train_features.std(axis = 0)\n",
    "    \n",
    "#     train_features = (train_features - mean) / std\n",
    "#     test_features = (test_features - mean) / std\n",
    "    \n",
    "#     train_features = train_features.reshape((cut_point, 32, 128))\n",
    "#     test_features = test_features.reshape((trials.shape[0] - cut_point, 32, 128))\n",
    "    \n",
    "#     save_dir = 'data/data_split/'\n",
    "#     np.save(save_dir + 'train_features.npy', train_features)\n",
    "#     np.save(save_dir + 'train_labels.npy', train_labels)\n",
    "#     np.save(save_dir + 'test_features.npy', test_features)\n",
    "#     np.save(save_dir + 'test_labels.npy', test_labels)\n",
    "\n",
    "# data_split(train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb3182e-f053-4458-b057-bfd300617f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(is_train_data=True):\n",
    "    save_dir = 'data/data_split/'\n",
    "    if is_train_data:\n",
    "        features = np.load(save_dir + 'train_features.npy')\n",
    "        labels = np.load(save_dir + 'train_labels.npy')\n",
    "    else:\n",
    "        features = np.load(save_dir + 'test_features.npy')\n",
    "        labels = np.load(save_dir + 'test_labels.npy')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd42b5bf-ca24-4f71-95ad-b0b494aaa18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_index(create_complete_graph=False, self_loop_only=False):\n",
    "    edge_index = [[],[]]\n",
    "    weight = []\n",
    "    \n",
    "    if self_loop_only:\n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "        return edge_index\n",
    "    \n",
    "    adjacency_edge = {\n",
    "        1:[2],\n",
    "        2:[3, 19],\n",
    "        3:[5, 6],\n",
    "        4:[5],\n",
    "        5:[8, 7],\n",
    "        6:[7, 24],\n",
    "        7:[9, 10],\n",
    "        8:[9],\n",
    "        9:[12, 11],\n",
    "        10:[11, 16],\n",
    "        11:[13],\n",
    "        12:[],\n",
    "        13:[14, 15],\n",
    "        14:[15],\n",
    "        15:[],\n",
    "        16:[13, 31],\n",
    "        17:[18],\n",
    "        18:[19, 20],\n",
    "        19:[6, 23],\n",
    "        20:[23, 22],\n",
    "        21:[22],\n",
    "        22:[25, 26],\n",
    "        23:[24, 25],\n",
    "        24:[10, 28],\n",
    "        25:[28, 27],\n",
    "        26:[27],\n",
    "        27:[29, 30],\n",
    "        28:[16, 29],\n",
    "        29:[31],\n",
    "        30:[],\n",
    "        31:[15, 32],\n",
    "        32:[15]\n",
    "    }\n",
    "    \n",
    "    for start, end_list in adjacency_edge.items():\n",
    "        if len(end_list) == 0:\n",
    "            continue\n",
    "        for end in end_list:\n",
    "            edge_index[0].append(start - 1)\n",
    "            edge_index[1].append(end - 1)\n",
    "            edge_index[0].append(end - 1)\n",
    "            edge_index[1].append(start - 1)\n",
    "           \n",
    "    edge_index = torch.tensor(np.array(edge_index), dtype=torch.long)\n",
    "    \n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=32)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcb8368-b508-41fb-baa4-5e35d16ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index = [[],[]]\n",
    "# weight = []\n",
    "\n",
    "# #用一个字典保存 通道下标对应 9 * 9 矩阵的下标\n",
    "# chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "#               9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "#               17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "#                 25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "# maps = np.zeros(shape=(9, 9), dtype=int)\n",
    "\n",
    "# for k, v in chan_to_1020.items():\n",
    "#     maps[v[0]][v[1]] = k + 1\n",
    "# print(maps)\n",
    "# plt.matshow(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5064c8-5b77-4c0a-8056-45b125c07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, Dataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    is_train_data = None\n",
    "    edge_index = None\n",
    "    def __init__(self, root, is_train_data, edge_index):\n",
    "        self.is_train_data = is_train_data\n",
    "        self.edge_index = edge_index\n",
    "        super(MyDataset, self).__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    # 检查self.processed_dir目录下是否存在self.processed_file_names属性方法返回的所有文件，没有就会走process\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.is_train_data:\n",
    "            return ['train.dataset']\n",
    "        return ['test.datset']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        features, labels = None, None\n",
    "        \n",
    "        if self.is_train_data:\n",
    "            features, labels = load_data(is_train_data=True)\n",
    "        else:\n",
    "            features, labels = load_data(is_train_data=False)\n",
    "        \n",
    "        data_list = []\n",
    "        for i in range(features.shape[0]):\n",
    "            x = torch.tensor(features[i], dtype=torch.float)\n",
    "            y = torch.tensor(labels[i].reshape(1, -1), dtype=torch.long)\n",
    "            data = Data(x = x, edge_index=self.edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379387d3-85fb-4d21-80cc-4541f55a331f",
   "metadata": {},
   "source": [
    "+ data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "\n",
    "+ data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "\n",
    "+ data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "\n",
    "+ data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "+ data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "--- \n",
    "\n",
    "- train_mask denotes against which nodes to train (140 nodes),\n",
    "\n",
    "- val_mask denotes which nodes to use for validation, e.g., to perform early stopping (500 nodes),\n",
    "\n",
    "- test_mask denotes against which nodes to test (1000 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0803e6-269e-4f79-832b-315ec6ea89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TopKPooling, SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        \n",
    "        # 通道内学习层\n",
    "        self.temporalMLPs1 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.temporalMLPs1.append(nn.Linear(128, 256, device=device))\n",
    "        \n",
    "        self.temporalMLPs2 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.temporalMLPs2.append(nn.Linear(256, 256, device=device))\n",
    "        \n",
    "        # 通道间学习层\n",
    "        self.channelMLPs1 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.channelMLPs1.append(nn.Linear(256, 256, device=device))\n",
    "        \n",
    "        self.channelMLPs2 = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            self.channelMLPs2.append(nn.Linear(256, 256, device=device))\n",
    "            \n",
    "            \n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(8192, 512)\n",
    "        self.lin2 = torch.nn.Linear(512, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # x： n * 1, 其中每个图中点的个数是不同的\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        batch_size = data.y.shape[0]\n",
    "        x = x.view(batch_size, 32, 128)\n",
    "        \n",
    "        temporalMLPs_out = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            # 通道内学习\n",
    "            x1 = self.temporalMLPs1[i](x[:, i, :])\n",
    "            x1 = F.relu(x1)\n",
    "#             x1 = self.temporalMLPs2[i](x1)\n",
    "#             x1 = F.relu(x1)\n",
    "            temporalMLPs_out.append(x1.unsqueeze(1))\n",
    "        \n",
    "        # 通道间学习\n",
    "        \n",
    "        x = torch.concat(temporalMLPs_out, dim = 1)\n",
    "        \n",
    "        channelMLPs_out = []\n",
    "        \n",
    "        for i in range(32):\n",
    "            x1 = self.channelMLPs1[i](x[:, i, :])\n",
    "            x1 = F.relu(x1)\n",
    "#             x1 = self.channelMLPs2[i](x1)\n",
    "#             x1 = F.relu(x1)\n",
    "            channelMLPs_out.append(x1)\n",
    "        \n",
    "        \n",
    "        x = torch.concat(channelMLPs_out, dim = 1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50636f52-fc65-4f4a-94e1-2e79dd89b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emo_dim):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, batch in enumerate(trainDataLoader):\n",
    "        batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        train_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_train_sample = len(trainDataLoader.dataset)\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    train_acc = train_acc / num_train_sample\n",
    "    \n",
    "    # check测试集的性能\n",
    "    vali_loss = 0\n",
    "    vali_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in testDataLoader:\n",
    "        batch.to(device)\n",
    "        output = model(batch)\n",
    "        loss = crit(output, batch.y[:, emo_dim])\n",
    "        vali_loss += loss.item()\n",
    "        output = torch.max(output, 1)[1]\n",
    "        vali_acc += (output == batch.y[:, emo_dim]).sum()\n",
    "    \n",
    "    num_test_sample = len(testDataLoader.dataset)\n",
    "    vali_loss = vali_loss / num_test_sample\n",
    "    vali_acc = vali_acc / num_test_sample\n",
    "    \n",
    "    print(f'train_loss:{train_loss:.6f}, train_acc:{train_acc:.6f}, test_loss:{vali_loss:.6f}, test_acc:{vali_acc:.6f}')\n",
    "    \n",
    "    return train_loss, train_acc, vali_loss, vali_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9bf77-c20d-484b-932b-8b98917f374b",
   "metadata": {},
   "source": [
    "# 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1e1611-fac8-4210-b800-bf02914ed6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete_graph = False\n",
    "self_loop_only = False\n",
    "emo_dim = 0\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54215ce-14bd-4072-bbd2-c16971a9f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "edge_index = get_edge_index(create_complete_graph=create_complete_graph, self_loop_only=self_loop_only)\n",
    "\n",
    "trainData = MyDataset(root='data/data_split', is_train_data=True, edge_index=edge_index)\n",
    "trainDataLoader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testData = MyDataset(root='data/data_split', is_train_data=False, edge_index=edge_index)\n",
    "testDataLoader = DataLoader(testData, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef3f3d55-fc83-475e-a697-9b8a12789830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "crit = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d67c7-8ea4-4e5d-aff1-4f2b832d6101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->epoch:1, train_loss:0.017078, train_acc:0.708955, test_loss:0.012725, test_acc:0.812500\n",
      "->epoch:2, train_loss:0.010045, train_acc:0.859245, test_loss:0.009201, test_acc:0.875391\n",
      "->epoch:3, train_loss:0.006849, train_acc:0.908536, test_loss:0.008985, test_acc:0.874219\n",
      "->epoch:4, train_loss:0.005037, train_acc:0.933058, test_loss:0.009262, test_acc:0.881901\n",
      "->epoch:5, train_loss:0.003825, train_acc:0.950362, test_loss:0.009255, test_acc:0.890104\n",
      "->epoch:6, train_loss:0.003115, train_acc:0.959780, test_loss:0.008717, test_acc:0.904818\n",
      "->epoch:7, train_loss:0.002557, train_acc:0.967535, test_loss:0.009090, test_acc:0.900000\n",
      "->epoch:8, train_loss:0.002123, train_acc:0.973437, test_loss:0.010622, test_acc:0.905469\n",
      "->epoch:9, train_loss:0.001922, train_acc:0.976461, test_loss:0.011134, test_acc:0.904688\n",
      "->epoch:10, train_loss:0.001628, train_acc:0.980208, test_loss:0.013186, test_acc:0.879688\n",
      "->epoch:11, train_loss:0.001572, train_acc:0.980642, test_loss:0.009826, test_acc:0.914323\n",
      "->epoch:12, train_loss:0.001324, train_acc:0.984071, test_loss:0.013615, test_acc:0.898958\n",
      "->epoch:13, train_loss:0.001174, train_acc:0.986806, test_loss:0.011601, test_acc:0.905599\n",
      "->epoch:14, train_loss:0.001140, train_acc:0.986863, test_loss:0.012994, test_acc:0.896224\n",
      "->epoch:15, train_loss:0.001052, train_acc:0.987891, test_loss:0.011653, test_acc:0.912891\n",
      "->epoch:16, train_loss:0.000963, train_acc:0.988730, test_loss:0.009958, test_acc:0.918620\n",
      "->epoch:17, train_loss:0.000899, train_acc:0.989497, test_loss:0.011293, test_acc:0.914844\n",
      "->epoch:18, train_loss:0.000865, train_acc:0.990090, test_loss:0.011107, test_acc:0.915104\n",
      "->epoch:19, train_loss:0.000759, train_acc:0.991594, test_loss:0.015435, test_acc:0.907422\n",
      "->epoch:20, train_loss:0.000819, train_acc:0.990929, test_loss:0.011876, test_acc:0.922917\n",
      "->epoch:21, train_loss:0.000682, train_acc:0.992260, test_loss:0.011151, test_acc:0.920833\n",
      "->epoch:22, train_loss:0.000675, train_acc:0.992752, test_loss:0.012985, test_acc:0.912630\n",
      "->epoch:23, train_loss:0.000648, train_acc:0.993113, test_loss:0.011421, test_acc:0.922526\n",
      "->epoch:24, train_loss:0.000636, train_acc:0.993316, test_loss:0.017658, test_acc:0.908594\n",
      "->epoch:25, train_loss:0.000641, train_acc:0.993041, test_loss:0.013924, test_acc:0.914714\n",
      "->epoch:26, train_loss:0.000580, train_acc:0.993663, test_loss:0.010843, test_acc:0.924089\n",
      "->epoch:27, train_loss:0.000560, train_acc:0.994256, test_loss:0.011441, test_acc:0.917839\n",
      "->epoch:28, train_loss:0.000485, train_acc:0.994936, test_loss:0.014079, test_acc:0.917188\n",
      "->epoch:29, train_loss:0.000483, train_acc:0.995081, test_loss:0.013514, test_acc:0.918750\n",
      "->epoch:30, train_loss:0.000516, train_acc:0.995067, test_loss:0.012174, test_acc:0.929427\n",
      "->epoch:31, train_loss:0.000453, train_acc:0.995385, test_loss:0.014525, test_acc:0.919792\n",
      "->epoch:32, train_loss:0.000483, train_acc:0.995284, test_loss:0.015023, test_acc:0.916146\n",
      "->epoch:33, train_loss:0.000437, train_acc:0.995587, test_loss:0.014857, test_acc:0.920052\n",
      "->epoch:34, train_loss:0.000438, train_acc:0.995877, test_loss:0.014498, test_acc:0.919792\n",
      "->epoch:35, train_loss:0.000444, train_acc:0.995848, test_loss:0.011841, test_acc:0.927474\n",
      "->epoch:36, train_loss:0.000395, train_acc:0.996007, test_loss:0.015079, test_acc:0.924089\n",
      "->epoch:37, train_loss:0.000387, train_acc:0.996195, test_loss:0.013513, test_acc:0.926172\n",
      "->epoch:38, train_loss:0.000501, train_acc:0.995356, test_loss:0.015854, test_acc:0.921354\n",
      "->epoch:39, train_loss:0.000365, train_acc:0.996123, test_loss:0.013559, test_acc:0.925130\n",
      "->epoch:40, train_loss:0.000375, train_acc:0.996238, test_loss:0.014526, test_acc:0.913411\n",
      "->epoch:41, train_loss:0.000315, train_acc:0.996586, test_loss:0.016636, test_acc:0.918620\n",
      "->epoch:42, train_loss:0.000409, train_acc:0.995877, test_loss:0.013688, test_acc:0.920833\n",
      "->epoch:43, train_loss:0.000355, train_acc:0.996354, test_loss:0.014615, test_acc:0.917448\n",
      "->epoch:44, train_loss:0.000294, train_acc:0.997135, test_loss:0.015435, test_acc:0.923047\n",
      "->epoch:45, train_loss:0.000342, train_acc:0.996846, test_loss:0.016404, test_acc:0.926953\n",
      "->epoch:46, train_loss:0.000314, train_acc:0.996817, test_loss:0.014688, test_acc:0.921484\n",
      "->epoch:47, train_loss:0.000374, train_acc:0.996701, test_loss:0.013244, test_acc:0.926953\n",
      "->epoch:48, train_loss:0.000306, train_acc:0.997106, test_loss:0.014707, test_acc:0.923698\n",
      "->epoch:49, train_loss:0.000287, train_acc:0.996991, test_loss:0.014460, test_acc:0.930339\n",
      "->epoch:50, train_loss:0.000289, train_acc:0.997121, test_loss:0.017545, test_acc:0.924609\n",
      "->epoch:51, train_loss:0.000330, train_acc:0.996571, test_loss:0.015871, test_acc:0.923698\n",
      "->epoch:52, train_loss:0.000274, train_acc:0.997627, test_loss:0.014533, test_acc:0.922396\n",
      "->epoch:53, train_loss:0.000308, train_acc:0.997121, test_loss:0.012271, test_acc:0.920833\n",
      "->epoch:54, train_loss:0.000259, train_acc:0.997410, test_loss:0.015636, test_acc:0.919531\n",
      "->epoch:55, train_loss:0.000302, train_acc:0.996991, test_loss:0.015064, test_acc:0.927344\n",
      "->epoch:56, train_loss:0.000211, train_acc:0.997815, test_loss:0.020058, test_acc:0.920313\n",
      "->epoch:57, train_loss:0.000225, train_acc:0.997772, test_loss:0.016250, test_acc:0.926823\n",
      "->epoch:58, train_loss:0.000283, train_acc:0.997222, test_loss:0.017906, test_acc:0.915885\n",
      "->epoch:59, train_loss:0.000273, train_acc:0.997497, test_loss:0.014660, test_acc:0.924740\n",
      "->epoch:60, train_loss:0.000244, train_acc:0.997844, test_loss:0.015169, test_acc:0.913411\n",
      "->epoch:61, train_loss:0.000240, train_acc:0.997700, test_loss:0.015073, test_acc:0.923828\n",
      "->epoch:62, train_loss:0.000244, train_acc:0.997512, test_loss:0.019399, test_acc:0.901823\n",
      "->epoch:63, train_loss:0.000208, train_acc:0.998235, test_loss:0.015872, test_acc:0.922526\n",
      "->epoch:64, train_loss:0.000241, train_acc:0.997598, test_loss:0.016498, test_acc:0.924740\n",
      "->epoch:65, train_loss:0.000229, train_acc:0.998105, test_loss:0.016476, test_acc:0.925000\n",
      "->epoch:66, train_loss:0.000230, train_acc:0.997656, test_loss:0.016651, test_acc:0.924479\n",
      "->epoch:67, train_loss:0.000197, train_acc:0.998076, test_loss:0.017247, test_acc:0.921354\n",
      "->epoch:68, train_loss:0.000217, train_acc:0.997873, test_loss:0.018301, test_acc:0.924089\n",
      "->epoch:69, train_loss:0.000236, train_acc:0.997714, test_loss:0.015290, test_acc:0.926302\n",
      "->epoch:70, train_loss:0.000219, train_acc:0.998003, test_loss:0.016793, test_acc:0.914974\n",
      "->epoch:71, train_loss:0.000226, train_acc:0.998148, test_loss:0.017462, test_acc:0.924609\n",
      "->epoch:72, train_loss:0.000212, train_acc:0.997786, test_loss:0.016167, test_acc:0.924349\n",
      "->epoch:73, train_loss:0.000234, train_acc:0.997989, test_loss:0.016701, test_acc:0.925521\n",
      "->epoch:74, train_loss:0.000170, train_acc:0.998683, test_loss:0.018021, test_acc:0.925391\n",
      "->epoch:75, train_loss:0.000248, train_acc:0.997613, test_loss:0.017069, test_acc:0.925391\n",
      "->epoch:76, train_loss:0.000197, train_acc:0.998119, test_loss:0.017725, test_acc:0.922917\n",
      "->epoch:77, train_loss:0.000247, train_acc:0.997873, test_loss:0.018054, test_acc:0.915755\n",
      "->epoch:78, train_loss:0.000173, train_acc:0.998293, test_loss:0.017852, test_acc:0.923177\n",
      "->epoch:79, train_loss:0.000173, train_acc:0.998452, test_loss:0.020933, test_acc:0.925260\n",
      "->epoch:80, train_loss:0.000262, train_acc:0.997917, test_loss:0.016474, test_acc:0.925130\n",
      "->epoch:81, train_loss:0.000192, train_acc:0.998307, test_loss:0.016540, test_acc:0.926302\n",
      "->epoch:82, train_loss:0.000183, train_acc:0.998481, test_loss:0.017443, test_acc:0.917448\n",
      "->epoch:83, train_loss:0.000152, train_acc:0.998510, test_loss:0.020411, test_acc:0.922005\n",
      "->epoch:84, train_loss:0.000272, train_acc:0.997729, test_loss:0.018047, test_acc:0.918750\n",
      "->epoch:85, train_loss:0.000187, train_acc:0.998770, test_loss:0.016170, test_acc:0.923828\n",
      "->epoch:86, train_loss:0.000198, train_acc:0.998163, test_loss:0.018306, test_acc:0.926432\n",
      "->epoch:87, train_loss:0.000176, train_acc:0.998495, test_loss:0.022566, test_acc:0.907422\n",
      "->epoch:88, train_loss:0.000218, train_acc:0.998105, test_loss:0.019244, test_acc:0.922396\n",
      "->epoch:89, train_loss:0.000107, train_acc:0.999089, test_loss:0.020264, test_acc:0.925781\n",
      "->epoch:90, train_loss:0.000232, train_acc:0.998134, test_loss:0.018627, test_acc:0.924089\n",
      "->epoch:91, train_loss:0.000181, train_acc:0.998568, test_loss:0.017961, test_acc:0.929167\n",
      "->epoch:92, train_loss:0.000200, train_acc:0.998437, test_loss:0.015150, test_acc:0.926563\n",
      "->epoch:93, train_loss:0.000187, train_acc:0.998192, test_loss:0.017176, test_acc:0.926302\n",
      "->epoch:94, train_loss:0.000176, train_acc:0.998336, test_loss:0.019313, test_acc:0.925651\n",
      "->epoch:95, train_loss:0.000168, train_acc:0.998597, test_loss:0.015770, test_acc:0.931120\n",
      "->epoch:96, train_loss:0.000145, train_acc:0.998712, test_loss:0.021252, test_acc:0.927214\n",
      "->epoch:97, train_loss:0.000200, train_acc:0.998192, test_loss:0.020688, test_acc:0.924740\n",
      "->epoch:98, train_loss:0.000221, train_acc:0.998423, test_loss:0.015059, test_acc:0.923047\n",
      "->epoch:99, train_loss:0.000205, train_acc:0.998264, test_loss:0.021893, test_acc:0.922396\n",
      "->epoch:100, train_loss:0.000144, train_acc:0.998553, test_loss:0.026495, test_acc:0.926302\n",
      "->epoch:101, train_loss:0.000196, train_acc:0.998394, test_loss:0.017613, test_acc:0.925911\n",
      "->epoch:102, train_loss:0.000241, train_acc:0.998524, test_loss:0.017559, test_acc:0.925000\n",
      "->epoch:103, train_loss:0.000203, train_acc:0.998163, test_loss:0.015870, test_acc:0.927474\n",
      "->epoch:104, train_loss:0.000111, train_acc:0.999002, test_loss:0.018500, test_acc:0.921484\n",
      "->epoch:105, train_loss:0.000150, train_acc:0.998539, test_loss:0.018781, test_acc:0.927995\n",
      "->epoch:106, train_loss:0.000115, train_acc:0.998987, test_loss:0.017606, test_acc:0.924740\n",
      "->epoch:107, train_loss:0.000136, train_acc:0.998799, test_loss:0.020494, test_acc:0.929167\n",
      "->epoch:108, train_loss:0.000168, train_acc:0.998741, test_loss:0.021854, test_acc:0.925130\n",
      "->epoch:109, train_loss:0.000177, train_acc:0.998568, test_loss:0.015529, test_acc:0.924870\n",
      "->epoch:110, train_loss:0.000136, train_acc:0.998683, test_loss:0.015468, test_acc:0.925391\n",
      "->epoch:111, train_loss:0.000136, train_acc:0.998741, test_loss:0.019463, test_acc:0.926172\n",
      "->epoch:112, train_loss:0.000132, train_acc:0.998683, test_loss:0.019718, test_acc:0.923698\n",
      "->epoch:113, train_loss:0.000134, train_acc:0.998770, test_loss:0.019583, test_acc:0.923047\n",
      "->epoch:114, train_loss:0.000146, train_acc:0.998611, test_loss:0.023808, test_acc:0.918099\n",
      "->epoch:115, train_loss:0.000136, train_acc:0.998915, test_loss:0.016536, test_acc:0.925000\n",
      "->epoch:116, train_loss:0.000114, train_acc:0.998944, test_loss:0.022306, test_acc:0.925651\n",
      "->epoch:117, train_loss:0.000227, train_acc:0.998351, test_loss:0.018993, test_acc:0.927344\n",
      "->epoch:118, train_loss:0.000175, train_acc:0.998828, test_loss:0.018657, test_acc:0.924870\n",
      "->epoch:119, train_loss:0.000072, train_acc:0.999320, test_loss:0.022441, test_acc:0.932292\n",
      "->epoch:120, train_loss:0.000132, train_acc:0.998886, test_loss:0.020717, test_acc:0.923307\n",
      "->epoch:121, train_loss:0.000139, train_acc:0.998669, test_loss:0.018922, test_acc:0.929427\n",
      "->epoch:122, train_loss:0.000183, train_acc:0.998568, test_loss:0.021131, test_acc:0.927214\n",
      "->epoch:123, train_loss:0.000182, train_acc:0.998510, test_loss:0.021503, test_acc:0.924089\n",
      "->epoch:124, train_loss:0.000133, train_acc:0.998944, test_loss:0.021161, test_acc:0.925521\n",
      "->epoch:125, train_loss:0.000131, train_acc:0.998929, test_loss:0.020340, test_acc:0.924089\n",
      "->epoch:126, train_loss:0.000118, train_acc:0.998973, test_loss:0.025262, test_acc:0.925651\n",
      "->epoch:127, train_loss:0.000184, train_acc:0.998524, test_loss:0.016378, test_acc:0.925781\n",
      "->epoch:128, train_loss:0.000080, train_acc:0.999132, test_loss:0.020099, test_acc:0.927474\n",
      "->epoch:129, train_loss:0.000159, train_acc:0.998785, test_loss:0.019686, test_acc:0.921484\n",
      "->epoch:130, train_loss:0.000127, train_acc:0.999031, test_loss:0.019799, test_acc:0.928906\n",
      "->epoch:131, train_loss:0.000148, train_acc:0.998741, test_loss:0.017113, test_acc:0.921094\n",
      "->epoch:132, train_loss:0.000065, train_acc:0.999450, test_loss:0.026073, test_acc:0.920573\n",
      "->epoch:133, train_loss:0.000217, train_acc:0.998148, test_loss:0.016826, test_acc:0.931120\n",
      "->epoch:134, train_loss:0.000097, train_acc:0.999132, test_loss:0.021878, test_acc:0.929427\n",
      "->epoch:135, train_loss:0.000205, train_acc:0.998553, test_loss:0.018917, test_acc:0.929427\n",
      "->epoch:136, train_loss:0.000105, train_acc:0.999060, test_loss:0.021903, test_acc:0.924349\n",
      "->epoch:137, train_loss:0.000157, train_acc:0.998857, test_loss:0.018332, test_acc:0.927083\n",
      "->epoch:138, train_loss:0.000095, train_acc:0.999190, test_loss:0.022291, test_acc:0.926172\n",
      "->epoch:139, train_loss:0.000135, train_acc:0.998553, test_loss:0.021882, test_acc:0.919662\n",
      "->epoch:140, train_loss:0.000106, train_acc:0.998915, test_loss:0.023300, test_acc:0.927344\n",
      "->epoch:141, train_loss:0.000161, train_acc:0.998466, test_loss:0.021541, test_acc:0.928125\n",
      "->epoch:142, train_loss:0.000128, train_acc:0.998944, test_loss:0.021369, test_acc:0.923177\n",
      "->epoch:143, train_loss:0.000115, train_acc:0.999320, test_loss:0.020055, test_acc:0.914583\n",
      "->epoch:144, train_loss:0.000130, train_acc:0.998973, test_loss:0.023612, test_acc:0.926953\n",
      "->epoch:145, train_loss:0.000094, train_acc:0.999407, test_loss:0.024994, test_acc:0.929818\n",
      "->epoch:146, train_loss:0.000186, train_acc:0.998626, test_loss:0.023239, test_acc:0.927474\n",
      "->epoch:147, train_loss:0.000121, train_acc:0.999146, test_loss:0.020809, test_acc:0.923438\n",
      "->epoch:148, train_loss:0.000198, train_acc:0.998886, test_loss:0.017523, test_acc:0.925651\n",
      "->epoch:149, train_loss:0.000137, train_acc:0.998944, test_loss:0.021296, test_acc:0.928776\n",
      "->epoch:150, train_loss:0.000110, train_acc:0.999378, test_loss:0.027215, test_acc:0.929818\n",
      "->epoch:151, train_loss:0.000171, train_acc:0.998597, test_loss:0.019504, test_acc:0.924479\n",
      "->epoch:152, train_loss:0.000112, train_acc:0.999146, test_loss:0.028094, test_acc:0.924609\n",
      "->epoch:153, train_loss:0.000177, train_acc:0.998756, test_loss:0.020106, test_acc:0.930078\n",
      "->epoch:154, train_loss:0.000105, train_acc:0.999103, test_loss:0.020320, test_acc:0.929688\n",
      "->epoch:155, train_loss:0.000148, train_acc:0.998987, test_loss:0.023568, test_acc:0.927083\n",
      "->epoch:156, train_loss:0.000159, train_acc:0.998944, test_loss:0.023407, test_acc:0.924219\n",
      "->epoch:157, train_loss:0.000129, train_acc:0.998987, test_loss:0.018799, test_acc:0.927604\n",
      "->epoch:158, train_loss:0.000066, train_acc:0.999363, test_loss:0.024237, test_acc:0.928125\n",
      "->epoch:159, train_loss:0.000096, train_acc:0.999219, test_loss:0.024534, test_acc:0.924349\n",
      "->epoch:160, train_loss:0.000140, train_acc:0.998973, test_loss:0.019939, test_acc:0.924479\n",
      "->epoch:161, train_loss:0.000181, train_acc:0.998843, test_loss:0.020518, test_acc:0.926563\n",
      "->epoch:162, train_loss:0.000090, train_acc:0.999306, test_loss:0.022919, test_acc:0.924479\n",
      "->epoch:163, train_loss:0.000146, train_acc:0.998727, test_loss:0.021911, test_acc:0.929167\n",
      "->epoch:164, train_loss:0.000140, train_acc:0.998973, test_loss:0.020319, test_acc:0.923828\n",
      "->epoch:165, train_loss:0.000139, train_acc:0.998828, test_loss:0.020593, test_acc:0.926172\n",
      "->epoch:166, train_loss:0.000067, train_acc:0.999508, test_loss:0.030194, test_acc:0.920313\n",
      "->epoch:167, train_loss:0.000147, train_acc:0.998828, test_loss:0.023854, test_acc:0.928125\n",
      "->epoch:168, train_loss:0.000094, train_acc:0.999219, test_loss:0.025005, test_acc:0.927604\n",
      "->epoch:169, train_loss:0.000140, train_acc:0.999002, test_loss:0.023327, test_acc:0.929557\n",
      "->epoch:170, train_loss:0.000153, train_acc:0.999016, test_loss:0.028448, test_acc:0.926042\n",
      "->epoch:171, train_loss:0.000127, train_acc:0.999146, test_loss:0.027866, test_acc:0.928255\n",
      "->epoch:172, train_loss:0.000163, train_acc:0.998741, test_loss:0.018906, test_acc:0.929427\n",
      "->epoch:173, train_loss:0.000174, train_acc:0.998886, test_loss:0.026217, test_acc:0.929818\n",
      "->epoch:174, train_loss:0.000087, train_acc:0.999349, test_loss:0.027083, test_acc:0.922396\n",
      "->epoch:175, train_loss:0.000105, train_acc:0.999161, test_loss:0.029592, test_acc:0.923568\n",
      "->epoch:176, train_loss:0.000098, train_acc:0.999204, test_loss:0.029293, test_acc:0.926953\n",
      "->epoch:177, train_loss:0.000085, train_acc:0.999479, test_loss:0.027005, test_acc:0.931510\n",
      "->epoch:178, train_loss:0.000140, train_acc:0.999016, test_loss:0.026579, test_acc:0.925391\n",
      "->epoch:179, train_loss:0.000082, train_acc:0.999190, test_loss:0.022215, test_acc:0.928255\n",
      "->epoch:180, train_loss:0.000073, train_acc:0.999436, test_loss:0.025698, test_acc:0.934115\n",
      "->epoch:181, train_loss:0.000127, train_acc:0.998973, test_loss:0.024197, test_acc:0.927995\n",
      "->epoch:182, train_loss:0.000131, train_acc:0.998973, test_loss:0.034984, test_acc:0.926823\n",
      "->epoch:183, train_loss:0.000153, train_acc:0.999016, test_loss:0.023707, test_acc:0.928906\n",
      "->epoch:184, train_loss:0.000109, train_acc:0.999204, test_loss:0.029931, test_acc:0.928516\n",
      "->epoch:185, train_loss:0.000123, train_acc:0.999175, test_loss:0.022259, test_acc:0.926693\n",
      "->epoch:186, train_loss:0.000077, train_acc:0.999306, test_loss:0.023324, test_acc:0.930339\n",
      "->epoch:187, "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f'->epoch:{epoch + 1}', end = ', ')\n",
    "    train_loss, train_acc, val_loss, val_acc = train(emo_dim)\n",
    "#     print(f'->epoch:{epoch:3d}, train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, val_loss={val_loss:.6f}, val_acc={val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaaa6f-c3fb-4b07-91b9-aa185cf2b0ba",
   "metadata": {},
   "source": [
    "- MLP_base ->epoch:195, train_loss:0.000070, train_acc:0.999494, test_loss:0.022968, test_acc:0.933854\n",
    "- MLP_2层dropout（p=0.2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa21ed2-da43-4dc3-ad70-081231e98ef4",
   "metadata": {},
   "source": [
    "# 增加模型容量\n",
    "- ->epoch:86, train_loss:0.002378, train_acc:0.980787, test_loss:0.017223, test_acc:0.908203\n",
    "- 改为heads=3， ->epoch:167, train_loss:0.001960, train_acc:0.985171, test_loss:0.022945, test_acc:0.910417"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964d82-e4ca-40db-ad83-0722b8a6b880",
   "metadata": {},
   "source": [
    "## 比较实验\n",
    "### GCN\n",
    "+ 仅包括自环时，->epoch:25, train_loss:0.000922, train_acc:0.990784, test_loss:0.010875, test_acc:0.919401\n",
    "+ 加上3x3卷积核的邻接边时，->epoch:32, train_loss:0.000819, train_acc:0.992173, test_loss:0.020655, test_acc:0.895313，邻接边设计的不好，限制了模型的发挥\n",
    "+ 别人的方法的准确率：89/90、93/94\n",
    "### GAT\n",
    "+ 仅包括自环时，->epoch:30, train_loss:0.001252, train_acc:0.986531, test_loss:0.015308, test_acc:0.912630\n",
    "+ 使用自己设计的边，->epoch:123, train_loss:0.002104, train_acc:0.982161, test_loss:0.029411, test_acc:0.904688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f4ff-9fbb-4323-ad0d-4dfc7848e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
