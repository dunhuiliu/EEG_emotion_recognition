{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9dbd5b7-7730-4ceb-a7c8-5803799f39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06b14e2-1602-476e-86dd-a80a1bde64e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76800, 128, 9, 9) (1280, 128, 9, 9) (1280, 2)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "load_path = '../global_data/time_76800x128x9x9/'\n",
    "\n",
    "trials = np.load(load_path + 'trials.npy')\n",
    "bases = np.load(load_path + 'bases.npy')\n",
    "labels = np.load(load_path + 'labels.npy')\n",
    "# chw格式\n",
    "print(trials.shape, bases.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07999f1-2ab1-4593-bc02-3431dd862492",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ac8020-c784-48f4-a744-34785e882662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76800, 128, 9, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去基线\n",
    "trials_de_base = []\n",
    "for i, base in enumerate(bases):\n",
    "    trials_de_base.append(trials[i * 60 : (i + 1) * 60] - base)\n",
    "trials_de_base = np.array(trials_de_base)\n",
    "trials_de_base = trials_de_base.reshape((-1, 128, 9, 9))\n",
    "trials_de_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680b57eb-5e34-4faa-9361-facf96bfda64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76800, 2)\n"
     ]
    }
   ],
   "source": [
    "# 离散化标签\n",
    "labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "# 复制以对齐样本\n",
    "labels = np.repeat(labels, 60, axis = 0)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa93a8e4-84e0-4123-bba7-17017d455545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_split(features, labels, IDs, train_ratio = 0.9):\n",
    "    print('split dataset...')\n",
    "    sample_cnt = labels.shape[0]\n",
    "    # 数据集划分\n",
    "    train_ratio = train_ratio\n",
    "    shuffer_list = np.arange(sample_cnt)\n",
    "    np.random.shuffle(shuffer_list)\n",
    "\n",
    "    features_shuffer = features[shuffer_list] # trials_de_base\n",
    "    labels_shuffer = labels[shuffer_list]\n",
    "    IDs_shuffer = IDs[shuffer_list]\n",
    "\n",
    "    x_train = features_shuffer[:int(train_ratio * sample_cnt)]\n",
    "    y_train = labels_shuffer[:int(train_ratio * sample_cnt)]\n",
    "    ID_train = IDs_shuffer[:int(train_ratio * sample_cnt)]\n",
    "    x_test = features_shuffer[int(train_ratio * sample_cnt):]\n",
    "    y_test = labels_shuffer[int(train_ratio * sample_cnt):]\n",
    "    ID_test = IDs_shuffer[int(train_ratio * sample_cnt):]\n",
    "    return x_train, y_train, ID_train, x_test, y_test, ID_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c13482-2a9a-4ada-ad9b-7aff8e426afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def z_norm(x_train, x_test):# z归一化\n",
    "    print('z_norm...')\n",
    "    chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "                  9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "                  17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "                    25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "\n",
    "    # 需要归一化的位置\n",
    "    need_norm_pos = np.zeros(shape=(9,9), dtype=int)\n",
    "\n",
    "    for val in chan_to_1020.values():\n",
    "        need_norm_pos[val[0]][val[1]] = 1\n",
    "    # print(need_norm_pos)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if need_norm_pos[i][j] == 0:\n",
    "                continue\n",
    "            for t in range(128):\n",
    "                mean = np.mean(x_train[:, t, i, j])\n",
    "                std = np.std(x_train[:, t, i, j])\n",
    "                x_train[:, t, i, j] -= mean\n",
    "                x_train[:, t, i, j] /= std\n",
    "                x_test[:, t, i, j] -= mean\n",
    "                x_test[:, t, i, j] /= std\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0edca54f-4487-47d3-aea2-c979b47068ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参设置\n",
    "batch_size = 32\n",
    "max_epoch = 1000\n",
    "emotion_dim = 0\n",
    "lr = 0.001\n",
    "\n",
    "patient = 100\n",
    "emo_imp = 0.9\n",
    "\n",
    "model_save_path = 'model/MTL_model/'\n",
    "IndiviTransNet_model_load_path = 'model/transformer_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abfb40a5-74a2-4757-8cc3-582fa990983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1795f6-bcc9-4a40-a868-4121e266f595",
   "metadata": {},
   "source": [
    "# 考虑流出一部分人，不用于特征映射，但也用于训练MTL，且只用情感损失来监督学习，这是否对新的测试受试者有利？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1236e154-5699-4834-b410-5e88850e76f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split dataset...\n",
      "z_norm...\n",
      "make tensor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "f:\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "epoch:0\n",
      "batch: 0, train_emo_loss: 0.022230, train_id_loss: 0.109053, train_emo_acc: 0.531250, train_id_acc: 0.062500\n",
      "batch: 100, train_emo_loss: 0.020709, train_id_loss: 0.095327, train_emo_acc: 0.656250, train_id_acc: 0.156250\n",
      "batch: 200, train_emo_loss: 0.017812, train_id_loss: 0.058151, train_emo_acc: 0.593750, train_id_acc: 0.312500\n",
      "batch: 300, train_emo_loss: 0.018586, train_id_loss: 0.034609, train_emo_acc: 0.781250, train_id_acc: 0.718750\n",
      "batch: 400, train_emo_loss: 0.019669, train_id_loss: 0.027489, train_emo_acc: 0.656250, train_id_acc: 0.750000\n",
      "batch: 500, train_emo_loss: 0.015764, train_id_loss: 0.015648, train_emo_acc: 0.781250, train_id_acc: 0.937500\n",
      "batch: 600, train_emo_loss: 0.013810, train_id_loss: 0.017668, train_emo_acc: 0.875000, train_id_acc: 0.781250\n",
      "batch: 700, train_emo_loss: 0.012896, train_id_loss: 0.019595, train_emo_acc: 0.812500, train_id_acc: 0.812500\n",
      "batch: 800, train_emo_loss: 0.010194, train_id_loss: 0.005600, train_emo_acc: 0.843750, train_id_acc: 0.968750\n",
      "batch: 900, train_emo_loss: 0.016433, train_id_loss: 0.007089, train_emo_acc: 0.750000, train_id_acc: 0.937500\n",
      "batch: 1000, train_emo_loss: 0.012905, train_id_loss: 0.011952, train_emo_acc: 0.781250, train_id_acc: 0.906250\n",
      "batch: 1100, train_emo_loss: 0.017290, train_id_loss: 0.017540, train_emo_acc: 0.750000, train_id_acc: 0.812500\n",
      "batch: 1200, train_emo_loss: 0.011966, train_id_loss: 0.013012, train_emo_acc: 0.812500, train_id_acc: 0.906250\n",
      "batch: 1300, train_emo_loss: 0.010981, train_id_loss: 0.018611, train_emo_acc: 0.875000, train_id_acc: 0.906250\n",
      "batch: 1400, train_emo_loss: 0.008203, train_id_loss: 0.007312, train_emo_acc: 0.937500, train_id_acc: 0.937500\n",
      "batch: 1500, train_emo_loss: 0.008223, train_id_loss: 0.004086, train_emo_acc: 0.906250, train_id_acc: 0.968750\n",
      "batch: 1600, train_emo_loss: 0.010031, train_id_loss: 0.006770, train_emo_acc: 0.875000, train_id_acc: 0.875000\n",
      "batch: 1700, train_emo_loss: 0.010711, train_id_loss: 0.006133, train_emo_acc: 0.812500, train_id_acc: 0.937500\n",
      "batch: 1800, train_emo_loss: 0.008450, train_id_loss: 0.022610, train_emo_acc: 0.906250, train_id_acc: 0.812500\n",
      "batch: 1900, train_emo_loss: 0.010341, train_id_loss: 0.003259, train_emo_acc: 0.843750, train_id_acc: 1.000000\n",
      "batch: 2000, train_emo_loss: 0.008022, train_id_loss: 0.008345, train_emo_acc: 0.937500, train_id_acc: 0.906250\n",
      "epoch: 0, vali_emo_loss: 0.000042, vali_id_loss: 0.000038, vali_emo_acc: 0.874462, vali_id_acc: 0.921909\n",
      "epoch:1\n",
      "batch: 0, train_emo_loss: 0.007728, train_id_loss: 0.004279, train_emo_acc: 0.906250, train_id_acc: 0.937500\n",
      "batch: 100, train_emo_loss: 0.008944, train_id_loss: 0.002916, train_emo_acc: 0.906250, train_id_acc: 1.000000\n",
      "batch: 200, train_emo_loss: 0.006788, train_id_loss: 0.005529, train_emo_acc: 1.000000, train_id_acc: 0.906250\n",
      "batch: 300, train_emo_loss: 0.008031, train_id_loss: 0.004876, train_emo_acc: 0.875000, train_id_acc: 0.906250\n",
      "batch: 400, train_emo_loss: 0.008214, train_id_loss: 0.010377, train_emo_acc: 0.937500, train_id_acc: 0.875000\n",
      "batch: 500, train_emo_loss: 0.008306, train_id_loss: 0.004315, train_emo_acc: 0.875000, train_id_acc: 0.968750\n",
      "batch: 600, train_emo_loss: 0.012723, train_id_loss: 0.032691, train_emo_acc: 0.750000, train_id_acc: 0.750000\n",
      "batch: 700, train_emo_loss: 0.011350, train_id_loss: 0.003068, train_emo_acc: 0.781250, train_id_acc: 0.968750\n",
      "batch: 800, train_emo_loss: 0.008206, train_id_loss: 0.004289, train_emo_acc: 0.906250, train_id_acc: 0.937500\n",
      "batch: 900, train_emo_loss: 0.010872, train_id_loss: 0.001685, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1000, train_emo_loss: 0.006764, train_id_loss: 0.010483, train_emo_acc: 0.875000, train_id_acc: 0.906250\n",
      "batch: 1100, train_emo_loss: 0.007428, train_id_loss: 0.005682, train_emo_acc: 0.968750, train_id_acc: 0.937500\n",
      "batch: 1200, train_emo_loss: 0.007379, train_id_loss: 0.002219, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1300, train_emo_loss: 0.008324, train_id_loss: 0.003353, train_emo_acc: 0.906250, train_id_acc: 0.968750\n",
      "batch: 1400, train_emo_loss: 0.005303, train_id_loss: 0.002647, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1500, train_emo_loss: 0.005701, train_id_loss: 0.001944, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1600, train_emo_loss: 0.005632, train_id_loss: 0.001447, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1700, train_emo_loss: 0.006875, train_id_loss: 0.006066, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 1800, train_emo_loss: 0.006728, train_id_loss: 0.003616, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1900, train_emo_loss: 0.005635, train_id_loss: 0.008450, train_emo_acc: 0.937500, train_id_acc: 0.937500\n",
      "batch: 2000, train_emo_loss: 0.007920, train_id_loss: 0.003401, train_emo_acc: 0.906250, train_id_acc: 0.968750\n",
      "epoch: 1, vali_emo_loss: 0.000028, vali_id_loss: 0.000014, vali_emo_acc: 0.931989, vali_id_acc: 0.977823\n",
      "epoch:2\n",
      "batch: 0, train_emo_loss: 0.005394, train_id_loss: 0.002841, train_emo_acc: 0.968750, train_id_acc: 0.968750\n",
      "batch: 100, train_emo_loss: 0.004570, train_id_loss: 0.001841, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 200, train_emo_loss: 0.006432, train_id_loss: 0.008981, train_emo_acc: 0.875000, train_id_acc: 0.906250\n",
      "batch: 300, train_emo_loss: 0.009013, train_id_loss: 0.004433, train_emo_acc: 0.812500, train_id_acc: 0.937500\n",
      "batch: 400, train_emo_loss: 0.006106, train_id_loss: 0.005312, train_emo_acc: 0.968750, train_id_acc: 0.906250\n",
      "batch: 500, train_emo_loss: 0.010983, train_id_loss: 0.003228, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 600, train_emo_loss: 0.006467, train_id_loss: 0.014095, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 700, train_emo_loss: 0.006243, train_id_loss: 0.002029, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 800, train_emo_loss: 0.007295, train_id_loss: 0.002936, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 900, train_emo_loss: 0.007444, train_id_loss: 0.007292, train_emo_acc: 0.906250, train_id_acc: 0.968750\n",
      "batch: 1000, train_emo_loss: 0.005358, train_id_loss: 0.005117, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1100, train_emo_loss: 0.006513, train_id_loss: 0.010182, train_emo_acc: 0.906250, train_id_acc: 0.937500\n",
      "batch: 1200, train_emo_loss: 0.007511, train_id_loss: 0.003364, train_emo_acc: 0.906250, train_id_acc: 0.968750\n",
      "batch: 1300, train_emo_loss: 0.003406, train_id_loss: 0.001854, train_emo_acc: 1.000000, train_id_acc: 1.000000\n",
      "batch: 1400, train_emo_loss: 0.009032, train_id_loss: 0.012171, train_emo_acc: 0.937500, train_id_acc: 0.937500\n",
      "batch: 1500, train_emo_loss: 0.005795, train_id_loss: 0.001038, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1600, train_emo_loss: 0.004030, train_id_loss: 0.003358, train_emo_acc: 1.000000, train_id_acc: 0.968750\n",
      "batch: 1700, train_emo_loss: 0.007702, train_id_loss: 0.002377, train_emo_acc: 0.875000, train_id_acc: 0.968750\n",
      "batch: 1800, train_emo_loss: 0.004927, train_id_loss: 0.004092, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 1900, train_emo_loss: 0.003332, train_id_loss: 0.006187, train_emo_acc: 1.000000, train_id_acc: 0.968750\n",
      "batch: 2000, train_emo_loss: 0.010272, train_id_loss: 0.002317, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "epoch: 2, vali_emo_loss: 0.000026, vali_id_loss: 0.000027, vali_emo_acc: 0.939651, vali_id_acc: 0.950403\n",
      "epoch:3\n",
      "batch: 0, train_emo_loss: 0.004632, train_id_loss: 0.005933, train_emo_acc: 0.937500, train_id_acc: 0.937500\n",
      "batch: 100, train_emo_loss: 0.006149, train_id_loss: 0.001639, train_emo_acc: 0.875000, train_id_acc: 1.000000\n",
      "batch: 200, train_emo_loss: 0.006771, train_id_loss: 0.003512, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 300, train_emo_loss: 0.005138, train_id_loss: 0.001469, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 400, train_emo_loss: 0.003407, train_id_loss: 0.003294, train_emo_acc: 1.000000, train_id_acc: 1.000000\n",
      "batch: 500, train_emo_loss: 0.006049, train_id_loss: 0.001704, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 600, train_emo_loss: 0.004366, train_id_loss: 0.001464, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 700, train_emo_loss: 0.002932, train_id_loss: 0.003126, train_emo_acc: 1.000000, train_id_acc: 1.000000\n",
      "batch: 800, train_emo_loss: 0.003843, train_id_loss: 0.002385, train_emo_acc: 0.968750, train_id_acc: 0.968750\n",
      "batch: 900, train_emo_loss: 0.003336, train_id_loss: 0.001819, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1000, train_emo_loss: 0.004772, train_id_loss: 0.002293, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1100, train_emo_loss: 0.004708, train_id_loss: 0.001152, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1200, train_emo_loss: 0.006252, train_id_loss: 0.005999, train_emo_acc: 0.937500, train_id_acc: 0.906250\n",
      "batch: 1300, train_emo_loss: 0.008520, train_id_loss: 0.002705, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 1400, train_emo_loss: 0.002035, train_id_loss: 0.000567, train_emo_acc: 1.000000, train_id_acc: 1.000000\n",
      "batch: 1500, train_emo_loss: 0.004902, train_id_loss: 0.000967, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1600, train_emo_loss: 0.004177, train_id_loss: 0.003907, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1700, train_emo_loss: 0.005128, train_id_loss: 0.002909, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1800, train_emo_loss: 0.004965, train_id_loss: 0.000409, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 1900, train_emo_loss: 0.003174, train_id_loss: 0.002651, train_emo_acc: 1.000000, train_id_acc: 1.000000\n",
      "batch: 2000, train_emo_loss: 0.008674, train_id_loss: 0.001429, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "epoch: 3, vali_emo_loss: 0.000021, vali_id_loss: 0.000010, vali_emo_acc: 0.956720, vali_id_acc: 0.990726\n",
      "epoch:4\n",
      "batch: 0, train_emo_loss: 0.002996, train_id_loss: 0.001955, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 100, train_emo_loss: 0.004144, train_id_loss: 0.000733, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 200, train_emo_loss: 0.005957, train_id_loss: 0.037414, train_emo_acc: 0.906250, train_id_acc: 0.906250\n",
      "batch: 300, train_emo_loss: 0.004307, train_id_loss: 0.025761, train_emo_acc: 0.937500, train_id_acc: 0.875000\n",
      "batch: 400, train_emo_loss: 0.007710, train_id_loss: 0.003639, train_emo_acc: 0.843750, train_id_acc: 0.968750\n",
      "batch: 500, train_emo_loss: 0.010198, train_id_loss: 0.023452, train_emo_acc: 0.812500, train_id_acc: 0.906250\n",
      "batch: 600, train_emo_loss: 0.022343, train_id_loss: 0.001294, train_emo_acc: 0.937500, train_id_acc: 0.968750\n",
      "batch: 700, train_emo_loss: 0.003775, train_id_loss: 0.002452, train_emo_acc: 0.968750, train_id_acc: 0.968750\n",
      "batch: 800, train_emo_loss: 0.005428, train_id_loss: 0.001335, train_emo_acc: 0.937500, train_id_acc: 1.000000\n",
      "batch: 900, train_emo_loss: 0.003718, train_id_loss: 0.029946, train_emo_acc: 0.906250, train_id_acc: 0.968750\n",
      "batch: 1000, train_emo_loss: 0.002835, train_id_loss: 0.001629, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1100, train_emo_loss: 0.004923, train_id_loss: 0.002644, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1200, train_emo_loss: 0.003205, train_id_loss: 0.007962, train_emo_acc: 0.968750, train_id_acc: 0.937500\n",
      "batch: 1300, train_emo_loss: 0.004998, train_id_loss: 0.001773, train_emo_acc: 0.968750, train_id_acc: 1.000000\n",
      "batch: 1400, train_emo_loss: 0.002302, train_id_loss: 0.000142, train_emo_acc: 1.000000, train_id_acc: 1.000000\n",
      "batch: 1500, train_emo_loss: 0.003533, train_id_loss: 0.000761, train_emo_acc: 0.968750, train_id_acc: 1.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21484\\1481339565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0memo_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m#             vis_graph = make_dot(model(x, x), params=dict(model.named_parameters()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\JupyterNotebookWorkSpace\\EEG emotion recognition\\221106_MTL_attention\\MTL_Net.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mindivNets_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindiv_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindivNets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mindivNets_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindiv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mindivNets_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindivNets_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mindivNets_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindivNets_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\JupyterNotebookWorkSpace\\EEG emotion recognition\\221106_MTL_attention\\IndiviTransNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_phase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10368\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 444\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "from IndiviTransNet import IndiviTransNet\n",
    "from MTL_Net import MTL_Net\n",
    "\n",
    "for sub_idx in range(32):\n",
    "    \n",
    "    transformer_sub_list = [_ for _ in range(32)]\n",
    "    transformer_sub_list.remove(sub_idx)\n",
    "    transfomer_group = []\n",
    "    for _ in transformer_sub_list:\n",
    "        ITN = IndiviTransNet(transformer_phase=True)\n",
    "        ITN.load_state_dict(torch.load(IndiviTransNet_model_load_path + f'sub_{_}_model_parameter.pkl'))\n",
    "        transfomer_group.append(ITN)\n",
    "            \n",
    "    model = MTL_Net(transfomer_group)\n",
    "    # 初始化权重参数\n",
    "    \n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr)\n",
    "    ID_criterion = nn.CrossEntropyLoss()\n",
    "    Emo_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 提前终止\n",
    "    acc_value=0\n",
    "    step=0\n",
    "    \n",
    "    train_mask = np.ones(shape=(32 * 2400), dtype=np.int64)\n",
    "    train_mask[sub_idx * 2400 : (sub_idx + 1) * 2400] = 0\n",
    "    train_mask = train_mask == 1\n",
    "    \n",
    "    x_train = trials_de_base[train_mask]\n",
    "    emo_train = labels[train_mask, emotion_dim]\n",
    "    IDs = torch.arange(31).unsqueeze(-1)\n",
    "    ID_train = IDs.repeat(1, 2400).view(-1)\n",
    "    \n",
    "    x_train, emo_train, ID_train, x_vali, emo_vali, ID_vali = data_split(x_train, emo_train, ID_train, train_ratio = 0.9)\n",
    "    x_train, x_vali = z_norm(x_train, x_vali)\n",
    "    \n",
    "    print('make tensor...')\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "    emo_train = torch.tensor(emo_train, dtype=torch.long).unsqueeze(-1)\n",
    "    ID_train = torch.tensor(ID_train, dtype=torch.long).unsqueeze(-1)\n",
    "    y_train = torch.concat([emo_train, ID_train], dim = 1)\n",
    "    \n",
    "    x_vali = torch.tensor(x_vali, dtype=torch.float)\n",
    "    emo_vali = torch.tensor(emo_vali, dtype=torch.long).unsqueeze(-1)\n",
    "    ID_vali = torch.tensor(ID_vali, dtype=torch.long).unsqueeze(-1)\n",
    "    y_vali = torch.concat([emo_vali, ID_vali], dim = 1)\n",
    "    \n",
    "    data_train = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print('train...')\n",
    "    for i in range(max_epoch):\n",
    "        model.train()\n",
    "        print(f'epoch:{i}')\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            x, y = batch_data\n",
    "            emo_out, id_out = model(x, x)\n",
    "            \n",
    "#             vis_graph = make_dot(model(x, x), params=dict(model.named_parameters()))\n",
    "#             vis_graph.view()\n",
    "            \n",
    "            Emo_loss = Emo_criterion(emo_out, y[:, 0])\n",
    "            ID_loss = ID_criterion(id_out, y[:, 1])\n",
    "            loss = emo_imp * Emo_loss + (1 - emo_imp) * ID_loss\n",
    "#             loss = Emo_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "#             print(model.state_dict()['id_net.fc1.bias'])\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                train_emo_loss = Emo_loss / x.shape[0]\n",
    "                train_id_loss = ID_loss / x.shape[0]\n",
    "\n",
    "                emo_out = torch.max(emo_out, 1)[1]\n",
    "                train_emo_acc = (emo_out == y[:, 0]).sum() / x.shape[0]\n",
    "\n",
    "                id_out = torch.max(id_out, 1)[1]\n",
    "                train_id_acc = (id_out == y[:, 1]).sum() / x.shape[0]\n",
    "\n",
    "                print(f'batch: {batch_idx}, train_emo_loss: {train_emo_loss:.6f}, train_id_loss: {train_id_loss:.6f}, train_emo_acc: {train_emo_acc:.6f}, train_id_acc: {train_id_acc:.6f}')\n",
    "        \n",
    "        model.eval()\n",
    "        emo_out, id_out = model(x_vali, x_vali)\n",
    "        vali_emo_loss = Emo_criterion(emo_out, y_vali[:, 0]) / x_vali.shape[0]\n",
    "        vali_id_loss = ID_criterion(id_out, y_vali[:, 1]) / x_vali.shape[0]\n",
    "        \n",
    "        emo_out = torch.max(emo_out, 1)[1]\n",
    "        vali_emo_acc = (emo_out == y_vali[:, 0]).sum() / x_vali.shape[0]\n",
    "        \n",
    "        id_out = torch.max(id_out, 1)[1]\n",
    "        vali_id_acc = (id_out == y_vali[:, 1]).sum() / x_vali.shape[0]\n",
    "        \n",
    "        print(f'epoch: {i}, vali_emo_loss: {vali_emo_loss:.6f}, vali_id_loss: {vali_id_loss:.6f}, vali_emo_acc: {vali_emo_acc:.6f}, vali_id_acc: {vali_id_acc:.6f}')\n",
    "        \n",
    "        step = step + 1\n",
    "        #模型性能有所提升则保存模型，并更新loss_value\n",
    "        if  vali_emo_acc > acc_value:\n",
    "            step = 0\n",
    "            torch.save(model.state_dict(), model_save_path + f'sub_{sub_idx}_model_parameter.pkl')\n",
    "            acc_value = vali_emo_acc\n",
    "        \n",
    "        if step >= patient:\n",
    "            break\n",
    "        \n",
    "    # 测试\n",
    "    x_test = trials_de_base[train_mask==False]\n",
    "    emo_test = labels[train_mask==False]\n",
    "    emo_test = emo_test[:, emotion_dim]\n",
    "\n",
    "    x_train, x_test = z_norm(x_train.numpy(), x_test)\n",
    "\n",
    "    print('make tensor...')\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "    emo_test = torch.tensor(emo_test, dtype=torch.long)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_save_path + f'sub_{0}_model_parameter.pkl'))\n",
    "    model.eval()\n",
    "    emo_out, id_out = model(x_test, x_test)\n",
    "    test_emo_loss = Emo_criterion(emo_out, emo_test) / x_test.shape[0]\n",
    "\n",
    "    emo_out = torch.max(emo_out, 1)[1]\n",
    "    test_emo_acc = (emo_out == emo_test).sum() / x_test.shape[0]\n",
    "    #------------------------------------------------看看测试者的id\n",
    "\n",
    "\n",
    "    print(f'sub: {sub_idx}, test_emo_loss: {vali_emo_loss:.6f}, test_emo_acc: {test_emo_acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca60bee-aa6c-463f-ae6b-0a9064689e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0151, -0.0278, -0.0150, -0.0164, -0.0173,  0.0016, -0.0066,  0.0091,\n",
      "         0.0129, -0.0232, -0.0003, -0.0231, -0.0143, -0.0002, -0.0057,  0.0151,\n",
      "         0.0020,  0.0042,  0.0618,  0.0283, -0.0025, -0.0111,  0.0416,  0.0689,\n",
      "         0.0010, -0.0021, -0.0061,  0.0154,  0.0150, -0.0045,  0.0240, -0.0334,\n",
      "         0.0157, -0.0150,  0.0126, -0.0149,  0.0193, -0.0128, -0.0071, -0.0535,\n",
      "         0.0106, -0.0117, -0.0329, -0.0148,  0.0059, -0.0045, -0.0154, -0.0007,\n",
      "         0.0107, -0.0082])\n"
     ]
    }
   ],
   "source": [
    "# print(model.state_dict()['id_net.fc1.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7ac71-6271-451d-b8d8-d3bcea99d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path + f'sub_{0}_model_parameter.pkl')\n",
    "x_test = trials_de_base[train_mask==False]\n",
    "emo_test = labels[train_mask==False]\n",
    "emo_test = emo_test[:, emotion_dim]\n",
    "\n",
    "x_train, x_test = z_norm(x_train.numpy(), x_test)\n",
    "\n",
    "print('make tensor...')\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "emo_test = torch.tensor(emo_test, dtype=torch.long)\n",
    "    \n",
    "model.load_state_dict(torch.load(model_save_path + f'sub_{0}_model_parameter.pkl'))\n",
    "model.eval()\n",
    "emo_out, id_out = model(x_test, x_test)\n",
    "test_emo_loss = Emo_criterion(emo_out, emo_test) / x_test.shape[0]\n",
    "\n",
    "emo_out = torch.max(emo_out, 1)[1]\n",
    "test_emo_acc = (emo_out == emo_test).sum() / x_test.shape[0]\n",
    "#------------------------------------------------看看测试者的id\n",
    "\n",
    "\n",
    "print(f'sub: {sub_idx}, test_emo_loss: {vali_emo_loss:.6f}, test_emo_acc: {test_emo_acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2c3e1-6799-4430-912e-a099ac3fb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PATH\"] += os.pathsep +'C:/Program Files/Graphviz/bin'\n",
    "# plot_model(model,'cnn2_dens2.jpg', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09fd3d-4718-4710-bbb0-122519ff7bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 建立模型\n",
    "\n",
    "\n",
    "# callbacksList = [EarlyStopping(monitor = 'val_acc', patience=10, verbose = 1), # patience当连续多少个epochs时验证集精度不再变好终止训练\n",
    "#                 ModelCheckpoint(filepath = f'model/model.h5',monitor='acc',save_best_only=True,)\n",
    "# ]\n",
    "\n",
    "# # 评估\n",
    "# model = load_model('model/model.h5')\n",
    "# score = model.evaluate(x_test, y__v_test, verbose = 1)\n",
    "\n",
    "# \"\"\"\n",
    "# Epoch 39/200\n",
    "# 2160/2160 [==============================] - 59s 27ms/step - loss: 0.0448 - acc: 0.9878 - val_loss: 0.5709 - val_acc: 0.9161\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
