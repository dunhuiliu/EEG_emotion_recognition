{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa1bac8-927b-45c5-8d2c-997f4ed13b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "load_path = '../global_data/time_9830400(76800x128)x9x9/'\n",
    "save_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b159a317-148a-4369-8f4c-5eab8cc55961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = np.load(load_path + 'trials.npy')\n",
    "# bases = np.load(load_path + 'bases.npy')\n",
    "# labels = np.load(load_path + 'labels.npy')\n",
    "# print(trials.shape, bases.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622a15f7-4cfc-466f-a052-0788678fecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8b0339-3794-467b-9018-22233c6516cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 切窗口\n",
    "# def slice(trials):\n",
    "#     samples = []\n",
    "#     for trial in trials:\n",
    "#         start = 0\n",
    "#         while start + window_size <= trial.shape[0]:\n",
    "#             samples.append(trial[start : start + window_size])\n",
    "#             start = start + window_size\n",
    "#     return np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39bd233-22e6-40b0-9edc-23028acfb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_samples = slice(trials)\n",
    "# bases_samples = slice(bases)\n",
    "\n",
    "\n",
    "# np.save(save_path + 'trials.npy', trial_samples)\n",
    "# np.save(save_path + 'bases.npy', bases_samples)\n",
    "# print(trial_samples.shape, bases_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc8320f-4446-408a-a1cb-96f933dea1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76800, 128, 9, 9) (3840, 128, 9, 9) (1280, 2)\n"
     ]
    }
   ],
   "source": [
    "trial_samples = np.load(save_path + 'trials.npy')\n",
    "bases_samples = np.load(save_path + 'bases.npy')\n",
    "labels = np.load(load_path + 'labels.npy')\n",
    "print(trial_samples.shape, bases_samples.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55541bfc-e84a-473a-b232-d25fc1c4f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去基线\n",
    "def de_base():\n",
    "    global bases_samples, trial_samples\n",
    "    bases_samples = bases_samples.reshape((1280, 3, 128, 9, 9))\n",
    "    bases_samples = bases_samples.mean(axis = 1)\n",
    "\n",
    "    for i, base in enumerate(bases_samples):\n",
    "        trial_samples[i * 60 : (i + 1) * 60] = trial_samples[i * 60 : (i + 1) * 60] - base\n",
    "\n",
    "de_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40104c1-f961-4b7f-93cf-77bc65cc909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76800, 2)\n"
     ]
    }
   ],
   "source": [
    "# 离散化标签\n",
    "labels = np.where(labels >= 5, 1, 0)\n",
    "\n",
    "# 复制以对齐样本\n",
    "labels = np.repeat(labels, 60, axis = 0)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0954400e-150c-49ef-87f5-4509edf836e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split dataset...\n"
     ]
    }
   ],
   "source": [
    "# 数据集划分\n",
    "def data_split(features, labels, train_ratio = 0.9):\n",
    "    print('split dataset...')\n",
    "    sample_cnt = labels.shape[0]\n",
    "    shuffer_list = np.arange(sample_cnt)\n",
    "    np.random.shuffle(shuffer_list)\n",
    "\n",
    "    features = features[shuffer_list] # trials_de_base\n",
    "    labels = labels[shuffer_list]\n",
    "\n",
    "    x_train = features[:int(train_ratio * sample_cnt)]\n",
    "    y_train = labels[:int(train_ratio * sample_cnt)]\n",
    "    x_test = features[int(train_ratio * sample_cnt):]\n",
    "    y_test = labels[int(train_ratio * sample_cnt):]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_vali, y_vali = data_split(trial_samples, labels, train_ratio = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972cdf6a-f3b8-4752-8611-3a1c9e5a8ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_norm...\n"
     ]
    }
   ],
   "source": [
    "def z_norm(x_train, x_test, window_size):# z归一化\n",
    "    print('z_norm...')\n",
    "    chan_to_1020={0:[0,3],1:[1,3],2:[2,2],3:[2,0],4:[3,1],5:[3,3],6:[4,2],7:[4,0],8:[5,1],\n",
    "                  9:[5,3],10:[6,2],11:[6,0],12:[7,3],13:[8,3],14:[8,4],15:[6,4],16:[0,5],\n",
    "                  17:[1,5],18:[2,4],19:[2,6],20:[2,8],21:[3,7],22:[3,5],23:[4,4],24:[4,6],\n",
    "                    25:[4,8],26:[5,7],27:[5,5],28:[6,6],29:[6,8],30:[7,5],31:[8,5]}\n",
    "\n",
    "    # 需要归一化的位置\n",
    "    need_norm_pos = np.zeros(shape=(9,9), dtype=int)\n",
    "\n",
    "    for val in chan_to_1020.values():\n",
    "        need_norm_pos[val[0]][val[1]] = 1\n",
    "    # print(need_norm_pos)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if need_norm_pos[i][j] == 0:\n",
    "                continue\n",
    "            for t in range(window_size):\n",
    "                mean = np.mean(x_train[:, t, i, j])\n",
    "                std = np.std(x_train[:, t, i, j])\n",
    "                x_train[:, t, i, j] -= mean\n",
    "                x_train[:, t, i, j] /= std\n",
    "                x_test[:, t, i, j] -= mean\n",
    "                x_test[:, t, i, j] /= std\n",
    "    return x_train, x_test\n",
    "\n",
    "x_train, x_vali = z_norm(x_train, x_vali, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2497680b-5fdd-427d-84b0-fc292ffd0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dim = 0\n",
    "y_train =y_train[:, emotion_dim]\n",
    "y_vali = y_vali[:, emotion_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9615d685-488c-4c50-a8a2-9d547189657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "# from torchviz import make_dot\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9571b0-54f0-4a0c-8e90-48b37bab83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参设置\n",
    "batch_size = 128\n",
    "max_epoch = 1000\n",
    "\n",
    "lr = 0.00001\n",
    "patient = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e1f686f-5c48-414d-820f-7a57617ac179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make tensor...\n"
     ]
    }
   ],
   "source": [
    "print('make tensor...')\n",
    "x_train = torch.tensor(x_train, dtype=torch.float).unsqueeze(1) # 做成3d卷积的输入形状 [batch_size, channel, depth, height, width]\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_vali = torch.tensor(x_vali, dtype=torch.float).unsqueeze(1)\n",
    "y_vali = torch.tensor(y_vali, dtype=torch.long)\n",
    "\n",
    "data_train = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae632cb-fcb9-4d80-acc0-fbb2ddec5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')       # 用GPU来运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f35b72e-b6fb-4f76-b755-e6c7f953cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3d, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels = 1, out_channels = 1, kernel_size = (3, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0), device=device)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(in_channels = 1, out_channels = 1, kernel_size = (3, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0), device=device)\n",
    "        \n",
    "        self.fc1 = nn.Linear(3100, 2560)\n",
    "        self.fc2 = nn.Linear(2560, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7a90f38-e032-4f12-bd80-9d10e41cb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  Inception_Conv3d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Inception_Conv3d, self).__init__()\n",
    "        self.conv11 = nn.Conv3d(in_channels = 1, out_channels = 1, kernel_size = (3, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0))\n",
    "        \n",
    "        self.conv12 = nn.Conv3d(in_channels = 1, out_channels = 1, kernel_size = (9, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0))\n",
    "        \n",
    "        self.conv13 = nn.Conv3d(in_channels = 1, out_channels = 1, kernel_size = (27, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0))\n",
    "        \n",
    "        \n",
    "        self.conv21 = nn.Conv3d(in_channels = 3, out_channels = 1, kernel_size = (3, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0))\n",
    "        \n",
    "        self.conv22 = nn.Conv3d(in_channels = 3, out_channels = 1, kernel_size = (9, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0))\n",
    "        \n",
    "        self.conv23 = nn.Conv3d(in_channels = 3, out_channels = 1, kernel_size = (27, 3, 3),\n",
    "            stride = (1, 1, 1), padding = (0, 0, 0))\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(8550, 2560)\n",
    "        self.fc2 = nn.Linear(2560, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.conv11(x))\n",
    "        x2 = F.relu(self.conv12(x))\n",
    "        x3 = F.relu(self.conv13(x))\n",
    "        \n",
    "        x2 = F.pad(x2, (0, 0, 0, 0, 0, x1.shape[2] - x2.shape[2]), \"constant\", 0)\n",
    "        x3 = F.pad(x3, (0, 0, 0, 0, 0, x1.shape[2] - x3.shape[2]), \"constant\", 0)\n",
    "        \n",
    "        x = torch.concat([x1, x2, x3], dim = 1)\n",
    "        \n",
    "        \n",
    "        x1 = F.relu(self.conv21(x))\n",
    "        x2 = F.relu(self.conv22(x))\n",
    "        x3 = F.relu(self.conv23(x))\n",
    "        \n",
    "#         x2 = F.pad(x2, (0, 0, 0, 0, 0, x1.shape[2] - x2.shape[2]), \"constant\", 0)\n",
    "#         x3 = F.pad(x3, (0, 0, 0, 0, 0, x1.shape[2] - x3.shape[2]), \"constant\", 0)\n",
    "\n",
    "        x1 = x1.view(x.shape[0], -1)\n",
    "        x2 = x2.view(x.shape[0], -1)\n",
    "        x3 = x3.view(x.shape[0], -1)\n",
    "        \n",
    "        x = torch.concat([x1, x2, x3], dim = 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27838f06-4bb2-4c21-bb9d-3689069d7089",
   "metadata": {},
   "source": [
    "- 时空多尺度感知，时间多尺度感知，空间也多尺度感知，改变卷积核正方形的变长。\n",
    "- 做padding\n",
    "- 加batch normal\n",
    "- 加Dropout\n",
    "- 调整batch_size、学习率\n",
    "- 调整参数量\n",
    "- 使用1x1的卷积核，增加网络深度\n",
    "- 设计一个损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e5684-63bc-4596-9ae4-5bc791b09195",
   "metadata": {},
   "source": [
    "# 对比实验\n",
    "\n",
    "- 使用不同的window_size\n",
    "- 能不能设计一个损失函数，比较你的损失和交叉熵的性能\n",
    "- 多数据集验证\n",
    "- 消融实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79bc818-3ddd-4c43-a4a2-6f8891693359",
   "metadata": {},
   "source": [
    "# 实验结果展示\n",
    "- 时间、内存分析\n",
    "- 混淆矩阵\n",
    "- 多评估指标比较\n",
    "- 模型中间过程验证模型设计\n",
    "- 与其他方法的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5726817-5823-4274-8dad-7890cb53faff",
   "metadata": {},
   "source": [
    "# 来这里初始化一下模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "380cd741-a217-4253-81eb-dd31123dbeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception_Conv3d(\n",
       "  (conv11): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  (conv12): Conv3d(1, 1, kernel_size=(9, 3, 3), stride=(1, 1, 1))\n",
       "  (conv13): Conv3d(1, 1, kernel_size=(27, 3, 3), stride=(1, 1, 1))\n",
       "  (conv21): Conv3d(3, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  (conv22): Conv3d(3, 1, kernel_size=(9, 3, 3), stride=(1, 1, 1))\n",
       "  (conv23): Conv3d(3, 1, kernel_size=(27, 3, 3), stride=(1, 1, 1))\n",
       "  (fc1): Linear(in_features=8550, out_features=2560, bias=True)\n",
       "  (fc2): Linear(in_features=2560, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化权重参数\n",
    "\n",
    "# 1. 根据网络层的不同定义不同的初始化方式     \n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    # 也可以判断是否为conv2d，使用相应的初始化方式 \n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "     # 是否为批归一化层\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model = Inception_Conv3d().to(device)\n",
    "\n",
    "# 3. 将weight_init应用在子模块上\n",
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328b8ab7-6ab8-4b72-ac21-422f4852582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05b603be-e34e-49ca-b8ed-3c8baea5fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.004703, train_acc:0.672584, vali_loss: 0.000069, vali_acc: 0.745703\n",
      "epoch:1, train_loss:0.003560, train_acc:0.795255, vali_loss: 0.000061, vali_acc: 0.807162\n",
      "epoch:2, train_loss:0.002800, train_acc:0.859404, vali_loss: 0.000052, vali_acc: 0.831901\n",
      "epoch:3, train_loss:0.002255, train_acc:0.894835, vali_loss: 0.000046, vali_acc: 0.861719\n",
      "epoch:4, train_loss:0.001824, train_acc:0.922772, vali_loss: 0.000048, vali_acc: 0.842448\n",
      "epoch:5, train_loss:0.001489, train_acc:0.942679, vali_loss: 0.000045, vali_acc: 0.856510\n",
      "epoch:6, train_loss:0.001219, train_acc:0.957407, vali_loss: 0.000040, vali_acc: 0.884245\n",
      "epoch:7, train_loss:0.000992, train_acc:0.968953, vali_loss: 0.000039, vali_acc: 0.890104\n",
      "epoch:8, train_loss:0.000808, train_acc:0.977575, vali_loss: 0.000036, vali_acc: 0.901302\n",
      "epoch:9, train_loss:0.000650, train_acc:0.984896, vali_loss: 0.000034, vali_acc: 0.908073\n",
      "epoch:10, train_loss:0.000552, train_acc:0.987905, vali_loss: 0.000037, vali_acc: 0.896354\n",
      "epoch:11, train_loss:0.000471, train_acc:0.991001, vali_loss: 0.000034, vali_acc: 0.913672\n",
      "epoch:12, train_loss:0.000375, train_acc:0.994184, vali_loss: 0.000034, vali_acc: 0.914323\n",
      "epoch:13, train_loss:0.000305, train_acc:0.996311, vali_loss: 0.000037, vali_acc: 0.911979\n",
      "epoch:14, train_loss:0.000290, train_acc:0.996210, vali_loss: 0.000037, vali_acc: 0.911589\n",
      "epoch:15, train_loss:0.000239, train_acc:0.997555, vali_loss: 0.000034, vali_acc: 0.918880\n",
      "epoch:16, train_loss:0.000202, train_acc:0.997642, vali_loss: 0.000037, vali_acc: 0.909635\n",
      "epoch:17, train_loss:0.000229, train_acc:0.996542, vali_loss: 0.000034, vali_acc: 0.917578\n",
      "epoch:18, train_loss:0.000139, train_acc:0.999002, vali_loss: 0.000036, vali_acc: 0.917448\n",
      "epoch:19, train_loss:0.000156, train_acc:0.998192, vali_loss: 0.000034, vali_acc: 0.919662\n",
      "epoch:20, train_loss:0.000105, train_acc:0.999117, vali_loss: 0.000036, vali_acc: 0.919662\n",
      "epoch:21, train_loss:0.000121, train_acc:0.998698, vali_loss: 0.000037, vali_acc: 0.920703\n",
      "epoch:22, train_loss:0.000137, train_acc:0.997729, vali_loss: 0.000038, vali_acc: 0.916276\n",
      "epoch:23, train_loss:0.000092, train_acc:0.998799, vali_loss: 0.000039, vali_acc: 0.913802\n",
      "epoch:24, train_loss:0.000102, train_acc:0.998987, vali_loss: 0.000038, vali_acc: 0.922135\n",
      "epoch:25, train_loss:0.000127, train_acc:0.998163, vali_loss: 0.000036, vali_acc: 0.923307\n",
      "epoch:26, train_loss:0.000062, train_acc:0.999508, vali_loss: 0.000035, vali_acc: 0.923177\n",
      "epoch:27, train_loss:0.000083, train_acc:0.999045, vali_loss: 0.000039, vali_acc: 0.924349\n",
      "epoch:28, train_loss:0.000056, train_acc:0.999450, vali_loss: 0.000039, vali_acc: 0.918750\n",
      "epoch:29, train_loss:0.000061, train_acc:0.999378, vali_loss: 0.000038, vali_acc: 0.924740\n",
      "epoch:30, train_loss:0.000048, train_acc:0.999392, vali_loss: 0.000036, vali_acc: 0.921745\n",
      "epoch:31, train_loss:0.000086, train_acc:0.998466, vali_loss: 0.000037, vali_acc: 0.924219\n",
      "epoch:32, train_loss:0.000079, train_acc:0.998322, vali_loss: 0.000040, vali_acc: 0.921484\n",
      "epoch:33, train_loss:0.000046, train_acc:0.999392, vali_loss: 0.000043, vali_acc: 0.918880\n",
      "epoch:34, train_loss:0.000080, train_acc:0.998394, vali_loss: 0.000039, vali_acc: 0.922266\n",
      "epoch:35, train_loss:0.000054, train_acc:0.999175, vali_loss: 0.000040, vali_acc: 0.920313\n",
      "epoch:36, train_loss:0.000046, train_acc:0.999306, vali_loss: 0.000039, vali_acc: 0.925651\n",
      "epoch:37, train_loss:0.000026, train_acc:0.999740, vali_loss: 0.000037, vali_acc: 0.930599\n",
      "epoch:38, train_loss:0.000011, train_acc:0.999986, vali_loss: 0.000039, vali_acc: 0.926302\n",
      "epoch:39, train_loss:0.000074, train_acc:0.998293, vali_loss: 0.000038, vali_acc: 0.929818\n",
      "epoch:40, train_loss:0.000047, train_acc:0.999175, vali_loss: 0.000043, vali_acc: 0.921224\n",
      "epoch:41, train_loss:0.000060, train_acc:0.998929, vali_loss: 0.000040, vali_acc: 0.925651\n",
      "epoch:42, train_loss:0.000024, train_acc:0.999711, vali_loss: 0.000040, vali_acc: 0.924089\n",
      "epoch:43, train_loss:0.000048, train_acc:0.999060, vali_loss: 0.000038, vali_acc: 0.929688\n",
      "epoch:44, train_loss:0.000040, train_acc:0.999320, vali_loss: 0.000042, vali_acc: 0.923958\n",
      "epoch:45, train_loss:0.000037, train_acc:0.999508, vali_loss: 0.000043, vali_acc: 0.917318\n",
      "epoch:46, train_loss:0.000037, train_acc:0.999479, vali_loss: 0.000040, vali_acc: 0.926693\n",
      "epoch:47, train_loss:0.000046, train_acc:0.999117, vali_loss: 0.000042, vali_acc: 0.920964\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'model/'\n",
    "\n",
    "# 提前终止\n",
    "acc_history=0\n",
    "step=0\n",
    "\n",
    "for i in range(max_epoch):\n",
    "    model.train()\n",
    "    print(f'epoch:{i}', end = ', ')\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        x, y = batch_data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        out = torch.max(out, 1)[1]\n",
    "        epoch_acc = epoch_acc + (out == y).sum()\n",
    "        \n",
    "#         if batch_idx % 1000 == 0:\n",
    "#             train_loss = loss / x.shape[0]\n",
    "#             out = torch.max(out, 1)[1]\n",
    "#             train_acc = (out == y).sum() / x.shape[0]\n",
    "#             print(f'batch: {batch_idx}, train_loss: {train_loss:.6f}, train_acc: {train_acc:.6f}')\n",
    "    print(f'train_loss:{epoch_loss / len(train_loader.dataset) :.6f}, train_acc:{epoch_acc / len(train_loader.dataset) :.6f}', end = ', ')\n",
    "    \n",
    "    # 测试\n",
    "    model.eval()\n",
    "    x_vali, y_vali = x_vali.to(device), y_vali.to(device)\n",
    "    out = model(x_vali)\n",
    "    vali_loss = criterion(out, y_vali) / x_vali.shape[0]\n",
    "\n",
    "    out = torch.max(out, 1)[1]\n",
    "    vali_acc = (out == y_vali).sum() / x_vali.shape[0]\n",
    "\n",
    "    print(f'vali_loss: {vali_loss:.6f}, vali_acc: {vali_acc:.6f}')\n",
    "    \n",
    "#     out = model(x)\n",
    "#     vali_loss = criterion(out, y) / y.shape[0]\n",
    "\n",
    "#     out = torch.max(out, 1)[1]\n",
    "#     vali_acc = (out == y).sum() / x.shape[0]\n",
    "\n",
    "#     print(f'vali_loss: {vali_loss:.6f}, vali_acc: {vali_acc:.6f}')\n",
    "\n",
    "    step = step + 1\n",
    "    #模型性能有所提升则保存模型，并更新loss_value\n",
    "    if  vali_acc > acc_history:\n",
    "        step = 0\n",
    "        torch.save(model.state_dict(), model_save_path + f'model_parameter.pkl')\n",
    "        acc_history = vali_acc\n",
    "\n",
    "    if step >= patient:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c212d718-2d2d-4f82-bf25-3c26a101ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过测试，不去基线，训练集收敛很慢，且训练集准确率可以达到100%，测试集准确率只能达到58%\n",
    "# 去基线，训练集很快收敛，测试集准确率在第10个epoch就达到了79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bd81b35-b931-4d21-ae09-76a2b293b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试\n",
    "\n",
    "# print('make tensor...')\n",
    "# x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "# emo_test = torch.tensor(emo_test, dtype=torch.long)\n",
    "\n",
    "# model.load_state_dict(torch.load(model_save_path + f'sub_{0}_model_parameter.pkl'))\n",
    "# model.eval()\n",
    "# emo_out, id_out = model(x_test, x_test)\n",
    "# test_emo_loss = Emo_criterion(emo_out, emo_test) / x_test.shape[0]\n",
    "\n",
    "# emo_out = torch.max(emo_out, 1)[1]\n",
    "# test_emo_acc = (emo_out == emo_test).sum() / x_test.shape[0]\n",
    "# #------------------------------------------------看看测试者的id\n",
    "\n",
    "\n",
    "# print(f'sub: {sub_idx}, test_emo_loss: {vali_emo_loss:.6f}, test_emo_acc: {test_emo_acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a37094-e83c-426f-a19d-dbd1e06404de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
